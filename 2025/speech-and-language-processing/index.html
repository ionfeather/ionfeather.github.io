<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Introduction\rÈòÖËØªSpeech and Language ProcessingËøôÊú¨‰π¶ÁöÑ‰∏Ä‰∫õÁ¨îËÆ∞„ÄÇ\nWords and Tokens\rÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™‰∏úË•øÊù•Âª∫Ê®°ËØ≠Ë®ÄÔºå‰∏ãÈù¢ÊòØÊàë‰ª¨ÁöÑÈÄâÊã©Ôºö\nWords\r‰∏∫‰ªÄ‰πà‰∏çÁî®ËØçÔºü\nÊúâ‰∫õËØ≠Ë®ÄÊ≤°Êúâorthographic words ËØçÁöÑÊï∞Èáè‰ºöÈöèÁùÄÊñáÁ´†Â¢ûÈïøÔºåËØçÊ±áË°®Ê∞∏ËøúÈÉΩ‰ºöË¶ÜÁõñ‰∏çË∂≥ Morphemes\rËØ≠Á¥†Á±ªÂûã\n">
<title>‰π¶Á±çÈòÖËØª | Speech and Language Processing</title>

<link rel='canonical' href='https://ionfeather.github.io/2025/speech-and-language-processing/'>

<link rel="stylesheet" href="/scss/style.min.cdd95828ca8971b17ccb14112222a60d19d84ea3f4e5b525c8c68fb4d2a4535d.css"><meta property='og:title' content="‰π¶Á±çÈòÖËØª | Speech and Language Processing">
<meta property='og:description' content="Introduction\rÈòÖËØªSpeech and Language ProcessingËøôÊú¨‰π¶ÁöÑ‰∏Ä‰∫õÁ¨îËÆ∞„ÄÇ\nWords and Tokens\rÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™‰∏úË•øÊù•Âª∫Ê®°ËØ≠Ë®ÄÔºå‰∏ãÈù¢ÊòØÊàë‰ª¨ÁöÑÈÄâÊã©Ôºö\nWords\r‰∏∫‰ªÄ‰πà‰∏çÁî®ËØçÔºü\nÊúâ‰∫õËØ≠Ë®ÄÊ≤°Êúâorthographic words ËØçÁöÑÊï∞Èáè‰ºöÈöèÁùÄÊñáÁ´†Â¢ûÈïøÔºåËØçÊ±áË°®Ê∞∏ËøúÈÉΩ‰ºöË¶ÜÁõñ‰∏çË∂≥ Morphemes\rËØ≠Á¥†Á±ªÂûã\n">
<meta property='og:url' content='https://ionfeather.github.io/2025/speech-and-language-processing/'>
<meta property='og:site_name' content='ionfeather&#39;Log'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='LLM' /><meta property='article:published_time' content='2025-08-28T19:28:49&#43;08:00'/><meta property='article:modified_time' content='2025-08-28T19:28:49&#43;08:00'/>
<meta name="twitter:title" content="‰π¶Á±çÈòÖËØª | Speech and Language Processing">
<meta name="twitter:description" content="Introduction\rÈòÖËØªSpeech and Language ProcessingËøôÊú¨‰π¶ÁöÑ‰∏Ä‰∫õÁ¨îËÆ∞„ÄÇ\nWords and Tokens\rÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™‰∏úË•øÊù•Âª∫Ê®°ËØ≠Ë®ÄÔºå‰∏ãÈù¢ÊòØÊàë‰ª¨ÁöÑÈÄâÊã©Ôºö\nWords\r‰∏∫‰ªÄ‰πà‰∏çÁî®ËØçÔºü\nÊúâ‰∫õËØ≠Ë®ÄÊ≤°Êúâorthographic words ËØçÁöÑÊï∞Èáè‰ºöÈöèÁùÄÊñáÁ´†Â¢ûÈïøÔºåËØçÊ±áË°®Ê∞∏ËøúÈÉΩ‰ºöË¶ÜÁõñ‰∏çË∂≥ Morphemes\rËØ≠Á¥†Á±ªÂûã\n">
    <link rel="shortcut icon" href="/ion.ico" />

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&display=swap"
    onload="this.media='all'" onError="this.media='none'">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@callmebill/lxgw-wenkai-web@latest/style.css"
    onload="this.media='all'" onError="this.media='none'">

  <style>
     
    :root {
      --sys-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --zh-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --base-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --code-font-family: 'Consolas', monospace; 
      --article-font-family: 'Noto Serif SC', serif; 
      --heading-font-family: 'LXGW WenKai', serif; 
    }

     
    body {
      font-family: var(--base-font-family);
      font-weight: normal;
    }
  </style>
</head>

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="ÂàáÊç¢ËèúÂçï">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_7d702df343b40e37.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">üå≥</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">ionfeather&#39;Log</a></h1>
            <h2 class="site-description">ÂçÅÂπ¥È•ÆÂÜ∞ÔºåÈöæÂáâÁÉ≠Ë°Ä</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/ionfeather'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg t="1732597946058" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3746" width="24" height="24"><path d="M850.346667 155.008a42.666667 42.666667 0 0 0-22.741334-23.509333c-8.704-3.754667-85.717333-33.322667-200.32 39.168H396.714667c-114.773333-72.618667-191.701333-42.922667-200.32-39.168a42.88 42.88 0 0 0-22.741334 23.466666c-26.197333 66.218667-18.048 136.448-7.850666 176.896C134.272 374.016 128 413.098667 128 469.333333c0 177.877333 127.104 227.882667 226.730667 246.272a189.568 189.568 0 0 0-13.013334 46.549334A44.373333 44.373333 0 0 0 341.333333 768v38.613333c-19.498667-4.138667-41.002667-11.946667-55.168-26.112C238.08 732.416 188.330667 682.666667 128 682.666667v85.333333c25.002667 0 65.365333 40.362667 97.834667 72.832 51.029333 51.029333 129.066667 55.253333 153.386666 55.253333 3.114667 0 5.376-0.085333 6.528-0.128A42.666667 42.666667 0 0 0 426.666667 853.333333v-82.090666c4.266667-24.746667 20.224-49.621333 27.946666-56.362667a42.666667 42.666667 0 0 0-23.125333-74.581333C293.333333 624.554667 213.333333 591.488 213.333333 469.333333c0-53.12 5.632-70.741333 31.573334-99.285333 11.008-12.117333 14.08-29.568 7.978666-44.8-4.821333-11.904-18.773333-65.450667-6.485333-117.546667 20.650667-1.578667 59.904 4.565333 113.706667 40.96C367.104 253.44 375.466667 256 384 256h256a42.666667 42.666667 0 0 0 23.936-7.338667c54.016-36.522667 92.970667-41.770667 113.664-41.130666 12.330667 52.224-1.578667 105.770667-6.4 117.674666a42.666667 42.666667 0 0 0 8.021333 44.928C805.077333 398.464 810.666667 416.085333 810.666667 469.333333c0 122.581333-79.957333 155.52-218.069334 170.922667a42.666667 42.666667 0 0 0-23.125333 74.709333c19.797333 17.066667 27.861333 32.469333 27.861333 53.034667v128h85.333334v-128c0-20.437333-3.925333-38.101333-9.770667-53.12C769.92 695.765333 896 643.712 896 469.333333c0-56.362667-6.272-95.530667-37.76-137.514666 10.197333-40.405333 18.261333-110.506667-7.893333-176.810667z" fill="currentColor" p-id="3747"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='mailto:lizishadowmay@gmail.com'
                        target="_blank"
                        title="Email"
                        rel="me"
                    >
                        
                        
                            <svg t="1732597869588" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23464" width="24" height="24"><path d="M926.47619 355.644952V780.190476a73.142857 73.142857 0 0 1-73.142857 73.142857H170.666667a73.142857 73.142857 0 0 1-73.142857-73.142857V355.644952l73.142857 62.000762V780.190476h682.666666V417.645714l73.142857-62.000762zM853.333333 170.666667a74.044952 74.044952 0 0 1 26.087619 4.778666 72.704 72.704 0 0 1 30.622477 22.186667 73.508571 73.508571 0 0 1 10.678857 17.67619c3.169524 7.509333 5.12 15.652571 5.607619 24.210286L926.47619 243.809524v24.380952L559.469714 581.241905a73.142857 73.142857 0 0 1-91.306666 2.901333l-3.632762-2.925714L97.52381 268.190476v-24.380952a72.899048 72.899048 0 0 1 40.155428-65.292191A72.97219 72.97219 0 0 1 170.666667 170.666667h682.666666z m-10.971428 73.142857H181.638095L512 525.58019 842.361905 243.809524z" p-id="23465" fill="currentColor"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='index.xml'
                        target="_blank"
                        title="RSS"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon" viewBox="0 0 1024 1024" width="200" height="200">
  <path 
    d="M170.666667 426.666667c-25.6 0-42.666667 17.066667-42.666667 42.666666s17.066667 42.666667 42.666667 42.666667c187.733333 0 341.333333 153.6 341.333333 341.333333 0 25.6 17.066667 42.666667 42.666667 42.666667s42.666667-17.066667 42.666666-42.666667c0-234.666667-192-426.666667-426.666666-426.666666z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
  <path 
    d="M170.666667 128c-25.6 0-42.666667 17.066667-42.666667 42.666667s17.066667 42.666667 42.666667 42.666666c354.133333 0 640 285.866667 640 640 0 25.6 17.066667 42.666667 42.666666 42.666667s42.666667-17.066667 42.666667-42.666667c0-401.066667-324.266667-725.333333-725.333333-725.333333z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
  <path 
    d="M213.333333 810.666667m-85.333333 0a85.333333 85.333333 0 1 0 170.666667 0 85.333333 85.333333 0 1 0-170.666667 0Z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
</svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>‰∏ªÈ°µ</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>ÂÖ≥‰∫é</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>ÂΩíÊ°£</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>ÊêúÁ¥¢</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%8F%8B%E9%93%BE/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>ÂèãÈìæ</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            

                
                    <span id="dark-mode-toggle">
                        <svg  xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left"   width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828a4 4 0 1 0 -5.656 -5.656a4 4 0 0 0 5.656 5.656z" /><path d="M6.343 17.657l-1.414 1.414" /><path d="M6.343 6.343l-1.414 -1.414" /><path d="M17.657 6.343l1.414 -1.414" /><path d="M17.657 17.657l1.414 1.414" /><path d="M4 12h-2" /><path d="M12 4v-2" /><path d="M20 12h2" /><path d="M12 20v2" /></svg>
                        <svg  xmlns="http://www.w3.org/2000/svg"  class="icon icon-tabler icon-tabler-toggle-right"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-moon"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" /></svg>
                    </span>
                
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">ÁõÆÂΩï</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#words-and-tokens">Words and Tokens</a>
      <ol>
        <li><a href="#words">Words</a></li>
        <li><a href="#morphemes">Morphemes</a></li>
        <li><a href="#unicode">Unicode</a>
          <ol>
            <li><a href="#code-points">Code Points</a></li>
            <li><a href="#utf-8">UTF-8</a></li>
          </ol>
        </li>
        <li><a href="#subword-tokenization-byte-pair-encoding">Subword Tokenization: Byte-Pair Encoding</a>
          <ol>
            <li><a href="#bpe">BPE</a></li>
            <li><a href="#bpe-encoder">BPE encoder</a></li>
            <li><a href="#bpe-in-practice">BPE in practice</a></li>
          </ol>
        </li>
        <li><a href="#rule-based-tokenization">Rule-based tokenization</a>
          <ol>
            <li><a href="#sentence-segmentation">Sentence Segmentation</a></li>
          </ol>
        </li>
        <li><a href="#corpora">Corpora</a></li>
        <li><a href="#regular-expressions">Regular Expressions</a></li>
        <li><a href="#simple-unix-tools-for-word-tokenization">Simple Unix Tools for Word Tokenization</a></li>
        <li><a href="#minimum-edit-distance">Minimum Edit Distance</a>
          <ol>
            <li><a href="#the-minimum-edit-distance-algorithm">The Minimum Edit Distance Algorithm</a></li>
          </ol>
        </li>
        <li><a href="#exercies">Exercies</a>
          <ol>
            <li></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#n-gram-language-models">N-gram Language Models</a>
      <ol>
        <li><a href="#n-grams">N-Grams</a>
          <ol>
            <li><a href="#how-to-estimate-probabilities">How to estimate probabilities</a></li>
            <li><a href="#dealing-with-scale-in-large-n-gram-models">Dealing with scale in large n-gram models</a></li>
            <li><a href="#evaluating-language-models-training-and-test-sets">Evaluating Language Models: Training and Test Sets</a></li>
            <li><a href="#evaluating-language-models-perplexity">Evaluating Language Models: Perplexity</a></li>
          </ol>
        </li>
        <li><a href="#sampling-sentences-from-a-language-model">Sampling sentences from a language model</a></li>
        <li><a href="#generalizing-vs-overfitting-the-training-set">Generalizing vs. overfitting the training set</a></li>
        <li><a href="#smoothing-interpolation-and-backoff">Smoothing, Interpolation, and Backoff</a>
          <ol>
            <li><a href="#laplace-smoothing">Laplace Smoothing</a></li>
            <li><a href="#add-k-smoothing">Add-k Smoothing</a></li>
            <li><a href="#language-model-interpolation">Language Model Interpolation</a></li>
            <li><a href="#stupid-backoff">Stupid Backoff</a></li>
          </ol>
        </li>
        <li><a href="#advanced-perplexitys-relation-to-entropy">Advanced: Perplexity&rsquo;s Relation to Entropy</a></li>
        <li><a href="#excercies">Excercies</a>
          <ol>
            <li></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#logistic-regression-and-text-classification">Logistic Regression and Text Classification</a>
      <ol>
        <li><a href="#machine-learning-and-classification">Machine Learning and Classification</a></li>
        <li><a href="#the-sigmoid-function">The Sigmoid Function</a></li>
        <li><a href="#classification-with-logistic-regression">Classification with Logistic Regression</a>
          <ol>
            <li><a href="#sentiment-classification">Sentiment Classification</a></li>
            <li><a href="#other-classification-tasks-and-features">Other Classification Tasks and Features</a></li>
            <li><a href="#processing-many-examples-at-once">Processing many examples at once</a></li>
          </ol>
        </li>
        <li><a href="#multinomial-logistic-regression">Multinomial Logistic Regression</a>
          <ol>
            <li><a href="#softmax">Softmax</a></li>
            <li><a href="#applying-softmax-in-logistic-regression">Applying Softmax in Logistic Regression</a></li>
            <li><a href="#features-in-multinomial-logistic-regression">Features in Multinomial Logistic Regression</a></li>
          </ol>
        </li>
        <li><a href="#learning-in-logistic-regression">Learning in Logistic Regression</a></li>
        <li><a href="#the-cross-entropy-loss-function">The Cross-entropy Loss Function</a></li>
        <li><a href="#gradient-descent">Gradient Descent</a>
          <ol>
            <li><a href="#the-gradient-for-logistic-regression">The Gradient for Logistic Regression</a></li>
            <li><a href="#the-stochastic-gradient-descent-algorithm">The Stochastic Gradient Descent Algorithm</a></li>
            <li><a href="#mini-batch-training">Mini-batch Training</a></li>
          </ol>
        </li>
        <li><a href="#learning-in-multinomial-logistic-regression">Learning in Multinomial Logistic Regression</a></li>
        <li><a href="#evaluation-precision-recall-f-measure">Evaluation: Precision, Recall, F-measure</a></li>
        <li><a href="#test-sets-and-cross-validation">Test sets and Cross-validation</a></li>
        <li><a href="#statistical-significance-testing">Statistical Significance Testing</a>
          <ol>
            <li><a href="#the-paired-bootstrap-test">The Paired Bootstrap Test</a></li>
          </ol>
        </li>
        <li><a href="#avoiding-harms-in-classification">Avoiding Harms in Classification</a></li>
        <li><a href="#interpereting-models">Interpereting Models</a></li>
        <li><a href="#advanced-regularization">Advanced: Regularization</a></li>
      </ol>
    </li>
    <li><a href="#embeddings">Embeddings</a>
      <ol>
        <li><a href="#lexical-semantics">Lexical Semantics</a></li>
        <li><a href="#vector-semantics-the-intuition">Vector Semantics: The Intuition</a>
          <ol>
            <li><a href="#simple-count-based-embeddings">Simple Count-based Embeddings</a></li>
            <li><a href="#cosine-for-measuring-similarity">Cosine for Measuring Similarity</a></li>
            <li><a href="#word2vec">Word2vec</a></li>
            <li><a href="#learning-skip-gram-embeddings">Learning Skip-gram Embeddings</a></li>
            <li><a href="#other-kinds-of-static-embeddings">Other kinds of static embeddings</a></li>
          </ol>
        </li>
        <li><a href="#visualizing-embeddings">Visualizing Embeddings</a></li>
        <li><a href="#semantic-properites-of-embeddings">Semantic Properites of Embeddings</a>
          <ol>
            <li><a href="#embeddings-and-historical-semantics">Embeddings and Historical Semantics</a></li>
          </ol>
        </li>
        <li><a href="#bias-and-embeddings">Bias and Embeddings</a></li>
        <li><a href="#evaluating-vector-models">Evaluating Vector Models</a></li>
      </ol>
    </li>
    <li><a href="#neural-networks">Neural Networks</a>
      <ol>
        <li><a href="#units">Units</a></li>
        <li><a href="#the-xor-problem">The XOR Problem</a>
          <ol>
            <li><a href="#the-solution-neural-networks">The Solution: Neural Networks</a></li>
          </ol>
        </li>
        <li><a href="#feedforward-neural-networks">Feedforward Neural Networks</a>
          <ol>
            <li><a href="#more-details-on-feedforward-networks">More Details on Feedforward Networks</a></li>
          </ol>
        </li>
        <li><a href="#feedforward-networks-for-nlp-classification">Feedforward Networks for NLP: Classification</a>
          <ol>
            <li><a href="#neural-net-classifiers-with-hand-built-features">Neural Net Classifiers with Hand-built Features</a></li>
            <li><a href="#vectorizing-for-parallelizing-inference">Vectorizing for Parallelizing Inference</a></li>
          </ol>
        </li>
        <li><a href="#embeddings-as-the-input-to-neural-net-classifiers">Embeddings as the Input to Neural Net Classifiers</a></li>
        <li><a href="#training-neural-nets">Training Neural Nets</a>
          <ol>
            <li><a href="#loss-function">Loss Function</a></li>
            <li><a href="#computing-the-gradient">Computing the Gradient</a></li>
            <li><a href="#computation-graphs">Computation Graphs</a></li>
            <li><a href="#backward-differentiation-on-computation-graphs">Backward Differentiation on Computation Graphs</a></li>
            <li><a href="#more-details-on-learning">More Details on Learning</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#large-language-models">Large Language Models</a>
      <ol>
        <li><a href="#three-architecture-for-language-models">Three Architecture for Language Models</a></li>
        <li><a href="#conditional-generation-of-text-the-intuition">Conditional Generation of Text: The Intuition</a></li>
        <li><a href="#prompting">Prompting</a></li>
        <li><a href="#generation-and-sampling">Generation and Sampling</a>
          <ol>
            <li><a href="#greedy-decoding">Greedy decoding</a></li>
            <li><a href="#random-sampling">Random Sampling</a></li>
            <li><a href="#temperature-sampling">Temperature Sampling</a></li>
          </ol>
        </li>
        <li><a href="#training-large-language-models">Training Large Language Models</a>
          <ol>
            <li><a href="#self-supervised-training-algorithm-for-pretraining">Self-supervised Training Algorithm for Pretraining</a></li>
            <li><a href="#pretraining-corpora-for-large-language-models">Pretraining Corpora for Large Language Models</a></li>
            <li><a href="#finetuning">Finetuning</a></li>
          </ol>
        </li>
        <li><a href="#evaluating-large-language-models">Evaluating Large Language Models</a>
          <ol>
            <li><a href="#perplexity">Perplexity</a></li>
            <li><a href="#downstream-tasks-reasoniong-and-world-knowledge">Downstream Tasks: Reasoniong and World Knowledge</a></li>
            <li><a href="#other-factors-for-evaluating-language-models">Other Factors for Evaluating Language Models</a></li>
          </ol>
        </li>
        <li><a href="#ethical-and-safety-issues-with-language-models">Ethical and Safety Issues with Language Models</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="background-color: #5DB9AE; color: #fff;">
                Â≠¶‰π†Êú≠ËÆ∞
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/2025/speech-and-language-processing/">‰π¶Á±çÈòÖËØª | Speech and Language Processing</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-08-28</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    ÈòÖËØªÊó∂Èïø: 23 ÂàÜÈíü
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="introduction">Introduction
</h2><p>ÈòÖËØª<a class="link" href="https://web.stanford.edu/~jurafsky/slp3/"  target="_blank" rel="noopener"
    >Speech and Language Processing</a>ËøôÊú¨‰π¶ÁöÑ‰∏Ä‰∫õÁ¨îËÆ∞„ÄÇ</p>
<h2 id="words-and-tokens">Words and Tokens
</h2><p>Êàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™‰∏úË•øÊù•Âª∫Ê®°ËØ≠Ë®ÄÔºå‰∏ãÈù¢ÊòØÊàë‰ª¨ÁöÑÈÄâÊã©Ôºö</p>
<h3 id="words">Words
</h3><p>‰∏∫‰ªÄ‰πà‰∏çÁî®ËØçÔºü</p>
<ul>
<li>Êúâ‰∫õËØ≠Ë®ÄÊ≤°Êúâorthographic words</li>
<li>ËØçÁöÑÊï∞Èáè‰ºöÈöèÁùÄÊñáÁ´†Â¢ûÈïøÔºåËØçÊ±áË°®Ê∞∏ËøúÈÉΩ‰ºöË¶ÜÁõñ‰∏çË∂≥</li>
</ul>
<h3 id="morphemes">Morphemes
</h3><p>ËØ≠Á¥†Á±ªÂûã</p>
<ul>
<li>Â±àÊäòËØ≠Á¥†Ôºöinflectional morphemes</li>
<li>Ê¥æÁîüËØ≠Á¥†Ôºöderivational morphemes</li>
<li>ÈôÑÁùÄËØ≠Á¥†Ôºöclitic</li>
</ul>
<p>ËØ≠Ë®ÄÁ±ªÂûã</p>
<ul>
<li>Analytic</li>
<li>polysynthetic</li>
<li>fusional</li>
<li>agglutinative</li>
</ul>
<p>‰∏∫‰ªÄ‰πà‰∏çÁî®ËØ≠Á¥†Ôºü</p>
<ul>
<li>ËØ≠Á¥†ÂæàÂ§çÊùÇÔºåÂæàÈöæÂÆö‰πâ</li>
<li>‰∏çÂêåËØ≠Ë®Ä‰∏çÂêå‰∏îÈöæ‰ª•Áªü‰∏Ä</li>
</ul>
<h3 id="unicode">Unicode
</h3><p>UnicodeÁöÑÂéÜÂè≤</p>
<ul>
<li>ASCII</li>
<li>CJKV</li>
<li>‰∏çÊñ≠Êõ¥Êñ∞‰∏≠ÔºåË∂äÊù•Ë∂äÂ§öÔºåUnicode 16.0Â∑≤ÁªèÂåÖÂê´Ë∂ÖËøá150000‰∏™Â≠óÁ¨¶</li>
</ul>
<h4 id="code-points">Code Points
</h4><ul>
<li>U+ÔºöË°®Á§∫Êé•‰∏ãÊù•Ë¶ÅÁî®UnicodeÂçÅÂÖ≠ËøõÂà∂Ë°®Á§∫‰∏Ä‰∏™code point</li>
<li>U+0061Ôºö0x0061‰∏Ä‰∏™ÊÑèÊÄùÔºå‰πüÂ∞±Â∞èÂÜôÂ≠óÊØça„ÄÇ</li>
</ul>
<h4 id="utf-8">UTF-8
</h4><p>ÁõÆÂâçÊúÄÂ∏∏Áî®ÁöÑencodingÂ≠óÁ¨¶ÁöÑÊñπÂºè„ÄÇ‰∏≠ÊñáÂ≠óÁ¨¶ ‚Äú‰∏≠‚Äù ÁöÑ Unicode Á†ÅÁÇπÊòØ<code>U+4E2D</code>ÔºåUTF-8 ÁºñÁ†ÅÂêé‰∏∫ 3 ‰∏™Â≠óËäÇÔºö<code>0xE4 0xB8 0xAD</code></p>
<p>UTF-8ÊòØ‰∏ÄÁßçÂèòÈïøÁºñÁ†ÅÔºåÂÖºÂÆπASCII„ÄÇ</p>
<ul>
<li>Â¶Ç„Äå‰∏ñ„ÄçÔºåUTF-8 ÁºñÁ†ÅÊòØ<code>0xE4 B8 96</code>ÔºåÂÖ∂‰∏≠E4ÁöÑ‰∫åËøõÂà∂‰∏∫<code>11100110H</code>ÔºåÂºÄÂ§¥ÁöÑ<code>1110H</code>Ë°®Á§∫ËøôÊòØ‰∏Ä‰∏™3Â≠óËäÇÂ≠óÁ¨¶ÁöÑÁ¨¨‰∏Ä‰∏™Â≠óËäÇ„ÄÇ</li>
</ul>
<h3 id="subword-tokenization-byte-pair-encoding">Subword Tokenization: Byte-Pair Encoding
</h3><p>‰∏äÈù¢ÁöÑ‰∏â‰∏™ÂÄôÈÄâÈÉΩ‰∏çË°åÔºåwordÂíåmorphemeÈöæ‰ª•ËßÑËåÉÂÆö‰πâÔºåcharacterÂèØ‰ª•ÈÄöËøáunicodeÊù•ÂÆö‰πâÔºå‰ΩÜÂèàÂØπ‰∫é‰Ωú‰∏∫tokensÊù•ËØ¥Â§™Â∞è‰∫Ü„ÄÇ</p>
<p>‰∏∫‰ªÄ‰πàË¶ÅtokenizeËæìÂÖ•Ôºü</p>
<ul>
<li>Â∞ÜËæìÂÖ•ËΩ¨Êç¢‰∏∫‰∏ÄÁªÑÁ°ÆÂÆöÁöÑ„ÄÅÂõ∫ÂÆöÁöÑÂçïÂÖÉÔºàTokenÔºâÔºåËÉΩËÆ©‰∏çÂêåÁöÑÁÆóÊ≥ïÂíåÁ≥ªÁªüÂú®‰∏Ä‰∫õÁÆÄÂçïÈóÆÈ¢ò‰∏äËææÊàêÂÖ±ËØÜ„ÄÇ‰æãÂ¶ÇÂõ∞ÊÉëÂ∫¶ÁöÑËÆ°ÁÆó„ÄÇ</li>
<li>ÂØπÂèØÂ§çÁé∞ÂæàÈáçË¶Å</li>
<li>‰∏∫‰∫ÜÊ∂àÈô§unknown wordsÁöÑÈóÆÈ¢ò</li>
</ul>
<p>‰∏∫‰∫ÜÊ∂àÈô§unknown wordsÈóÆÈ¢òÔºåÁé∞‰ª£tokenizersËá™Âä®ÂºïÂÖ•‰∫ÜtokenÂåÖÂê´ÈÇ£‰∫õÊØîwordsÂ∞èÁöÑtokenÔºåÂè´subword„ÄÇ</p>
<p>‰ΩøÁî®<a class="link" href="https://platform.openai.com/tokenizer"  target="_blank" rel="noopener"
    >Tokenizer - OpenAI API</a>‰∏≠ÁöÑ<code>GPT-4o &amp;  GPT-4o mini</code>Êù•ÂàÜËØç‰∏ãÈù¢Ëøô‰∏ÄÂ§ßÊÆµËØùÔºö</p>
<blockquote>
<p>For example, if we had happened not to ever see the word lower, when it appears we could segment it successfully into low and er which we had already seen. In the worst case, a really unusual word (perhaps an acronym like GRPO) could be tokenized as a sequence of individual letters if necessary.</p></blockquote>
<p>ÊúÄÁªàÂæóÂà∞ÁöÑÊòØ
<img src="/2025/speech-and-language-processing/assets/IMG-20250828205105982.png"
	width="711"
	height="279"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250828205105982_hu_b760c9dcae731844.png 480w, /2025/speech-and-language-processing/assets/IMG-20250828205105982_hu_3812253da13ed9c0.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="254"
		data-flex-basis="611px"
	
>
Áé∞Âú®ÊúÄÊµÅË°åÁöÑtokenization algorithmÊúâ‰∏§‰∏™Ôºö</p>
<ul>
<li>Byte-Pair Encoding(BPE)</li>
<li>Unigram Language modeling(ULM)</li>
</ul>
<h4 id="bpe">BPE
</h4><p>ÈÄöËøáÂàÜÊûêËÆ≠ÁªÉËØ≠ÊñôÔºåËá™Âä®Â≠¶‰π†Âá∫‰∏ÄÂ•óÂ≠êËØçÈõÜÂêàÔºàËØçÊ±áË°®ÔºâÔºå‰ΩøÂæóÈ´òÈ¢ëÂá∫Áé∞ÁöÑÂ≠óÁ¨¶ / Â≠êËØçÁªÑÂêàË¢´ÂêàÂπ∂‰∏∫Êõ¥Â§ßÁöÑÂ≠êËØçÂçï‰Ωç„ÄÇ</p>
<p>ËÆ≠ÁªÉÊñπÊ≥ï‰ªãÁªç„ÄÇ</p>
<h4 id="bpe-encoder">BPE encoder
</h4><h4 id="bpe-in-practice">BPE in practice
</h4><p>ÈÄöÂ∏∏ÔºåÊàë‰ª¨‰ºöÂØπ UTF-8 ÁºñÁ†ÅÊñáÊú¨ÁöÑ<strong>Âçï‰∏™Â≠óËäÇ</strong>ÊâßË°å BPE Êìç‰Ωú„ÄÇBPE Â§ÑÁêÜ ‚Äú‰∏≠‚Äù Êó∂ÔºåËæìÂÖ•Âπ∂Èùû<code>U+4E2D</code>Ëøô‰∏™Á†ÅÁÇπÔºåËÄåÊòØ<code>E4</code>„ÄÅ<code>B8</code>„ÄÅ<code>AD</code>Ëøô‰∏â‰∏™Áã¨Á´ãÂ≠óËäÇ„ÄÇ</p>
<p>‰ªÖÂú®<strong>È¢ÑÂÖàÂàáÂàÜÂá∫ÁöÑÂçïËØçÂÜÖÈÉ®</strong>ÊâßË°å BPE Êìç‰ΩúÔºåÊúâÂä©‰∫éÈÅøÂÖçÊΩúÂú®ÈóÆÈ¢ò„ÄÇ</p>
<p>‰∏Ä‰∫õËã±ËØ≠ÈáåÁöÑÂ∞èÂèëÁé∞Ôºö</p>
<ul>
<li>Â§ßÂ§öÊï∞ÂçïËØçÁöÑtokensÊòØ‰ªñ‰ª¨Ëá™Â∑±ÔºåÂåÖÂê´ËØçÂâçÁ©∫Ê†º„ÄÇËøôÊ†∑ÂèØ‰ª•ÈÅøÂÖçÁã¨Á´ãÂçïËØçÂíåÂçïËØçÂÜÖÈÉ®ÁöÑsubword„ÄÇ</li>
<li>ÈôÑÁùÄËØ≠Á¥†CliticsÂú®ÂêçÂ≠óÂêéÈù¢ÂàÜÂºÄÂçïÁã¨ÊàêtokenÔºå‰ΩÜÂú®Â∏∏ËßÅÁöÑËØçËØ≠ÂêéÈù¢‰ºöÊòØtokenÁöÑ‰∏ÄÈÉ®ÂàÜ</li>
<li>Êï∞Â≠óÈÄöÂ∏∏‰∏â‰Ωç‰∏ÄÁªÑ</li>
<li>‰∏Ä‰∫õËØçÔºåÂ¶ÇAnyhowÂíåanyhow‰ºöÊúâ‰∏çÂêåÁöÑÂàÜÂâ≤ÊñπÊ≥ï</li>
</ul>
<p>Ëøô‰∏™ÂíåÈ¢ÑÂ§ÑÁêÜÊúâÂÖ≥Á≥ª„ÄÇ</p>
<p>SuperBPE‰ºöÂêàÂπ∂Â∏∏ËßÑÁöÑBPEÂ≠êËØçÂàÜËØçÔºåÊïàÁéáÊõ¥È´ò„ÄÇ</p>
<p>ÁâπÂà´Âú∞Ôºå‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑtokensÊõ¥Á¢éÔºåÂ∞±‰ºöËæìÂá∫ËæπÈïøÔºåÊúÄÁªàLLMÁöÑÊïàÁéáÂèò‰Ωé„ÄÇ</p>
<h3 id="rule-based-tokenization">Rule-based tokenization
</h3><p>Penn Treebank Tokenization StandardÔºâÔºö‰∫ãÂÆûÊÄßËßÑËåÉ„ÄÇ</p>
<ul>
<li>ÂàÜÂºÄÈôÑÁùÄËØ≠Á¥†</li>
<li>‰øùÁïôËøûÂ≠óÁ¨¶ËøûÊé•ÁöÑËØç</li>
<li>ÂàÜÂºÄÊâÄÊúâÁöÑÊ†áÁÇπÁ¨¶Âè∑</li>
</ul>
<h4 id="sentence-segmentation">Sentence Segmentation
</h4><p>sentence tokenizationÂèØ‰ª•Âíåword tokenizationËÅîÂêàÂ§ÑÁêÜ„ÄÇ</p>
<h3 id="corpora">Corpora
</h3><p>ËØ≠ÊñôÂ∫ìÂíåËØ≠Ë®ÄÊï∞Èáè„ÄÅ‰ΩøÁî®ËÄÖÁöÑÁâπÂæÅÈÉΩÊúâÂÖ≥„ÄÇ</p>
<p>code switchingÔºöÂú®‰∏ÄÊ¨°ÊåÅÁª≠ÁöÑ‰∫§ÊµÅÔºâ‰∏≠ÔºåËØ¥ËØùËÄÖÊàñ‰ΩúËÄÖ‰∫§Êõø‰ΩøÁî®‰∏§ÁßçÊàñÂ§öÁßç ‚ÄúËØ≠Á†Å‚ÄùÁöÑÁé∞Ë±°„ÄÇ</p>
<p>datasheetÔºöÂ≠òÂÇ®‰∏ÄÂè•ËØùÁöÑÁâπÂæÅÔºåÂ¶ÇÊó∂Èó¥„ÄÅËØ¥ËØù‰∫∫ÊÄßÊ†º„ÄÅÈò∂Á∫ß&hellip;</p>
<h3 id="regular-expressions">Regular Expressions
</h3><p>Ê≠£ÂàôË°®ËææÂºèÁöÑÂÖ∑‰ΩìÂÆûÁé∞„ÄÇÂåÖÂê´Â≠óÁ¨¶ÊûêÂèñ„ÄÅËÆ°Êï∞„ÄÅÂèØÈÄâÊÄß„ÄÅÈÄöÈÖçÁ¨¶„ÄÅÈîöÁÇπÂíåËæπÁïå„ÄÅÊõøÊç¢ÂíåÊçïËé∑ÁªÑ„ÄÅÂâçÂêëÊñ≠Ë®ÄÁ≠â„ÄÇ</p>
<h3 id="simple-unix-tools-for-word-tokenization">Simple Unix Tools for Word Tokenization
</h3><p>ÂèØ‰ª•Âú®Unix„ÄÅLinuxÁ≥ªÁªü‰∏≠‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºè„ÄÇÂ¶Ç<code>tr -sc 'A-Za-z' '\n' &lt; sh.txt</code>Ë°®Á§∫‰ªé¬†<code>sh.txt</code>¬†Êñá‰ª∂‰∏≠ÊèêÂèñÊâÄÊúâËã±ÊñáÂ≠óÊØçÔºåÂπ∂Â∞ÜÈùûÂ≠óÊØçÂ≠óÁ¨¶ÊõøÊç¢‰∏∫Êç¢Ë°åÁ¨¶ÔºåÂêåÊó∂ÂéãÁº©ËøûÁª≠ÁöÑÈùûÂ≠óÊØçÂ≠óÁ¨¶‰∏∫Âçï‰∏™Êç¢Ë°åÁ¨¶„ÄÇ</p>
<h3 id="minimum-edit-distance">Minimum Edit Distance
</h3><p><strong>ÊúÄÂ∞èÁºñËæëË∑ùÁ¶ª</strong>ÔºöÂ∞Ü‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÈÄöËøá ‚ÄúÊèíÂÖ•‚Äù‚ÄúÂà†Èô§‚Äù‚ÄúÊõøÊç¢‚Äù ‰∏âÁßçÂü∫Êú¨Êìç‰ΩúËΩ¨Êç¢‰∏∫Âè¶‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÊâÄÈúÄÁöÑÊúÄÂ∞ëÊìç‰ΩúÊ¨°Êï∞</p>
<h4 id="the-minimum-edit-distance-algorithm">The Minimum Edit Distance Algorithm
</h4><p>‰∏Ä‰∏™ÁªèÂÖ∏ÁöÑÂä®ÊÄÅËßÑÂàíÈóÆÈ¢ò„ÄÇ</p>
<p><strong>Â≠óÁ¨¶ÂØπÈΩê</strong>ÔºöÈÄöËøáÂõûÊ∫ØÁºñËæëË∑ùÁ¶ªÁü©Èòµ‰∏≠ÁöÑ ‚ÄúÊúÄ‰ºòË∑ØÂæÑ‚ÄùÔºåÂèçÂêëÊé®ÂØºÂá∫Â∞Ü‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ËΩ¨Êç¢‰∏∫Âè¶‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÁöÑÂÖ∑‰ΩìÊìç‰ΩúÂ∫èÂàó„ÄÇ‰πüÂ∞±ÊòØ<strong>Ë∑ØÂæÑÂèØËßÜÂåñ</strong>„ÄÇ</p>
<h3 id="exercies">Exercies
</h3><h5 id="21">2.1
</h5><p>Write regular expressions for the following languages.</p>
<ol>
<li>The set of all alphabetic strings.</li>
<li>The set of all lowercase alphabetic strings ending in &ldquo;b&rdquo;.</li>
<li>The set of all strings from the alphabet {a, b} such that each &ldquo;a&rdquo; is immediately preceded by and immediately followed by a &ldquo;b&rdquo;.</li>
</ol>
<h5 id="22">2.2
</h5><p>Write regular expressions for the following languages. By &ldquo;word&rdquo;, we mean an alphabetic string separated from other words by whitespace, relevant punctuation, line breaks, etc.</p>
<ol>
<li>The set of all strings with two consecutive repeated words (e.g., &ldquo;Humbert Humbert&rdquo; and &ldquo;the the&rdquo; but not &ldquo;the bug&rdquo; or &ldquo;the big bug&rdquo;).</li>
<li>All strings that start at the beginning of the line with an integer and end at the end of the line with a word.</li>
<li>All strings that have both the word &ldquo;grotto&rdquo; and the word &ldquo;raven&rdquo; in them (but not, e.g., words like &ldquo;grottos&rdquo; that merely contain &ldquo;grotto&rdquo;).</li>
<li>Write a pattern that places the first word of an English sentence in a register. Deal with punctuation.</li>
</ol>
<h5 id="23">2.3
</h5><p>Implement an ELIZA-like program, using substitutions such as those described on page 27. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your program can legitimately engage in a lot of simple repetition.</p>
<h5 id="24">2.4
</h5><p>Compute the edit distance (using insertion cost 1, deletion cost 1, substitution cost 1) of &ldquo;leda&rdquo; to &ldquo;deal&rdquo;. Show your work (using the edit distance grid).</p>
<h5 id="25">2.5
</h5><p>Figure out whether &ldquo;drive&rdquo; is closer to &ldquo;brief&rdquo; or to &ldquo;divers&rdquo; and what the edit distance is to each. You may use any version of distance that you like.</p>
<h5 id="26">2.6
</h5><p>Now implement a minimum edit distance algorithm and use your hand-computed results to check your code.</p>
<h5 id="27">2.7
</h5><p>Augment the minimum edit distance algorithm to output an alignment; you will need to store pointers and add a stage to compute the backtrace.</p>
<h2 id="n-gram-language-models">N-gram Language Models
</h2><p>Êú¨Á´†‰ªãÁªçÊúÄÁÆÄÂçïÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºö<strong>NÂÖÉËØ≠Ê≥ïËØ≠Ë®ÄÊ®°Âûã</strong>„ÄÇ</p>
<h3 id="n-grams">N-Grams
</h3><p><strong>Ê¶ÇÁéáÈìæÂºèÊ≥ïÂàô</strong></p>
<h4 id="how-to-estimate-probabilities">How to estimate probabilities
</h4><p><strong>È©¨Â∞îÁßëÂ§´ÂÅáËÆæ</strong>ÔºöÂÅáËÆæ‰∏Ä‰∏™ÂçïËØçÁöÑÂá∫Áé∞Ê¶ÇÁéáÂè™ÂíåÂâçÈù¢ÁöÑ‰∏Ä‰∏™ÂçïËØçÊúâÂÖ≥„ÄÇÈÇ£‰πàn-gramÂç≥Âè™ÂíåÂâçÈù¢ÁöÑ$n-1$‰∏™ÂçïËØçÊúâÂÖ≥„ÄÇ</p>
<p><strong>ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°</strong>ÔºöÂ∑≤Áü•Ââç‰∏Ä‰∏™ËØç$w_{n‚àí1}$‚ÄãÊó∂ÔºåÂΩìÂâçËØç$w_n$‚ÄãÁöÑÊ¶ÇÁéá</p>
<p>ÁªàÊ≠¢Á¨¶Âè∑Ôºàend-symbolÔºâÔºöÊâÄÊúâÂèØËÉΩÂè•Â≠êÁöÑÊ¶ÇÁéáÊÄªÂíå‰∏∫ 1ÔºåÂê¶ÂàôÊòØÁâπÂÆöÈïøÂ∫¶ÁöÑÊâÄÊúâÂè•Â≠êÊ¶ÇÁéá‰πãÂíå‰∏∫ 1„ÄÇ</p>
<h4 id="dealing-with-scale-in-large-n-gram-models">Dealing with scale in large n-gram models
</h4><p>Log probabilities</p>
<p>NÂÖÉËØ≠Ê≥ïÁöÑËÆ°ÁÆóÁé∞Âú®ÁîöËá≥ËÉΩËææÂà∞Êó†ÈôêÂÖÉ„ÄÇ</p>
<p>ÂØπNÂÖÉËØ≠Ê≥ïÊ®°ÂûãËøõË°å‰øÆÂâ™‰πüÊòØÂæàÈáçË¶ÅÁöÑ„ÄÇ</p>
<h4 id="evaluating-language-models-training-and-test-sets">Evaluating Language Models: Training and Test Sets
</h4><p><strong>ÂÜÖÈÉ®ËØÑ‰º∞</strong>ÂíåÂ§ñÈÉ®ËØÑ‰º∞„ÄÇ</p>
<p>ËÆ≠ÁªÉÈõÜ„ÄÅÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜ„ÄÇ</p>
<h4 id="evaluating-language-models-perplexity">Evaluating Language Models: Perplexity
</h4><p><strong>PerplexityÔºàPPLÔºâ</strong>ÔºöÂõ∞ÊÉëÂ∫¶Ë∂ä‰ΩéÔºåËØ¥ÊòéÊ®°ÂûãÂØπÊñáÊú¨ÁöÑÈ¢ÑÊµãË∂äÂáÜÁ°ÆÔºàÂç≥Ê®°ÂûãË∂ä ‚Äú‰∏çÂõ∞ÊÉë‚ÄùÔºâ„ÄÇ</p>
<ul>
<li>ÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊòØ‚ÄúËÅîÂêàÊ¶ÇÁéáÂÄíÊï∞ÁöÑÂá†‰ΩïÂπ≥ÂùáÂÄº‚Äù„ÄÇ</li>
<li>Âú®ËÆ°ÁÆóÁöÑÊó∂ÂÄôÂ∏∏Â∏∏‰ºöÂèñÂØπÊï∞Êù•Â∞ÜÊ±Ç‰πòÁßØÂèò‰∏∫Ê±ÇÂíåÔºåÈÅøÂÖçÊï∞ÂÄºÈóÆÈ¢ò</li>
</ul>
<h5 id="perplexity-as-weighted-average-branching-factor">Perplexity as Weighted Average Branching Factor
</h5><p>Âõ∞ÊÉëÂ∫¶‰πüÂèØ‰ª•ÁêÜËß£‰∏∫<strong>Âä†ÊùÉÂπ≥ÂùáÂàÜÊîØÁ≥ªÊï∞</strong>„ÄÇÂÖ∂‰∏≠ÔºåËØ≠Ë®ÄÁöÑ ‚ÄúÂàÜÊîØÁ≥ªÊï∞‚ÄùÊåáÁöÑÊòØ ‚Äú‰ªª‰Ωï‰∏Ä‰∏™ËØç‰πãÂêéÂèØËÉΩÂá∫Áé∞ÁöÑ‰∏ã‰∏Ä‰∏™ËØçÁöÑÊï∞Èáè‚Äù„ÄÇ</p>
<h3 id="sampling-sentences-from-a-language-model">Sampling sentences from a language model
</h3><p>‚Äú0-1 Êï∞ËΩ¥ + Âå∫Èó¥Êò†Â∞Ñ‚ÄùÊù•ÁêÜËß£ÈááÊ†∑ÁöÑÂü∫Êú¨ÂéüÁêÜ„ÄÇ</p>
<h3 id="generalizing-vs-overfitting-the-training-set">Generalizing vs. overfitting the training set
</h3><p>ÂØπ‰∫éËééÂ£´ÊØî‰∫öÊñáÊú¨ÂíåÂçéÂ∞îË°óÊó•Êä•ÁöÑÊñáÊú¨Ôºå‰∏§ËÄÖÂ∑ÆÂºÇËøáÂ§ß‰ª•Ëá≥‰∫é‰∏çËÉΩÂàÜÂà´‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ„ÄÇ</p>
<p>ÊâÄ‰ª•ËØ¥Ë¶ÅÁ°Æ‰øùËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÁöÑÈ¢ÜÂüüË¶ÅÁõ∏‰ºº„ÄÇ</p>
<h3 id="smoothing-interpolation-and-backoff">Smoothing, Interpolation, and Backoff
</h3><p>zero probability n-gramsÊúâ‰∏§‰∏™ÈóÆÈ¢òÔºö</p>
<ul>
<li>‰Ωé‰º∞‰∫ÜËØçËØ≠Â∫èÂèØËÉΩÂá∫Áé∞ÁöÑÂèØËÉΩÊÄßÔºåÂØºËá¥ÊúÄÁªàÁöÑÊÄßËÉΩÂèòÂ∑Æ</li>
<li>Âõ∞ÊÉëÂ∫¶Êó†Ê≥ïËÆ°ÁÆóÔºåÂõ†‰∏∫Êó†Ê≥ïÈô§‰ª•0</li>
</ul>
<p>Âõ†Ê≠§ÈúÄË¶ÅSmoothingÊàñËÄÖdiscounting</p>
<h4 id="laplace-smoothing">Laplace Smoothing
</h4><p>ÂÖ∂ÂÆû‰πüÂ∞±ÊòØadd one smoothingÔºåÂ∞±ÊòØÂØπ‰∫éÊâÄÊúâÁöÑNÂÖÉËØ≠Ê≥ïÈÉΩÂä†‰∏Ä„ÄÇ</p>
<p>ÂØπ‰∫éËØ≠Ë®ÄÊ®°ÂûãÊù•ËØ¥ÔºåÁªìÊûúÂπ∂‰∏çÊòØÂæàÂ•Ω„ÄÇÂØπÊñáÊú¨ÂàÜÁ±ªÊúâÊïà„ÄÇ</p>
<h4 id="add-k-smoothing">Add-k Smoothing
</h4><p>‰πüÂ∞±ÊòØÂØπÊâÄÊúâÁöÑÈÉΩÂä†K„ÄÇ</p>
<p>ÂØπËØ≠Ë®ÄÊ®°ÂûãÊù•ËØ¥‰ªçÁÑ∂ÊïàÊûú‰∏ÄËà¨„ÄÇ</p>
<h4 id="language-model-interpolation">Language Model Interpolation
</h4><p><strong>n ÂÖÉËØ≠Ê≥ïÊèíÂÄºÊ≥ïÔºöÂä†ÊùÉËûçÂêà‰∏çÂêåÈò∂Êï∞ n ÂÖÉËØ≠Ê≥ïÁöÑÊ¶ÇÁéá</strong>ÔºåÈÅøÂÖçÈ´òÈò∂ÁöÑnÂÖÉËØ≠Ê≥ïÈõ∂Ê¶ÇÁéáÂØºËá¥ÁöÑÈ¢ÑÊµãÂ§±Êïà„ÄÇ</p>
<p>Âä†ÊùÉÁöÑ$\lambda$Â∫îËØ•ËÆæÁΩÆÊàêÂ§öÂ∞ëÂë¢ÔºüÂèØ‰ª•‰ªéÈ¢ÑÁïôÈõÜheld-out corpus‰∏≠Â≠¶‰π†„ÄÇ‰ΩøÁî®EMÔºàÊúüÊúõÊúÄÂ§ßÂåñÔºâÁÆóÊ≥ïÊù•Â≠¶‰π†„ÄÇ</p>
<h4 id="stupid-backoff">Stupid Backoff
</h4><p><strong>ÂõûÈÄÄÊ®°Âûã</strong>ÔºöÈ´òÈò∂nÈò∂ÁöÑÊ®°ÂûãÊó†Ê≥ï‰ΩøÁî®ÁöÑÊó∂ÂÄôÔºåÂõûÈÄÄÂà∞‰ΩéÈò∂Ê®°Âûã„ÄÇ</p>
<p><strong>Discount</strong>ÔºöË¶ÅËÆ©ÂõûÈÄÄÊ®°ÂûãÔºàbackoff modelÔºâËæìÂá∫ÂêàÁêÜÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÊàë‰ª¨ÂøÖÈ°ªÂØπÈ´òÈò∂ n ÂÖÉËØ≠Ê≥ïÁöÑÊ¶ÇÁéáËøõË°å ‚ÄúÊäòÊâ£Â§ÑÁêÜ‚ÄùÔºàdiscountÔºâÔºå‰ªéËÄåÈ¢ÑÁïôÂá∫ÈÉ®ÂàÜÊ¶ÇÁéá‰ΩôÈáèÔºàprobability massÔºâÔºå‰æõ‰ΩéÈò∂ n ÂÖÉËØ≠Ê≥ï‰ΩøÁî®„ÄÇ‰ΩÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Ôºå‰∫∫‰ª¨Â∏∏‰ΩøÁî®‰∏ÄÁßçÊõ¥ÁÆÄÂçïÁöÑ ‚ÄúÊó†ÊäòÊâ£ÂõûÈÄÄÁÆóÊ≥ï‚Äù‚Äî‚Äî Âç≥Âêç‰∏∫<strong>Stupid Backoff</strong>„ÄÇ</p>
<h3 id="advanced-perplexitys-relation-to-entropy">Advanced: Perplexity&rsquo;s Relation to Entropy
</h3><p><strong>ÁÜµ</strong>Ôºö‰∏çÁ°ÆÂÆöÊÄßÁöÑÂ∫¶ÈáèÊñπÂºè„ÄÇÂèØ‰ª•ÁêÜËß£ÊòØÁºñÁ†ÅÊüê‰∏™ÂÜ≥Á≠ñÊàñÊüêÊù°‰ø°ÊÅØÊâÄÈúÄÁöÑÊúÄÂ∞èÂπ≥ÂùáÊØîÁâπÊï∞„ÄÇË∂ä‰∏çÁ°ÆÂÆöÔºåÁÜµË∂äÂ§ß„ÄÇ</p>
<p><strong>ÁÜµÁéá</strong>ÔºöÂπ≥ÂùáÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÁÜµÁéáÂÆö‰πâ‰∏∫ ‚Äú<strong>Êó†ÈôêÈïøÂ∫èÂàó‰∏≠ÔºåÊØè‰∏™ËØçÁöÑÂπ≥ÂùáÁÜµ</strong>‚ÄùÔºåÂèçÊò†ËØ≠Ë®ÄÁöÑÈïøÊúü‰∏çÁ°ÆÂÆöÊÄß„ÄÇ‰æãÂ¶ÇÔºåËã±ÊñáÁöÑÁÜµÁéáÁ∫¶ 1-2 ÊØîÁâπ / ËØçÔºåÊÑèÂë≥ÁùÄÂπ≥ÂùáÊØè‰∏™ËØçÈúÄË¶Å 1-2 ÊØîÁâπÊù•ÁºñÁ†Å„ÄÇ</p>
<p><strong>Âπ≥Á®≥ÊÄß</strong>ÔºöÂ∫èÂàóÊ¶ÇÁéá‰∏çÈöèÁùÄÊó∂Èó¥ÊîπÂèò„ÄÇËá™ÁÑ∂ËØ≠Ë®Ä‰∏çÊòØÔºå‰ΩÜÊòØNÂÖÉËØ≠Ê≥ïÊòØÂπ≥Á®≥ÁöÑ„ÄÇ</p>
<p><strong>ÈÅçÂéÜÊÄß</strong>ÔºöÈïøÂ∫èÂàó‰∏≠ÂåÖÂê´‰∫ÜÊâÄÊúâÁöÑÁü≠Â∫èÂàó„ÄÇ</p>
<p><strong>Shannon-McMillan-Breiman theorem</strong>ÔºöÂ¶ÇÊûúËØ≠Ë®ÄÊª°Ë∂≥Êüê‰∫õÊ≠£ÂàôÊù°‰ª∂ÔºàÂáÜÁ°ÆÂú∞ËØ¥ÔºåÊòØÂπ≥Á®≥‰∏îÈÅçÂéÜÁöÑÔºâÔºå<strong>Â∫èÂàóÈïøÂ∫¶Ë∂ãËøë‰∫éÊó†Á©∑Â§ßÊó∂Ôºå‚ÄúÂ∫èÂàóÁöÑÂπ≥ÂùáÂØπÊï∞Ê¶ÇÁéáÁöÑË¥üÂÄº‚Äù ÔºåÂç≥ÁªèÈ™åÁÜµÁéá‰ºö‰ª•Ê¶ÇÁéá1Êî∂ÊïõÊïõÂà∞ËØ•ËøáÁ®ãÁöÑÁêÜËÆ∫ÁÜµÁéá</strong>„ÄÇ</p>
<p><strong>‰∫§ÂèâÁÜµÔºàCross-EntropyÔºâ</strong>ÔºöÊàë‰ª¨ËôΩÁÑ∂‰∏çÁü•ÈÅìÊï∞ÊçÆÁöÑÁúüÂÆûÊ¶ÇÁéáÂàÜÂ∏ÉpÔºå‰ΩÜÊòØÂèØ‰ª•Áî®Ê®°ÂûãmÊù•Ëøë‰ººp„ÄÇÔºàÂç≥Êàë‰ª¨ËôΩÁÑ∂‰∏çÁü•ÈÅìËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÁúüÂÆûÊÉÖÂÜµÔºå‰ΩÜÊòØÂèØ‰ª•Áî®NÂÖÉËØ≠Ê≥ïÊù•Ëøë‰ºº„ÄÇÔºâ<strong>‰∫§ÂèâÁÜµË∂äÂ∞èÔºåÊ®°ÂûãË∂äÊé•ËøëÁúüÂÆûÂàÜÂ∏É</strong>„ÄÇ</p>
<p><strong>Âõ∞ÊÉëÂ∫¶</strong>Ôºö<strong>Âõ∞ÊÉëÂ∫¶ÊòØÁÜµÁöÑÊåáÊï∞ÂΩ¢Âºè</strong>„ÄÇÊØîËæÉÁõ¥ËßÇ„ÄÇ</p>
<h3 id="excercies">Excercies
</h3><h5 id="31">3.1
</h5><p>Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 40.</p>
<h5 id="32">3.2
</h5><p>Calculate the probability of the sentence <code>i want chinese food</code>. Give two probabilities, one using Fig. 3.2 and the ‚Äòuseful probabilities‚Äô just below it on page 42, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(i|&lt;s&gt;) = 0.19$ and $P(&lt;/s&gt;|food) = 0.40$.</p>
<h5 id="33">3.3
</h5><p>Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why.</p>
<h5 id="34">3.4
</h5><p>We are given the following corpus, modified from the one in the chapter:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; Sam I am &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;
</span></span></code></pre></div><p>Using a bigram language model with add-one smoothing, what is $P(Sam | am)$? Include $&lt;s&gt;$ and $&lt;/s&gt;$ in your counts just like any other token.</p>
<h5 id="35">3.5
</h5><p>Suppose we didn‚Äôt use the end-symbol $&lt;/s&gt;$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $&lt;/s&gt;$:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; a b  
</span></span><span class="line"><span class="cl">&lt;s&gt; b b  
</span></span><span class="line"><span class="cl">&lt;s&gt; b a  
</span></span><span class="line"><span class="cl">&lt;s&gt; a a
</span></span></code></pre></div><p>Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet a,b is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet a,b is also 1.0.</p>
<h5 id="36">3.6
</h5><p>Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating $P(w3|w1,w2)$, where $w3$ is a word which follows the bigram$ (w1,w2)$, in terms of various n-gram counts and V. Use the notation $c(w1,w2,w3)$ to denote the number of times that trigram $(w1,w2,w3)$ occurs in the corpus, and so on for bigrams and unigrams.</p>
<h5 id="37">3.7
</h5><p>We are given the following corpus, modified from the one in the chapter:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; Sam I am &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;
</span></span></code></pre></div><p>If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $Œª‚ÇÅ = 1/2$ and $Œª‚ÇÇ = 1/2,$ what is $P(Sam|am)$? Include $&lt;s&gt;$ and $&lt;/s&gt;$ in your counts just like any other token.</p>
<h5 id="38">3.8
</h5><p>Write a program to compute unsmoothed unigrams and bigrams.</p>
<h5 id="39">3.9
</h5><p>Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?</p>
<h5 id="310">3.10
</h5><p>Add an option to your program to generate random sentences.</p>
<h5 id="311">3.11
</h5><p>Add an option to your program to compute the perplexity of a test set.</p>
<h5 id="312">3.12
</h5><p>You are given a training set of 100 numbers that consists of 91 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?</p>
<h2 id="logistic-regression-and-text-classification">Logistic Regression and Text Classification
</h2><p>ÁªèÂÖ∏‰ªªÂä°Ôºö</p>
<ul>
<li>sentiment analysis</li>
<li>spam detection</li>
<li>language id</li>
<li>authorship attribution</li>
</ul>
<h3 id="machine-learning-and-classification">Machine Learning and Classification
</h3><ul>
<li>‰∫∫Â∑•ËßÑÂàôÂæàËÑÜÂº±ÔºåÊï∞ÊçÆ‰∏ÄÂèòÂåñÂ∞±Êó†Ê≥ï‰ΩøÁî®</li>
<li>LLMÁöÑÂº±ÁÇπÔºöÂπªËßâ„ÄÅÊó†Ê≥ïËß£Èáä„ÄÇ</li>
</ul>
<p>Âõ†Ê≠§ÊúÄÂ∏∏ËßÅÁöÑÂàÜÁ±ªÊñπÊ≥ïÊòØ<strong>ÊúâÁõëÁù£Êú∫Âô®Â≠¶‰π†</strong>„ÄÇ</p>
<ul>
<li>Ê¶ÇÁéáÂàÜÁ±ªÂô®ÔºöËæìÂá∫<strong>Ê†∑Êú¨Â±û‰∫éÊØè‰∏™Á±ªÂà´ÁöÑÊ¶ÇÁéá</strong>ËÄå‰∏çÊòØÁ±ªÂà´Ê†áÁ≠æÔºå‰øùËØÅÂú®ÂêàÂπ∂ÁöÑÁ≥ªÁªüÈáå‰∏çËøáÊó©Âú∞ËæìÂá∫ÁªìÊûú„ÄÇ</li>
</ul>
<p>ÂàÜÁ±ªÂô®ÁöÑÊ†∏ÂøÉÁªÑ‰ª∂Ôºö</p>
<ul>
<li>A feature representation of the input</li>
<li>A classificaition function that computes $\hat{y}$</li>
<li>An objective funcion that we want to potimize for learning
<ul>
<li>loss function</li>
</ul>
</li>
<li>An algorithm for optimizing the objective function
<ul>
<li>stochastic gradient descent algorithm</li>
</ul>
</li>
</ul>
<h3 id="the-sigmoid-function">The Sigmoid Function
</h3><p>‰∫åÂàÜÁ±ªÈÄªËæëÂõûÂΩíÁöÑÁõÆÊ†áÊòØÔºöËÆ°ÁÆóÊ†∑Êú¨Â±û‰∫éÊ≠£Á±ªÁöÑÊ¶ÇÁéá„ÄÇ</p>
<ul>
<li>Á¨¨‰∏ÄÊ≠•ÔºöËÆ°ÁÆóÁ∫øÊÄßÂæóÂàÜ$z=w‚ãÖx+b$ÔºåÂÄºÂüü‰∏∫$[-\infty, +\infty ]$</li>
<li>Á¨¨‰∫åÊ≠•Ôºö<strong>ÈÄöËøá Sigmoid ÂáΩÊï∞ËΩ¨Êç¢‰∏∫Ê¶ÇÁéá</strong>Ôºö  Â∞ÜÁ∫øÊÄßÂæóÂàÜ¬†z¬†Êò†Â∞ÑÂà∞¬†$[0,1]¬†$Âå∫Èó¥</li>
</ul>
<p>$z$Â∏∏Â∏∏Ë¢´Áß∞‰ΩúLogitÔºàÂØπÊï∞Âá†ÁéáÔºâ„ÄÇLogitÂ∞±ÊòØSigmoidÁöÑÂèçÂáΩÊï∞„ÄÇÂèØ‰ª•ÊèêÈÜíÊàë‰ª¨ÂêéÁª≠Ë¶ÅÂä†‰∏äSigmoidËøõË°åËΩ¨Êç¢ÔºåÂõ†‰∏∫$z$Âπ∂‰∏çÊòØ‰∏Ä‰∏™ÁúüÂÆûÁöÑÂÄº„ÄÇ</p>
<p>ÁâπÂà´Âú∞Ôºå<strong>‚ÄúÊ≠£Á±ªÁöÑÂØπÊï∞Âá†Áéá‚Äù ‰∏éÁâπÂæÅÂëàÁ∫øÊÄßÂÖ≥Á≥ª</strong>„ÄÇ‰πüÂ∞±ÊòØÂΩìÂÖ∂‰ªñÊù°‰ª∂‰∏çÂèòÊó∂ÔºåÁâπÂæÅ$x_1$ÊØèÂ¢ûÂä†1ÔºåLogitÂ∞±Â¢ûÂä†$w_1$„ÄÇËøôÈùûÂ∏∏ÊúâÂèØËß£ÈáäÊÄß„ÄÇ</p>
<h3 id="classification-with-logistic-regression">Classification with Logistic Regression
</h3><p>ÂΩìÊ¶ÇÁéáÂ§ß‰∫é0.5ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊääÂÆÉÂàÜÁ±ªÂà∞Ê≠£Á±ªÈáå„ÄÇ</p>
<h4 id="sentiment-classification">Sentiment Classification
</h4><p>‰∏æ‰∫Ü‰∏Ä‰∏™‰æãÂ≠ê„ÄÇ</p>
<h4 id="other-classification-tasks-and-features">Other Classification Tasks and Features
</h4><p>Period disambiguationÔºöÁ°ÆÂÆöÂè•Âè∑ÊòØEOSËøòÊòØÂÖ∂‰ªñ„ÄÇ</p>
<p><strong>Designing v.s. Learning featuresÔºö</strong></p>
<ul>
<li>ÂàöÂàöÁöÑ‰æãÂ≠êÔºåÁâπÂæÅÈÉΩÊòØ‰∫∫Â∑•ËÆæËÆ°ÁöÑ„ÄÇÊ≠§Â§ñËøòÊúâÔºö
<ul>
<li>feaure interactionsÔºöÂü∫Á°ÄÁâπÂæÅÁªÑÂêàÊàêÁöÑÂ§çÊùÇÁâπÂæÅ</li>
<li>feature templatesÔºöÊäΩË±°ÁöÑÁâπÂæÅËßÑËåÉÊù•ÂÆö‰πâÁâπÂæÅ„ÄÇËøôÈáåÁöÑÁâπÂæÅÁ©∫Èó¥ÊòØÁ®ÄÁñèÁöÑÔºåÊ≠§Â§ñÁâπÂæÅ‰∏ÄËà¨ÊòØÂ≠óÁ¨¶‰∏≤ÊèèËø∞ÁöÑHashÂÄº„ÄÇ</li>
</ul>
</li>
<li>‰∫∫Â∑•ËÆæËÆ°Â§™Â§çÊùÇ‰∫Ü„ÄÇÂõ†Ê≠§Áé∞‰ª£ÁöÑNLPÁ≥ªÁªüÈÉΩÊòØÁî®Representation LearningÊù•Ëß£ÂÜ≥„ÄÇ</li>
</ul>
<p>standardizeÂíånormalize„ÄÇ</p>
<h4 id="processing-many-examples-at-once">Processing many examples at once
</h4><p>Â¶ÇÊûúÊúâËÆ∏Â§öÁöÑÂÄºË¶ÅËÆ°ÁÆóÔºåÂèØ‰ª•‰ΩøÁî®matrix arithmeticÊù•‰∏ÄÊ¨°ËÆ°ÁÆóÂÆå„ÄÇ</p>
<h3 id="multinomial-logistic-regression">Multinomial Logistic Regression
</h3><p>Â§öÈ°πÈÄªËæëÂõûÂΩí‰πüÁß∞softmax regressionÔºåËÄÅÁöÑÊïôÊùê‰∏ä‰πüÂè´maxent clasifier„ÄÇ</p>
<p>Âú®Â§öÈ°πÈÄªËæëÂõûÂΩí‰∏≠ÔºåÁõ¥Êé•ËæìÂá∫ÁªìÊûúËÄå‰∏çÊòØ‰∏Ä‰∏™Ê¶ÇÁéáÂÄº„ÄÇ</p>
<ul>
<li>hard classification</li>
</ul>
<h4 id="softmax">Softmax
</h4><p>SigmoidÂáΩÊï∞Âú®Â§öÂàÜÁ±ªÊÉÖÂÜµ‰∏ãÁöÑÊé®Âπø„ÄÇ</p>
<h4 id="applying-softmax-in-logistic-regression">Applying Softmax in Logistic Regression
</h4><p>ÂèØ‰ª•‰ΩøÁî®Áü©ÈòµËøêÁÆóÊñπÂºèÂä†Âø´ËÆ°ÁÆó„ÄÇ</p>
$$\hat{y}=softmax(Wx+b)$$<blockquote>
<p><a class="link" href="https://arxiv.org/abs/2506.11035"  target="_blank" rel="noopener"
    >Doumbouya et al., 2025</a>ÊòØËøô‰πàËÆ§‰∏∫ÁöÑÔºöÈÄªËæëÂõûÂΩíÂ∞ÜÁü©ÈòµÁöÑÊØè‰∏ÄË°å¬†$w_k$ËßÜ‰∏∫<strong>Á¨¨¬†$k$¬†Á±ªÁöÑÂéüÂûãÔºàprototypeÔºâ</strong>ÔºåÁî±‰∫é‰∏§‰∏™ÂêëÈáèÁöÑÁõ∏‰ººÂ∫¶Ë∂äÈ´òÔºåÂÆÉ‰ª¨ÁöÑÁÇπÁßØÔºàdot productÔºâÂÄºÂ∞±Ë∂äÂ§ßÔºåÂõ†Ê≠§ÁÇπÁßØÂèØ‰Ωú‰∏∫Ë°°ÈáèÂêëÈáèÁõ∏‰ººÂ∫¶ÁöÑÂáΩÊï∞„ÄÇÊ®°ÂûãÊúÄÁªàÂ∞ÜËæìÂÖ•ÂàÜÈÖçÁªôÁõ∏‰ººÂ∫¶ÊúÄÈ´òÁöÑÁ±ªÂà´„ÄÇ</p></blockquote>
<h4 id="features-in-multinomial-logistic-regression">Features in Multinomial Logistic Regression
</h4><p>ÁâπÂæÅÊùÉÈáçÂêåÊó∂‰æùËµñ‰∫éËæìÂÖ•ÊñáÊú¨ÂíåËæìÂá∫Á±ªÂà´„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250830163440597.png"
	width="744"
	height="837"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830163440597_hu_dcd37b06fe097e65.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830163440597_hu_87f8d0d9a05a6baf.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="88"
		data-flex-basis="213px"
	
></p>
<h3 id="learning-in-logistic-regression">Learning in Logistic Regression
</h3><p>ÈÄªËæëÂõûÂΩíÊòØÂ¶Ç‰ΩïÂÆûÁé∞Â≠¶‰π†ÁöÑÔºü</p>
<ul>
<li>‰Ωøsystem outputÔºàclassifier outputÔºâÂíågold outputÔºàcorrect outputÔºâË∂äÊé•ËøëË∂äÂ•Ω„ÄÇ‰∏§ËÄÖ‰πãÈó¥ÁöÑË∑ùÁ¶ªÂèØ‰ª•Áß∞‰Ωú<strong>ÊçüÂ§±ÂáΩÊï∞</strong>ÊàñËÄÖ<strong>‰ª£‰ª∑ÂáΩÊï∞</strong>„ÄÇ‰∏ãÈù¢‰ªãÁªç‰∫§ÂèâÁÜµ„ÄÇ</li>
<li>ÈúÄË¶Å‰∏Ä‰∏™ÁÆóÊ≥ïÊù•ÊúÄÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞„ÄÇ‰∏ãÈù¢‰ªãÁªçÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ï„ÄÇ</li>
</ul>
<h3 id="the-cross-entropy-loss-function">The Cross-entropy Loss Function
</h3><p>Êù°‰ª∂ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºöÂú®ÁªôÂÆö$x$‰∏ãÔºåÈÄâÊã©ÂèÇÊï∞$w$Âíå$b$‰ΩøÂæó$y$ÁöÑÂØπÊï∞Ê¶ÇÁéáÊúÄÂ§ß„ÄÇ</p>
<p>ËøôÈáåÊçüÂ§±ÂáΩÊï∞ÊòØ<strong>Ë¥üÂØπÊï∞‰ººÁÑ∂ÊçüÂ§±Ôºànegative log likelihood lossÔºâ</strong>ÔºåÈÄöÂ∏∏‰πüË¢´Áß∞‰∏∫<strong>‰∫§ÂèâÁÜµÊçüÂ§±Ôºàcross-entropy lossÔºâ</strong>„ÄÇ</p>
<p>‰ªãÁªç‰∫Ü‰∏Ä‰∏ã‰∏∫‰ªÄ‰πà<strong>ÊúÄÂ∞èÂåñ‰∫§ÂèâÁÜµÊçüÂ§±ÂèØ‰ª•‰ΩøÂæóÁúüÂÆûÂàÜÂ∏ÉÂíåÈ¢ÑÊµãÂàÜÂ∏ÉÊõ¥Âä†Êé•Ëøë</strong>„ÄÇ</p>
<h3 id="gradient-descent">Gradient Descent
</h3><p>Ê¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÁöÑÂéüÁêÜ„ÄÇ‰ªãÁªç‰∫ÜÊ¢ØÂ∫¶„ÄÅÂ≠¶‰π†Áéá„ÄÇ</p>
<h4 id="the-gradient-for-logistic-regression">The Gradient for Logistic Regression
</h4><p>ÈÄªËæëÂõûÂΩíÁöÑÊ¢ØÂ∫¶Â∞±ÊòØ
</p>
$$\frac{\partial L_{\mathrm{CE}}(\hat{y},y)}{\partial w_{j}}=-(y-\hat{y})x_{j}$$<p>
‰πüÂ∞±ÊòØÈ¢ÑÊµãÂÄº$\hat{y}$ÂíåÂÆûÈôÖÂÄº$y$‰πãÈó¥ÁöÑÂ∑Æ‰πòËæìÂÖ•ÂÄº$x_j$„ÄÇ</p>
<h4 id="the-stochastic-gradient-descent-algorithm">The Stochastic Gradient Descent Algorithm
</h4><p>ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÊòØ‰∏ÄÁßçÂú®Á∫øÁÆóÊ≥ïÔºåÂèØ‰ª•ËæπÊé•Êî∂Êï∞ÊçÆËæπÂ≠¶‰π†„ÄÇ</p>
<p>SGDÊØèÊ¨°Áî®<strong>Âçï‰∏™ÈöèÊú∫Ê†∑Êú¨</strong>ËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ</p>
<h4 id="mini-batch-training">Mini-batch Training
</h4><p>batch trainingÂíåmini-batch trainingÁöÑÂå∫Âà´„ÄÇ</p>
<ul>
<li>batch gradientÔºöÊâÄÊúâÁöÑÈöèÊú∫Ê†∑Êú¨ËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ</li>
<li>mini-batch gradientÔºöÂ∞èÊâπÈáèÊ¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ï„ÄÇÊØèÊ¨°ÈÄâÊã©<strong>‰∏ÄÂ∞èÊâπÈöèÊú∫Ê†∑Êú¨</strong>ËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ</li>
</ul>
<h3 id="learning-in-multinomial-logistic-regression">Learning in Multinomial Logistic Regression
</h3><p>Â§öÈ°πÂºèÈÄªËæëÂõûÂΩíÂÖ∂ÂÆûÂíå‰∫åÈ°πÂºèÈÄªËæëÂõûÂΩíÂ∑Æ‰∏çÂ§ö„ÄÇ</p>
<ul>
<li>Êú¨Ë¥®ÊòØ‰ΩøÁî®Áã¨ÁÉ≠Ê†áÁ≠æ+Ê¶ÇÁéáÂêëÈáèÁöÑÂΩ¢ÂºèËøõË°åËÆ°ÁÆó„ÄÇ</li>
<li>Ê†∏ÂøÉÊòØ ‚ÄúÂØπÊ≠£Á°ÆÁ±ªÂà´ÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂèñË¥üÂØπÊï∞‚ÄùÔºåÂæóÂà∞‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÂÖ∂Ë∂äÂ∞èÂàôÈ¢ÑÊµãÊ¶ÇÁéáË∂äÈ´ò„ÄÇ</li>
</ul>
<h3 id="evaluation-precision-recall-f-measure">Evaluation: Precision, Recall, F-measure
</h3><ul>
<li>confusion matrix</li>
<li>accuracy</li>
<li>precision</li>
<li>recall</li>
<li>F-measure
<ul>
<li>F1</li>
<li>a weighted harmonic mean of precision and recall.</li>
</ul>
</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250830172049893.png"
	width="928"
	height="394"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830172049893_hu_5b835770261a7097.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830172049893_hu_9163edd8010643ed.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="235"
		data-flex-basis="565px"
	
></p>
<p>microaveraging v.s. macroaveraging</p>
<ul>
<li>ÂæÆËßÇÂπ≥ÂùáÔºöÊõ¥ÂÖ≥Ê≥® ‚ÄúÊï¥‰ΩìÊ†∑Êú¨ÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄß‚ÄùÔºå<strong>Â∞ëÊï∞Á±ªÈîôÂà§‰ª£‰ª∑‰Ωé‰∫éÂ§öÊï∞Á±ª</strong></li>
<li>ÂÆèËßÇÂπ≥ÂùáÔºöÊõ¥ÂÖ≥Ê≥®ÊâÄÊúâÁöÑÁ±ªÁöÑÈîôÂà§‰ª£‰ª∑ÁöÑÂÖ¨Âπ≥Ôºå<strong>Â∞ëÊï∞Á±ªÂíåÂ§öÊï∞Á±ªÁöÑ‰ª£‰ª∑Áõ∏Á≠â</strong>
<img src="/2025/speech-and-language-processing/assets/IMG-20250830173257005.png"
	width="1084"
	height="468"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830173257005_hu_bca6ffdfb72f8971.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830173257005_hu_899e55b55133737a.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="231"
		data-flex-basis="555px"
	
></li>
</ul>
<h3 id="test-sets-and-cross-validation">Test sets and Cross-validation
</h3><p>Cross-validationÔºöËß£ÂÜ≥ÊµãËØïÈõÜ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇ</p>
<ul>
<li>Âõ∫ÂÆöËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ„ÄÇ</li>
<li>ËÆ≠ÁªÉÈõÜ‰∏≠ËøõË°åÂàÜÂâ≤„ÄÇ
<img src="/2025/speech-and-language-processing/assets/IMG-20250830173844382.png"
	width="895"
	height="462"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830173844382_hu_4558bdf3b4166cbe.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830173844382_hu_e53d98bd5ba967c8.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="193"
		data-flex-basis="464px"
	
></li>
</ul>
<h3 id="statistical-significance-testing">Statistical Significance Testing
</h3><p><strong>ÁªüËÆ°ÊòæËëóÊÄßÊ£ÄÈ™å</strong></p>
<ul>
<li>‰∏çÂè™ÊòØÁÆÄÂçïÂú∞Ê£ÄÊü•AÂú®ÊµãËØïÈõÜ‰∏äÁöÑÁªìÊûú$M(A,x)$Â•Ω‰∫éBÂú®ÊµãËØïÈõÜ‰∏äÁöÑÁªìÊûú$M(B,x)$„ÄÇÂ¶ÇÊûúÂ∑ÆÂæàÂ∞èÁöÑËØùÔºåÂÖ∂ÂÆû‰∏ç‰∏ÄÂÆöËÉΩËØÅÊòéAÁöÑÁªìÊûúÊØîBÂ∞èÔºå‰∏çÂÖ∑ÊúâÁªüËÆ°Â≠¶‰∏äÁöÑÊòæËëóÊÄß„ÄÇ</li>
<li>ËÆæËÆ°‰∏Ä‰∏™ÊïàÂ∫îÈáè$\delta (x)=M(A,x)-M(B,x)$ÔºåÂéüÂÅáËÆæÊòØ$H_0 :\delta (x)\leq 0$„ÄÇËøôÊ†∑ËÆ°ÁÆópÂÄºÊòØÂê¶Â∞è‰∫éÈòàÂÄºÂèØ‰ª•ÂæóÂá∫ÊòØÂê¶ÊòæËëóÊÄßÂú∞AÊØîBË¶ÅÂ•Ω„ÄÇ</li>
</ul>
<p>Âú®NLP‰∏≠Ôºå‰∏ÄËà¨‰∏çÁî®ANOVAsÊàñËÄÖtÊ£ÄÈ™åÔºåËÄåÊòØ‰ΩøÁî®ÈùûÂèÇÊï∞Ê£ÄÈ™åÔºö</p>
<ul>
<li><strong>Ëøë‰ººÈöèÊú∫ÂåñÊ£ÄÈ™å</strong>ÔºàApproximate Randomization TestÔºâ</li>
<li><strong>bootstrap Ê£ÄÈ™å</strong>ÔºàBootstrap TestÔºâ</li>
</ul>
<blockquote>
<p>ÊñπÂ∑ÆÂàÜÊûêÂíåtÊ£ÄÈ™åÈÉΩÈúÄË¶ÅÊúâÂÅáËÆæÔºöÊñπÂ∑ÆÈΩêÊÄßÊàñÊï∞ÊçÆÊúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇÊâÄ‰ª•Âè™ËÉΩÁî®ÈùûÂèÇÊï∞Ê£ÄÈ™å„ÄÇ</p></blockquote>
<h4 id="the-paired-bootstrap-test">The Paired Bootstrap Test
</h4><p>Bootstrap Ê£ÄÈ™åÁöÑÊ†∏ÂøÉÊòØ ‚ÄúÈáçÊäΩÊ†∑‚Äù‚Äî‚Äî ‰ªéÂéüÂßãÊµãËØïÈõÜ<code>x</code>‰∏≠<strong>ÊúâÊîæÂõûÂú∞ÈöèÊú∫ÊäΩÂèñ</strong>ÁîüÊàêÊñ∞ÁöÑÊµãËØïÈõÜ„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250830181020970.png"
	width="1097"
	height="490"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830181020970_hu_8f406568f15b6414.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830181020970_hu_c48413deb765f0e1.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="223"
		data-flex-basis="537px"
	
>
Âú®Êñ∞ÁöÑÊµãËØïÈõÜ‰∏äÔºåP ÂÄºÁ≠â‰∫é ‚ÄúÈáçÊäΩÊ†∑ÊµãËØïÈõÜ‰∏≠Ôºå$d(x^{(i)})‚â•2d(x)$ÁöÑÊï∞ÈáèÂç†ÊÄªÈáçÊäΩÊ†∑Ê¨°Êï∞bÁöÑÊØî‰æã‚Äù„ÄÇ<strong>Âà§Êñ≠Ê≠§Êó∂ÁöÑpÂÄºÊòØÂê¶‰Ωé‰∫éÈòàÂÄºÂ∞±ÂèØ‰ª•Âà§Êñ≠Âá∫ÊòØÂê¶ÊòØÊï∞ÊçÆÈõÜÊú¨Ë∫´ÁöÑÂÅèÂ∑ÆÂØºËá¥‰∫ÜAÊØîBË¶ÅÁªìÊûúÂ•Ω„ÄÇ</strong></p>
<blockquote>
<p>LLMÁöÑËß£ÈáäÔºöBootstrap ÈáçÊäΩÊ†∑ÔºåÂ∞±ÊòØÂú® ‚Äú‰∏çÊîπÂèòÂ§©Âπ≥ÂàùÂßãÂÄæÊñúÔºàÊµãËØïÈõÜÂÅèÂêëÔºâ‚Äù ÁöÑÂâçÊèê‰∏ãÔºåÂèçÂ§çÊîæ ‚ÄúÈöèÊú∫ÈáçÈáèÔºàÈáçÊäΩÊ†∑ÁöÑÊ†∑Êú¨Ôºâ‚ÄùÔºåÁúãÂ∑¶Ëæπ‰ºöÊØîÂè≥ËæπÂ§ö‰ΩéÂ§öÂ∞ëÊ†º ‚Äî‚Äî Â¶ÇÊûúÂè™ÊòØÂ§©Âπ≥Êú¨Ë∫´Ê≠™‰∫ÜÔºåÈöèÊú∫ÊîæÈáçÈáèÊó∂ÔºåÂ∑¶ËæπÊúÄÂ§ö‰Ωé 2 Ê†ºÂ∑¶Âè≥ÔºàÂ∏∏ËßÑÊ≥¢Âä®ÔºâÔºõÂ¶ÇÊûúÂ∑¶ËæπÁúüÁöÑÊõ¥ÈáçÔºåÂ∞±ÂèØËÉΩ‰Ωé 4 Ê†º‰ª•‰∏äÔºàÊûÅÁ´ØÊÉÖÂÜµÔºâ„ÄÇËøôÁßçÊûÅÁ´ØÊÉÖÂÜµÂ§ö‰∫ÜÔºåË∂ÖËøá‰∫ÜÈòàÂÄºÔºåÊàë‰ª¨Â∞±Êõ¥Áõ∏‰ø°ÊòØÂ§©Âπ≥Êú¨Ë∫´ÁöÑÈóÆÈ¢ò„ÄÇ</p></blockquote>
<h3 id="avoiding-harms-in-classification">Avoiding Harms in Classification
</h3><p>representational harmsÔºöÁî±‰∫éÂØπÁâπÂÆöÁ§æ‰ºöÁæ§‰ΩìÁöÑË¥¨‰ΩéÊàñÂàªÊùøÂç∞Ë±°ÂØºËá¥ÁöÑ‰º§ÂÆ≥„ÄÇ</p>
<p>toxic detection</p>
<p>model card</p>
<h3 id="interpereting-models">Interpereting Models
</h3><p>Ê®°ÂûãÁöÑÂèØËß£ÈáäÊÄß‰πüÊòØÂæàÈáçË¶ÅÁöÑ„ÄÇÈÄªËæëÂõûÂΩíÂ∞±ÊòØÊØîËæÉÂ•ΩÁöÑÂèØËß£ÈáäÁöÑÊ®°Âûã„ÄÇ</p>
<h3 id="advanced-regularization">Advanced: Regularization
</h3><p>regularizationÊù•Ëß£ÂÜ≥ËøáÊãüÂêàÁöÑÈóÆÈ¢òÔºåÊèêÈ´òÊ®°ÂûãÊ≥õÂåñËÉΩÂäõ„ÄÇ</p>
<ul>
<li>L2 regulaization</li>
<li>L1 regulaization</li>
<li>lasso</li>
<li>ridge</li>
</ul>
<h2 id="embeddings">Embeddings
</h2><p>ÂàÜÂ∏ÉÂÅáËØ¥ÔºöÁõ∏‰ººÁöÑ‰∏ä‰∏ãÊñáÊÄª‰ºöË°®Áé∞Âá∫Áõ∏‰ººÁöÑÊÑèÊÄù„ÄÇ</p>
<p>EmbeddingsÂàÜÁ±ª</p>
<ul>
<li>static embeddings</li>
<li>contextualized embeddings</li>
</ul>
<p>Â≠¶‰π†embeddingsÂíåÂÆÉÁöÑÊÑè‰πâÁöÑÁêÜËÆ∫Áß∞‰∏∫ÂêëÈáèËØ≠‰πâ„ÄÇ</p>
<ul>
<li>Ëá™ÁõëÁù£Ê®°Âûã</li>
<li>representation learningÁöÑ‰∏ÄÁßç</li>
<li>Êó†ÈúÄÈÄöËøáÁâπÂæÅÂ∑•Á®ãÁöÑ‰∫∫Â∑•Âà∂ÈÄ†representations</li>
</ul>
<h3 id="lexical-semantics">Lexical Semantics
</h3><ul>
<li>lemmaÔºöcitation formÔºåËØçÂÖÉÔºåÂºïÁî®ÂΩ¢ÂºèÔºå‰∏Ä‰∏™ËØçÁöÑÂü∫Êú¨ÂΩ¢Âºè„ÄÇ</li>
<li>wordformÔºöËØçÂΩ¢Ôºå‰∏Ä‰∏™ËØçÁöÑÂÖ∑‰Ωì‰ΩøÁî®ÂΩ¢ÊÄÅ„ÄÇ</li>
<li>word senseÔºöËØç‰πâ„ÄÇ</li>
<li>synonymyÔºöÂêå‰πâÂÖ≥Á≥ª„ÄÇ</li>
<li>word similarityÔºöËØçËØ≠Áõ∏‰ººÂ∫¶„ÄÇ
<ul>
<li><a class="link" href="https://arxiv.org/abs/1408.3456"  target="_blank" rel="noopener"
    >SimLex-999</a>‰∏≠Â∞±ËÆ©‰∫∫‰ª¨Áªô‰∏Ä‰∏™ËØçÂíåÂè¶‰∏Ä‰∏™ËØçÁöÑÁõ∏‰ººÂ∫¶ÊâìÂàÜ„ÄÇ</li>
</ul>
</li>
<li>word relatedness/associationÔºöËØçÊ±áÂÖ≥ËÅîÊÄßÔºåÊâÄÊúâËÉΩËÆ©ËØçÊ±áÊúâÂÖ≥ËÅîÊÑüÁöÑÂÖ≥Á≥ª„ÄÇ
<ul>
<li>semantic fields</li>
<li>topic modelsÔºöÁâπÂà´Âú∞ÊúâLatent Dirichlet AllocationÔºåLDA</li>
<li>ÊúÄÂ∏∏ËßÅÁöÑÂÖ≥Á≥ª
<ul>
<li>hypernymy or IS-A</li>
<li>antonymy</li>
<li>mernoymy</li>
</ul>
</li>
</ul>
</li>
<li>connotation</li>
</ul>
<h3 id="vector-semantics-the-intuition">Vector Semantics: The Intuition
</h3><p>‰∏Ä‰∏™ÂçïËØçÂèØ‰ª•Ë°®Á§∫‰∏∫<strong>Â§öÁª¥ËØ≠‰πâÁ©∫Èó¥‰∏≠ÁöÑ‰∏Ä‰∏™ÁÇπ</strong>„ÄÇËÄåËøô‰∏™Â§öÁª¥ËØ≠‰πâÁ©∫Èó¥ÊòØ‰ªéÂçïËØçÈÇªÂ±ÖÁöÑÂàÜÂ∏ÉËßÑÂæã‰∏≠Êé®ÂØºËÄåÊù•ÁöÑ„ÄÇ</p>
<ul>
<li>tf-idf</li>
<li>word2vec</li>
<li>cosine</li>
</ul>
<h4 id="simple-count-based-embeddings">Simple Count-based Embeddings
</h4><ul>
<li>ËØçÊ±áË°®‰∏ÄËà¨Âú®1-5‰∏á‰πãÈó¥</li>
<li>Á®ÄÁñèÂêëÈáèË°®Á§∫ÔºöÂ§öÊï∞Êï∞ÂÄº‰∏∫0ÔºåÁõÆÂâçÊúâÊúâÊïàÁÆóÊ≥ïÊù•ÊúâÊïàÂ≠òÂÇ®ÂíåËÆ°ÁÆó</li>
<li>ÊùÉÈáçÂáΩÊï∞
<ul>
<li>ËÆ°Êï∞ÁöÑÊó∂ÂÄôÂèØ‰ª•Áî®ÊùÉÈáçÂáΩÊï∞</li>
<li>ÁõÆÂâçÊúÄÊµÅË°åÁöÑÊñπÊ≥ïÊòØtf-idf</li>
<li>ËøòÊúâ‰∏Ä‰∫õÂéÜÂè≤ÊùÉÈáçÊñπÂºè</li>
</ul>
</li>
</ul>
<h4 id="cosine-for-measuring-similarity">Cosine for Measuring Similarity
</h4><p>‰ΩøÁî®‰ΩôÂº¶ËÆ°ÁÆóÁõ∏‰ººÂ∫¶„ÄÇÈÄÇÁî®‰∫éÁ®ÄÁñèÈïøÂêëÈáè„ÄÇ</p>
<h4 id="word2vec">Word2vec
</h4><p>embeddingsË¶ÅÂå∫Âà´‰∫éÂéüÊù•ÁöÑÁ®ÄÁñèÈïøÂêëÈáèÔºåÈÄöÂ∏∏ÊåáÁü≠ËÄåÁ®†ÂØÜÁöÑÂêëÈáè„ÄÇ</p>
<ul>
<li>Â≠¶‰π†ÁöÑÊùÉÈáçÂèòÂ∞ëÔºåÂ≠¶‰π†Êõ¥Âø´</li>
<li>ÊúâÂä©‰∫éÊ≥õÂåñÂíåÈÅøÂÖçËøáÊãüÂêà</li>
<li>ËÉΩÂ§üÊõ¥Â•ΩÊçïÊçâÂêå‰πâÊÄß</li>
</ul>
<p>skip-gram with negative samplingÔºàSGNSÔºâÊòØword2vec‰∏§ÁßçÊñπÊ≥ïÁöÑ‰∏ÄÁßç„ÄÇword2vecÊòØ‰∏ÄÁßç<strong>ÈùôÊÄÅembedding</strong>ÊñπÊ≥ïÔºåÂå∫Âà´‰∫éÂä®ÊÄÅembeddingÔºåÂ¶ÇBERTË°®Á§∫„ÄÇ</p>
<p>ËøôÈáåÊúâ‰∏Ä‰∏™ÊûÅÂÖ∑ÂàõÊñ∞ÊÄßÁöÑÊÉ≥Ê≥ï ‚Äî‚Äî<strong>‰∏çÁõ¥Êé•ËÆ°ÁÆó ‚ÄúËØç‰∏éËØçÁöÑÂÖ≥ËÅî‚ÄùÔºàÂÖ±Áé∞Áü©ÈòµÔºâÔºåËÄåÊòØÈÄöËøá‰∏Ä‰∏™ ‚ÄúÈ¢ÑÊµã‰ªªÂä°‚Äù ËÆ©Ê®°ÂûãËá™Âä®Â≠¶‰π†ËøôÁßçÂÖ≥ËÅîÔºåÂÜçÂ∞ÜÂ≠¶‰π†ÊàêÊûúÔºàÊùÉÈáçÔºâ‰Ωú‰∏∫ËØçÂµåÂÖ•</strong>„ÄÇ</p>
<ul>
<li>ËøôË¢´Áß∞‰∏∫<strong>Ëá™ÁõëÁù£ÊñπÊ≥ï</strong></li>
</ul>
<p>Skip-gram Ê®°ÂûãÁöÑÊ†∏ÂøÉÊÄùË∑ØÂ¶Ç‰∏ãÔºö</p>
<ol>
<li>Â∞ÜÁõÆÊ†áËØç‰∏éÂÖ∂Áõ∏ÈÇªÁöÑËØ≠Â¢ÉËØçËßÜ‰∏∫<strong>Ê≠£‰æã</strong>„ÄÇ</li>
<li>‰ªéËØçÊ±áË°®‰∏≠ÈöèÊú∫ÈÄâÂèñÂÖ∂‰ªñËØçËØ≠Ôºå‰Ωú‰∏∫<strong>Ë¥ü‰æã</strong>„ÄÇ</li>
<li>Âà©Áî®<strong>ÈÄªËæëÂõûÂΩíËÆ≠ÁªÉ‰∏Ä‰∏™ÂàÜÁ±ªÂô®Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÂå∫ÂàÜ‰∏äËø∞‰∏§ÁßçÊÉÖÂÜµ</strong>ÔºàÂç≥Âå∫ÂàÜ ‚ÄúÁõÆÊ†áËØç‰∏éËØ≠Â¢ÉËØçÊòØÁõ∏ÈÇªÂÖ≥Á≥ª‚Äù Âíå ‚ÄúÁõÆÊ†áËØç‰∏éÈöèÊú∫ËØçÊó†Áõ∏ÈÇªÂÖ≥Á≥ª‚ÄùÔºâ„ÄÇ</li>
<li>Â∞ÜËÆ≠ÁªÉËøáÁ®ã‰∏≠Â≠¶Âà∞ÁöÑÊùÉÈáç‰Ωú‰∏∫embedding„ÄÇ</li>
</ol>
<h5 id="the-classifier">The Classifier
</h5><p>Skip-gramÁõÆÊ†áÊòØËÆ≠ÁªÉ‰∏Ä‰∏™ÂàÜÁ±ªÂô®ÔºåËÆ°ÁÆóËøô‰∏™Âú∞ÊñπÂ°´Ëøô‰∏™ËØçÁöÑÊ¶ÇÁéá„ÄÇ</p>
<ul>
<li><strong>Ê†∏ÂøÉÊÄùË∑Ø</strong>Ôºö‰∏Ä‰∏™ËØçÊòØÂê¶ÂèØËÉΩÂá∫Áé∞Âú®ÁõÆÊ†áËØçÈôÑËøëÔºåÂèñÂÜ≥‰∫éÂÆÉÁöÑÂµåÂÖ•ÂêëÈáè‰∏éÁõÆÊ†áËØçÁöÑÂµåÂÖ•ÂêëÈáèÊòØÂê¶Áõ∏‰ºº„ÄÇ</li>
<li><strong>Áõ∏‰ººÂ∫¶ËÆ°ÁÆó</strong>ÔºöÁÇπÁßØ</li>
<li>ÁÇπÁßØÁªìÊûúÂπ∂ÈùûÊ¶ÇÁéáÂÄºÔºåËøòÈúÄË¶ÅÁªèËøáSigmoidÂáΩÊï∞ËøêÁÆó‚Äò</li>
</ul>
<p>Skip-gramÊ®°ÂûãÂÖ∂ÂÆûÊòØÂ≠òÂÇ®‰∫ÜÊØè‰∏™ÂçïËØçÁöÑ‰∏§‰∏™embeddingsÔºå‰∏Ä‰∏™‰Ωú‰∏∫ÁõÆÊ†áËØçÔºå‰∏Ä‰∏™‰Ωú‰∏∫‰∏ä‰∏ãÊñá„ÄÇÂàÜÂà´‰ªétarget matrix WÂíåcontext matrix CÁü©Èòµ‰∏≠Â≠¶‰π†„ÄÇ</p>
<h4 id="learning-skip-gram-embeddings">Learning Skip-gram Embeddings
</h4><p><strong>positive examples</strong>Ôºö‰∏ä‰∏ãÊñáÊªëÂä®Á™óÂè£</p>
<p><strong>negative examples</strong>ÔºöËØçÊ±áË°®‰∏≠ÈöèÊú∫ÊäΩÂèñÔºå‰∏ÄËà¨ÊòØpositive examplesÁöÑkÂÄçÔºåÁî±ÁõÆÊ†áËØç$w$ÂíåÂô™Â£∞ËØçÁªÑÊàê„ÄÇ</p>
<p>Âú®ÊäΩÊ†∑ÁöÑÊó∂ÂÄôÔºå‰ºöËÆæÁΩÆ‰∏Ä‰∏™ÊùÉÈáçÁ≥ªÊï∞$\alpha =0.75$Êù•Ë∞ÉÊï¥Ê¶ÇÁéá$P(w)$ÈÅøÂÖçÊÄªÊòØÈÄâÊã©È´òÈ¢ëËØç„ÄÇ</p>
<p>Â≠¶‰π†ÁõÆÊ†áÔºö</p>
<ul>
<li>ÊúÄÂ§ßÂåñpositive examples‰∏≠Â≠¶‰π†ÁöÑÁõ∏‰ººÂ∫¶</li>
<li>ÊúÄÂ∞èÂåñnegative examples‰∏≠ÁöÑÁõ∏‰ººÂ∫¶</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250831142750258.png"
	width="880"
	height="558"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250831142750258_hu_b96cde190aab3329.png 480w, /2025/speech-and-language-processing/assets/IMG-20250831142750258_hu_1050abc09f7158b.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="157"
		data-flex-basis="378px"
	
>
Â¶ÇÂõæÔºåÁõÆÊ†áÊòØ‰ΩøÂæóapricotÂíåjamÁöÑÁÇπÁßØÊõ¥Â§ßÔºåÂíåmatrix„ÄÅTolstoyÁöÑÁÇπÁßØÊõ¥Â∞è„ÄÇ</p>
<p>Âú®Â≠¶‰π†ÂÆå‰∫Ü‰πãÂêéÔºå‰∏ÄËà¨Âè™‰ΩøÁî®WÊù•Ë°®Á§∫„ÄÇ</p>
<h4 id="other-kinds-of-static-embeddings">Other kinds of static embeddings
</h4><p>fasttext</p>
<ul>
<li>Ëß£ÂÜ≥‰∫Üword2vecÁöÑÊú™Áü•ËØçÈóÆÈ¢ò</li>
<li>subword models</li>
</ul>
<p>GloVe</p>
<ul>
<li>Global Vectors</li>
<li>Âü∫‰∫éËØçËØçÂÖ±Áé∞Áü©ÈòµÁöÑÊ¶ÇÁéá</li>
</ul>
<p>word2vecÂèØ‰ª•ÁúãÊàê<strong>Èó¥Êé•‰ºòÂåñ‰∏Ä‰∏™ ‚ÄúÂ∏¶PPMIÔºàPositive Pointwise Mutual InformationÔºâÊùÉÈáçÁöÑÂÖ±Áé∞Áü©Èòµ‚Äù ÁöÑÂáΩÊï∞</strong>„ÄÇ</p>
<blockquote>
<p>ÊàëÁöÑ‰∏Ä‰∫õÊÉ≥Ê≥ïÔºöGNNÊòØÂê¶‰πüÂèØ‰ª•ÂÅöÁ±ª‰ººÁöÑÂ∑•‰ΩúÔºüGNNÁõÆÂâçÁöÑÊñπÊ≥ïÂ∞±ÊòØÁõ¥Êé•Â§ÑÁêÜÂõæÁªìÊûÑÔºåËøõË°åÂ±ÄÈÉ®ÂõæÁªìÊûÑÂ≠¶‰π†„ÄÇ‰∏ç‰∏ìÈó®ËÆ≠ÁªÉ‰∏Ä‰∏™Ê®°ÂûãËøõË°åÈ¢ÑÊµãÔºåËÄåÊòØÊääËøô‰∏™ËÆ≠ÁªÉÁöÑÊ®°ÂûãÁöÑÂèÇÊï∞Áõ¥Êé•‰Ωú‰∏∫‰∏Ä‰∏™ÂêëÈáè‰ΩøÁî®ÔºåÂèçËÄåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÂà∞ÈöêÂê´ÂÖ≥Á≥ª„ÄÇ</p>
<p>ËøôÁßçÊÉ≥Ê≥ïÊòØÂê¶ÂèØ‰ª•Â∞ÜLLMÂíåGNNÊõ¥Â•ΩÂú∞ËøûÊé•Ëµ∑Êù•Ôºü</p></blockquote>
<h3 id="visualizing-embeddings">Visualizing Embeddings
</h3><ul>
<li>Áõ¥Êé•ÂàóÂá∫ÊúÄÁõ∏‰ººÁöÑÂçïËØç</li>
<li>ËÅöÁ±ªÁÆóÊ≥ï</li>
<li>t-SNEÔºöËÆ©‰ΩéÁª¥Á©∫Èó¥‰∏≠ ‚ÄúÁÇπ‰∏éÁÇπÁöÑÁõ∏‰ººÊ¶ÇÁéá‚ÄùÔºåÂ∞ΩÈáèÂíåÈ´òÁª¥Á©∫Èó¥‰∏≠ ‚ÄúÁÇπ‰∏éÁÇπÁöÑÁõ∏‰ººÊ¶ÇÁéá‚Äù ‰∏ÄËá¥</li>
</ul>
<h3 id="semantic-properites-of-embeddings">Semantic Properites of Embeddings
</h3><p>ÂÖ≥ËÅîÂíåÁõ∏‰ººÁöÑÂå∫Âà´ÔºöË∂äÁü≠ÁöÑ‰∏ä‰∏ãÊñáÂæóÂà∞ÁöÑÂêëÈáèÔºåÁõ∏‰ººË∂äÂ•ΩÊâæÔºåÂÖ≥ËÅîË∂äÈöæÔºõË∂äÈïøÊ≠£Â•ΩÁõ∏Âèç„ÄÇ</p>
<ul>
<li>‰∏ÄÈò∂ÂÖ±Áé∞ÔºöÁªÑÂêàÂÖ≥Á≥ªÔºå‰∏ÄËµ∑ÁªÑÂêàÂá∫Áé∞ÔºåÂ¶ÇwriteÂíåpoem</li>
<li>‰∫åÈò∂ÂÖ±‰∫´ÔºöËÅöÂêàÂÖ≥Á≥ªÔºåÁõ¥Êé•Áõ∏ÂÖ≥ÔºåÂ¶ÇwriteÂíåsay</li>
</ul>
<p>Á±ªÊØîÂÖ≥Á≥ªÔºöÂπ≥Ë°åÂõõËæπÂΩ¢Ê®°Âûã
<img src="/2025/speech-and-language-processing/assets/IMG-20250831150132974.png"
	width="1134"
	height="469"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250831150132974_hu_363723ff4c0d8ebb.png 480w, /2025/speech-and-language-processing/assets/IMG-20250831150132974_hu_327a3c1c3636cedb.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="241"
		data-flex-basis="580px"
	
>
ÈóÆÈ¢òÊòØÂè™ËÉΩÁî®Âú®ÊòéÁ°ÆÁöÑÂÖ≥Á≥ª„ÄÅÁü≠ÁöÑË∑ùÁ¶ªÂíåÈ¢ëÁπÅÁöÑÂçïËØç‰∏ä„ÄÇ</p>
<h4 id="embeddings-and-historical-semantics">Embeddings and Historical Semantics
</h4><p>ÂµåÂÖ•ÁöÑÂ∫îÁî®„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250831150603967.png"
	width="839"
	height="505"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250831150603967_hu_baa0cb87e66ee17e.png 480w, /2025/speech-and-language-processing/assets/IMG-20250831150603967_hu_76e49fbca324cd4d.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="166"
		data-flex-basis="398px"
	
></p>
<p>ÂæàÊúâÊÑèÊÄù„ÄÇ</p>
<ul>
<li>gayÔºöÊÑâÊÇ¶ÁöÑ-&gt;Êòé‰∫ÆÁöÑ-&gt;Áî∑ÂêåÊÄßÊÅã</li>
<li>broadcastÔºöÊï£Êí≠-&gt;Êä•Á∫∏-&gt;BBC</li>
<li>awfulÔºöÂ∫Ñ‰∏•ÁöÑ-&gt;ÂèØÊÄïÁöÑ</li>
</ul>
<h3 id="bias-and-embeddings">Bias and Embeddings
</h3><p>allocational harm</p>
<p>embedding‰∏ç‰ªÖÂèçÊò†ËæìÂÖ•ÔºåËøòÊîæÂ§ßÂÅèËßÅ„ÄÇ</p>
<h3 id="evaluating-vector-models">Evaluating Vector Models
</h3><p>Áõ∏‰ººÂ∫¶Â∫¶ÈáèÔºö</p>
<ul>
<li>‰∏çÂê´‰∏ä‰∏ãÊñá
<ul>
<li>WordSim-353</li>
<li>SimLex-999</li>
<li>TOEFL dataset</li>
</ul>
</li>
<li>Âê´‰∏ä‰∏ãÊñá
<ul>
<li>SCWS</li>
<li>WiC</li>
<li>semantic textual similarity task</li>
</ul>
</li>
</ul>
<p>Á±ªÊØîÂ∫¶ÈáèÔºö</p>
<ul>
<li>SemEval-2012 Task 2 dataset</li>
</ul>
<p>ÊâÄÊúâÁöÑEmbeddingÁÆóÊ≥ïÈÉΩ‰ºöÂ≠òÂú®Âõ∫ÊúâÁöÑÂèòÂºÇÊÄß„ÄÇÂª∫ËÆÆ‰ΩøÁî®bootstrapÈááÊ†∑ÂêéÁöÑÊñáÊ°£‰∏≠ËÆ≠ÁªÉÂ§ö‰∏™embeddingsÂπ∂Âπ≥Âùá„ÄÇ</p>
<h2 id="neural-networks">Neural Networks
</h2><p>McCulloch-Pitts neuron</p>
<p>feedforward</p>
<p>deep learning</p>
<h3 id="units">Units
</h3><p>bias term</p>
<p>activationÔºö‰ΩøÁî®non-linear functionsÔºåÂ¶Çsigmoid„ÄÅtanh„ÄÅReLUÁ≠â</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250901195216978.png"
	width="833"
	height="422"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250901195216978_hu_e98d5b59e57b93ca.png 480w, /2025/speech-and-language-processing/assets/IMG-20250901195216978_hu_75d0b4b410c52d3a.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="197"
		data-flex-basis="473px"
	
></p>
<ul>
<li>sigmoidÂíåtanhÁöÑÈóÆÈ¢òÔºöÁâπÂà´ÂÆπÊòìÂá∫Áé∞È•±ÂíåÁöÑÁé∞Ë±°ÔºåÂ¶ÇÂΩìÁâπÂà´Èù†Ëøë1ÁöÑÊó∂ÂÄôÔºåÊ≠§Êó∂ÁöÑÂØºÊï∞Êé•Ëøë‰∫é0ÔºåËæìÂÖ•ÁöÑÂæÆÂ∞èÊîπÂèòÂ∞ÜÊó†Ê≥ïÂºïËµ∑ËæìÂá∫ÁöÑ‰ªª‰ΩïÂèòÂåñ„ÄÇËøôÁßçÁé∞Ë±°Áß∞‰∏∫<strong>Ê¢ØÂ∫¶Ê∂àÂ§±</strong>„ÄÇ</li>
</ul>
<h3 id="the-xor-problem">The XOR Problem
</h3><p>Minsky and PapertÔºö‰∏ÄÂ±ÇÁ•ûÁªèÂÖÉÊó†Ê≥ïËß£ÂÜ≥ÂºÇÊàñÈóÆÈ¢ò„ÄÇÂ¶ÇÊÑüÁü•Êú∫„ÄÇ</p>
<h4 id="the-solution-neural-networks">The Solution: Neural Networks
</h4><p>ÂåÖÂê´ÈöêËóèÂ±ÇÁöÑÂ§öÂ±ÇÊÑüÁü•Êú∫MLPËß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢ò„ÄÇ</p>
<p>‰∏ãÈù¢Ëøô‰∏™ÂõæÔºå‰∏çÁÆ°ÂØπ‰∫éÂéüÊù•ÁöÑ[0,1]ËøòÊòØ[1,0]ÈÉΩ‰ºöÂ∞Ü‰ªñ‰ª¨Âú®ÈöêÂê´Â±ÇÈáåËΩ¨Âåñ‰∏∫[1,0]„ÄÇËÄåÂéüÊù•ÁöÑ[0,0]Âíå[1,1]ÂàôÂèò‰∏∫[2,1]ÔºåÊ≠§Êó∂‰æøÁ∫øÊÄßÂèØÂàÜ„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250901200315265.png"
	width="762"
	height="337"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250901200315265_hu_820ad2365c82284f.png 480w, /2025/speech-and-language-processing/assets/IMG-20250901200315265_hu_8ed92ef255ef500a.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="226"
		data-flex-basis="542px"
	
></p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250901201345068.png"
	width="686"
	height="446"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250901201345068_hu_a56fa62e2362e855.png 480w, /2025/speech-and-language-processing/assets/IMG-20250901201345068_hu_c12f6c72632725f1.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="153"
		data-flex-basis="369px"
	
></p>
<h3 id="feedforward-neural-networks">Feedforward Neural Networks
</h3><p>‰∏éRNNÂØπÂ∫îÔºåÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÔºàFNNÔºâ‰∏çÂ∏¶ÊúâÂæ™ÁéØ„ÄÇ</p>
<ul>
<li>Áî±‰∫éÂéÜÂè≤ÂéüÂõ†ÔºåFNN‰πüÁß∞‰∏∫Â§öÂ±ÇÊÑüÁü•Êú∫ÔºåMLPsÔºå‰∫ãÂÆû‰∏äÁé∞Âú®ÁöÑÂ§öÂ±ÇÁΩëÁªú‰∏≠Â∑≤Áªè‰∏çÊòØÊÑüÁü•Êú∫‰∫Ü„ÄÇ</li>
<li>ÊÑüÁü•Êú∫‰ΩøÁî®Èò∂Ë∑ÉÂáΩÊï∞ÔºåÁé∞Âú®ÁöÑÁ•ûÁªèÁΩëÁªúÁî®ÁöÑÊòØÂ§öÁßçÈùûÁ∫øÊÄßÁöÑÂçïÂÖÉÂ¶ÇReLUsÊàñËÄÖSigmoidÁ≠â</li>
</ul>
<p>ÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÁöÑÊ†áÂáÜÁªìÊûÑÊòØÂÖ®ËøûÊé•ÁöÑ„ÄÇ
<img src="/2025/speech-and-language-processing/assets/IMG-20250902182112223.png"
	width="726"
	height="429"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902182112223_hu_28bfdb5ad0f61a8a.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902182112223_hu_d4c96ab1783e2a89.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="169"
		data-flex-basis="406px"
	
>
Âú®ËøôÈáåÔºåW‰Ωú‰∏∫ËæìÂÖ•Â±ÇÂà∞ÈöêÂê´Â±ÇÁöÑÊùÉÈáçÁü©ÈòµÔºåUÂàôÊòØÈöêÂê´Â±ÇÂà∞ËæìÂá∫Â±ÇÁöÑÊùÉÈáçÁü©Èòµ„ÄÇ
</p>
$$h=\sigma(Wx+b)$$<p>
</p>
$$z=Uh$$<p>
Âú®ÂæóÂà∞ËæìÂá∫ÁªìÊûú$z$‰πãÂêéÔºåÁî±‰∫é$z$ÊòØ‰∏Ä‰∏™ÂÆûÊï∞ÂÄºÂêëÈáèÔºàÂÖ∂ÂÆûÂ∞±ÊòØlogitsÔºâÔºåËÄåÂàÜÁ±ªÈúÄË¶ÅÁöÑÊòØÊ¶ÇÁéáÂàÜÂ∏ÉÂêëÈáèÔºåÊâÄ‰ª•Ë¶ÅÂØπÂÖ∂ËøõË°åÂΩí‰∏ÄÂåñÔºànormalizingÔºâ„ÄÇËøôÈáå‰ΩøÁî®ÁöÑÊòØsoftmaxÂáΩÊï∞„ÄÇ
</p>
$$y = \text{softmax}(z)$$<p>Á•ûÁªèÁΩëÁªúÂíåÂ§öÈ°πÈÄªËæëÂõûÂΩíÁöÑÂå∫Âà´Ôºö</p>
<ul>
<li>ÊúâËÆ∏Â§öÂ±Ç</li>
<li>‰∏≠Èó¥Â±ÇÁöÑÊøÄÊ¥ªÂáΩÊï∞‰∏çÂè™‰ΩøÁî®sigmoid</li>
<li>ÁâπÂæÅÂèØ‰ª•‰∏çÂè™ÊòØÁî±‰∫∫Â∑•ÁöÑÁâπÂæÅÊ®°ÊùøËÆæËÆ°ÔºåËÄåÂèØ‰ª•Áî±ÁΩëÁªúËá™Ë∫´ÂæóÂà∞</li>
</ul>
<p>ÈÄªËæëÂõûÂΩíÂèØ‰ª•ÁêÜËß£Êàê‰∏ÄÂ±ÇÁöÑÁ•ûÁªèÁΩëÁªú„ÄÇ</p>
<h4 id="more-details-on-feedforward-networks">More Details on Feedforward Networks
</h4><p>‰∏∫‰ªÄ‰πàÊøÄÊ¥ªÂáΩÊï∞Ë¶ÅÈùûÁ∫øÊÄß</p>
<p>ÊõøÊç¢ÂÅèÁΩÆÈ°πÔºö‰ΩøÁî®dummy nodeÊù•‰ª£ÊõøÂéüÊù•ÁöÑÂÅèÁΩÆÈ°π„ÄÇ‰πüÂ∞±ÊòØ‰∏ãÂõæ‰∏≠ÊääbÊç¢Êàê‰∏Ä‰∏™Êñ∞ÁöÑÂõ∫ÂÆö‰∏∫1ÁöÑ$x_0$„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902185044344.png"
	width="692"
	height="372"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902185044344_hu_22233632699e1e11.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902185044344_hu_f917792a9f3a40e1.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="186"
		data-flex-basis="446px"
	
></p>
<h3 id="feedforward-networks-for-nlp-classification">Feedforward Networks for NLP: Classification
</h3><p>ÂµåÂÖ•Áü©Èòµ„ÄÅË°®Á§∫Ê±†Âåñ„ÄÅË°®Á§∫Â≠¶‰π†ÂÖà‰∏çËÆ≤ÔºåÂÖàÂ≠¶‰∏Ä‰∏ãÁî®ÂâçÈ¶àÁ•ûÁªèÁΩëÁªúËß£ÂÜ≥ÂàÜÁ±ªÈóÆÈ¢ò„ÄÇ</p>
<h4 id="neural-net-classifiers-with-hand-built-features">Neural Net Classifiers with Hand-built Features
</h4><p>‰∫∫Â∑•ËÆæËÆ°ÁöÑÁâπÂæÅÔºåÈô§‰∫ÜÊääMLPÊç¢ÊàêFNN‰πãÂ§ñÊ≤°ÂèòÂåñ„ÄÇ
<img src="/2025/speech-and-language-processing/assets/IMG-20250902185551826.png"
	width="688"
	height="399"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902185551826_hu_e7290fd892ec1524.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902185551826_hu_7954655261211383.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="172"
		data-flex-basis="413px"
	
></p>
<h4 id="vectorizing-for-parallelizing-inference">Vectorizing for Parallelizing Inference
</h4><p>Âçï‰∏™Ê†∑Êú¨ÁâπÂæÅÁª¥Â∫¶ÊòØdÔºåÊúâm‰∏™Ê†∑Êú¨ÔºåÂ∞±ÂèØ‰ª•Â∞ÜËæìÂÖ•ÂÜôÊàê‰∏Ä‰∏™Áü©ÈòµX‰∏∫m√ódÁª¥„ÄÇ</p>
<p>ÂÖ∂‰ªñÁöÑÂÅèÁΩÆÈ°πÁ≠â‰πüÂèØ‰ª•ÂÜôÊàêÁü©ÈòµÁöÑÂΩ¢ÂºèÔºåÊúâÂä©‰∫éÊúÄÂêéÁöÑËøêÁÆó„ÄÇÊ≠§Êó∂Êúâ
</p>
$$H=\sigma(XW^T+\textbf{b})$$$$Z=HU^T$$$$\hat{Y}=\text{softmax}(Z)$$<blockquote>
<p>ËøôÈáåÁöÑX‰∏≠Ë°åÂêëÈáèË°®Á§∫‰∏Ä‰∏™Ê†∑Êú¨ÁöÑÂÆåÊï¥ÁâπÂæÅ„ÄÇÊúâÁöÑÊó∂ÂÄô‰ºöÂÜôÊàê$WX+b$ÔºåÊúâÁöÑÊó∂ÂÄôÊòØ$XW+b$„ÄÇÊ≥®ÊÑèXÁöÑÂΩ¢Áä∂ÊúâÊâÄ‰∏çÂêå„ÄÇ</p></blockquote>
<h3 id="embeddings-as-the-input-to-neural-net-classifiers">Embeddings as the Input to Neural Net Classifiers
</h3><p>static embeddings‰ª£Êõøhand-designed features„ÄÇ</p>
<ul>
<li>Â≠òÂÇ®static embeddingsÁöÑËØçÂÖ∏Áß∞‰∏∫embedding matrix <strong>E</strong></li>
</ul>
<p>one-hot vectorÔºö‰ªéembedding matrixÈÄâÂèñtoken embeddingÁöÑÊñπÊ≥ï
<img src="/2025/speech-and-language-processing/assets/IMG-20250902192356608.png"
	width="688"
	height="215"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902192356608_hu_35c57fe6c6d3b00d.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902192356608_hu_c416dc54a98e3898.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="320"
		data-flex-basis="768px"
	
></p>
<p>ÂàÜÁ±ªÂô®Ôºö</p>
<ul>
<li>concatenationÔºöÈÄÇÂêàÂØπtokenÈ°∫Â∫èÂíåÁªÜËäÇÊïèÊÑüÁöÑ‰ªªÂä°ÔºåÂ¶ÇËØ≠Ë®ÄÂª∫Ê®°</li>
<li>poolingÔºöÈÄÇÂêàÂØπÊï¥‰ΩìËØ≠‰πâÊïèÊÑüÁöÑ‰ªªÂä°ÔºåÂ¶ÇÊÉÖÊÑüÂàÜÊûê
<ul>
<li>mean-pooling</li>
<li>max-pooling</li>
</ul>
</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902192912076.png"
	width="807"
	height="712"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902192912076_hu_e6ec73621f005a0d.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902192912076_hu_c30b59ad826afcc3.png 1024w"
	loading="lazy"
	
		alt="ÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÁî®‰∫éÊÉÖÊÑüÂàÜÊûêÔºåÂ∏¶ÊúâÊ±†ÂåñÂ±ÇÁöÑÁªìÊûÑ"
	
	
		class="gallery-image" 
		data-flex-grow="113"
		data-flex-basis="272px"
	
></p>
<p>Á•ûÁªèËØ≠Ë®ÄÊ®°ÂûãÂíåNÂÖÉËØ≠Ê≥ïÊ®°ÂûãÁöÑÂå∫Âà´Ôºö</p>
<ul>
<li>ÂèØ‰ª•Â§ÑÁêÜÊõ¥Â§öÁöÑ‰∏ä‰∏ãÊñáÔºåÊõ¥Âä†Ê≥õÂåñÔºåÈ¢ÑÊµãÊõ¥ÂáÜÁ°ÆÔºõÈÄüÂ∫¶Êõ¥ÊÖ¢ÔºåËÆ≠ÁªÉÊõ¥È∫ªÁÉ¶</li>
<li>‰ΩøÁî®embeddingsË°®Á§∫ËØçËÄå‰∏çÊòØword identity</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902193413335.png"
	width="717"
	height="645"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902193413335_hu_aab9c40f89390eef.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902193413335_hu_98641d26ebbce7c.png 1024w"
	loading="lazy"
	
		alt="embedding layer eËøôÈáåÊòØÊãºÊé•Ëµ∑Êù•ÁöÑ"
	
	
		class="gallery-image" 
		data-flex-grow="111"
		data-flex-basis="266px"
	
></p>
<h3 id="training-neural-nets">Training Neural Nets
</h3><p>ÊçüÂ§±ÂáΩÊï∞„ÄÅ‰∫§ÂèâÁÜµ</p>
<p>ËØØÂ∑ÆÂèçÂêë‰º†Êí≠/ÂèçÂêëÂæÆÂàÜ</p>
<h4 id="loss-function">Loss Function
</h4><p>‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÔºöË¥üÂØπÊï∞‰ººÁÑ∂ÊçüÂ§±„ÄÇÁî®‰∫é<strong>ËæìÂá∫‰∏∫ ‚ÄúÁ±ªÂà´Ê¶ÇÁéá‚Äù ÁöÑ‰ªªÂä°</strong>„ÄÇ</p>
<h4 id="computing-the-gradient">Computing the Gradient
</h4><p>‰∏ÄÂ±ÇÁöÑÊó∂ÂÄôÂèØ‰ª•Áî®ÊçüÂ§±ÁöÑÂØºÊï∞Ôºå‰ΩÜÊòØÂ§öÂ±ÇÁöÑÊó∂ÂÄôÈúÄË¶ÅÁî®ÂèçÂêë‰º†Êí≠ÁÆóÊ≥ï„ÄÇ</p>
<h4 id="computation-graphs">Computation Graphs
</h4><p>‰ªãÁªç‰∫Ü‰ªÄ‰πàÊòØËÆ°ÁÆóÂõæ„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902201310848.png"
	width="697"
	height="263"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902201310848_hu_9525e3130f2a9740.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902201310848_hu_d15c38716a3d7624.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="265"
		data-flex-basis="636px"
	
></p>
<h4 id="backward-differentiation-on-computation-graphs">Backward Differentiation on Computation Graphs
</h4><p><strong>ÂèçÂêë‰º†Êí≠</strong>ÔºöÁî®ÈìæÂºèÊ≥ïÂàô‰∏ÄÊ¨°ÊÄßËÆ°ÁÆóÂá∫ÊâÄÊúâÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶„ÄÇ
<img src="/2025/speech-and-language-processing/assets/IMG-20250902213405406.png"
	width="829"
	height="432"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902213405406_hu_aea55f690f9591d7.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902213405406_hu_e6e577c00244a646.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="191"
		data-flex-basis="460px"
	
>
ÂÖàÊ≠£Âêë‰º†Êí≠‰∏ÄÊ¨°ÔºåÁÑ∂ÂêéÂèçÂêë‰º†Êí≠ËÆ°ÁÆó„ÄÇ</p>
<ul>
<li>Ê≠£Âêë‰º†Êí≠ÊòØ‰∏∫‰∫ÜÂæóÂà∞ÊçüÂ§±ÂÄº</li>
<li>ÂèçÂêë‰º†Êí≠ÊòØ‰∏∫‰∫ÜÂæóÂà∞Ê¢ØÂ∫¶Ôºå‰ªéËÄåËøõË°åÂèÇÊï∞‰ºòÂåñ</li>
</ul>
<p>ÂΩìÁÑ∂Âú®ÁúüÊ≠£ÁöÑÁ•ûÁªèÁΩëÁªú‰∏≠ÔºåËÆ°ÁÆó‰ºöÊõ¥Âä†Â§çÊùÇ„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902213726628.png"
	width="851"
	height="528"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902213726628_hu_86b4e3d87a5a98fe.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902213726628_hu_508ea0186fde9736.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="161"
		data-flex-basis="386px"
	
>
‰ΩÜÊòØÊñπÊ≥ï‰∏áÂèò‰∏çÁ¶ªÂÖ∂ÂÆó„ÄÇ</p>
<h4 id="more-details-on-learning">More Details on Learning
</h4><p>Á•ûÁªèÁΩëÁªú‰ºòÂåñÈóÆÈ¢òÊòØ‰∏Ä‰∏™ÈùûÂá∏‰ºòÂåñÈóÆÈ¢ò„ÄÇÁõÆÂâçÊúâ‰∫ÜÂæàÂ§öÂ•ΩÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ïÔºö</p>
<ul>
<li>ÂàùÂßãÂÄº‰∏çËÆæ‰∏∫0ÔºåËÄåÊòØÈöèÊú∫ÁöÑ‰∏Ä‰∫õÂ∞èÁöÑÊï∞</li>
<li>dropout</li>
<li>Ë∂ÖÂèÇÊï∞ÔºöAdamÁ≠â</li>
</ul>
<p>GPUsÁ≠âËÆ°ÁÆóÂä†ÈÄü</p>
<h2 id="large-language-models">Large Language Models
</h2><p>ELIZA</p>
<p>Distributional hypothesis</p>
<p>Pretraining</p>
<p>ËØ≠Ë®ÄÊ®°ÂûãÔºö‰æùÊçÆÂâçÊñáÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™ËØçÁöÑÂàÜÂ∏É</p>
<p>ÂèëÂ±ïÔºö</p>
<ul>
<li>NÂÖÉËØ≠Ê≥ï</li>
<li>LSA/LSIÔºåÈöêÂê´ËØ≠‰πâÂàÜÊûêÔºåÂºÄÂßãÁî®ÂêëÈáèË°®Á§∫ËØç</li>
<li>Á•ûÁªèÁΩëÁªúËØ≠Ë®ÄÊ®°Âûãneural language model</li>
<li>RNNËØ≠Ë®ÄÊ®°Âûã</li>
<li>word2vec</li>
<li>È¢ÑËÆ≠ÁªÉÊäÄÊúØ</li>
<li>TransformerÊèêÂá∫</li>
<li>Êé©Á†ÅËØ≠Ë®ÄÊ®°Âûã</li>
<li>Ëá™ÂõûÂΩíËØ≠Ë®ÄÊ®°Âûã</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903221442377.png"
	width="817"
	height="367"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903221442377_hu_625cc627b168f68a.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903221442377_hu_9ee53070fd85a32c.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="222"
		data-flex-basis="534px"
	
></p>
<p>Â¶ÇÊûúÂèØ‰ª•È¢ÑÊµã‰∏ã‰∏Ä‰∏™ËØçÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÈÇ£‰πàÂ∞±ÂèØ‰ª•‰ªéÊ¶ÇÁéáÂàÜÂ∏É‰∏≠ËøõË°åÈááÊ†∑Ôºå‰ªéËÄåÁîüÊàê‰∏ã‰∏Ä‰∏™ËØç„ÄÇËøôÊ†∑Â∞±‰ªéÈ¢ÑÊµãÊ®°ÂûãÂèòÊàê‰∫ÜÁîüÊàêÊ®°Âûã„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903221449983.png"
	width="800"
	height="551"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903221449983_hu_3354a2ee29195159.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903221449983_hu_38463e6c14e8a47d.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="145"
		data-flex-basis="348px"
	
></p>
<ul>
<li>Âõ†ÊûúËØ≠Ë®ÄÊ®°Âûã/Ëá™ÂõûÂΩíËØ≠Ë®ÄÊ®°ÂûãÔºö‰ªéÂ∑¶Âà∞Âè≥‰æùÊ¨°ÁîüÊàê</li>
<li>Êé©Á†ÅËØ≠Ë®ÄÊ®°ÂûãÔºöBERTÁ≠âÔºåÂèØ‰ª•ÂêåÊó∂Âà©Áî®Â∑¶Âè≥‰∏§‰æßÁöÑ‰ø°ÊÅØ</li>
</ul>
<p>ÁîüÊàêÂºèAI</p>
<h3 id="three-architecture-for-language-models">Three Architecture for Language Models
</h3><p>‰∏âÁßçÁªìÊûÑÔºö</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903221500783.png"
	width="842"
	height="383"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903221500783_hu_5e7d45143603e021.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903221500783_hu_756c75157cc64aa.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="219"
		data-flex-basis="527px"
	
></p>
<ul>
<li>
<p>decoderÔºö‰∏äÊñá‰∏≠ÁöÑÊû∂ÊûÑ„ÄÇËæìÂÖ•‰∏∫‰∏ÄÁ≥ªÂàóÁöÑtokenÔºå<strong>‰æùÊ¨°Ëø≠‰ª£</strong>ÁîüÊàêËæìÂá∫ÁöÑtoken„ÄÇÁî®‰∫éËá™ÂõûÂΩíËØ≠Ë®ÄÊ®°Âûã„ÄÇ</p>
<ul>
<li>ÁîüÊàêÊñáÊú¨Á≠âÔºåÂ¶ÇGPT</li>
</ul>
</li>
<li>
<p>encoderÔºöÁî®‰∫éÊé©Á†ÅËØ≠Ë®ÄÊ®°Âûã„ÄÇËæìÂÖ•‰∏∫ÊñáÊú¨ÔºåËæìÂá∫‰∏∫Ê†áÁ≠æ„ÄÇ</p>
<ul>
<li>ÂàÜÁ±ªÊñáÊú¨Á≠âÔºåÂ¶ÇBERT</li>
</ul>
</li>
<li>
<p>encoder-decoderÔºöËæìÂÖ•‰∏∫‰∏Ä‰∏≤tokenÔºåËæìÂá∫‰πüÊòØ‰∏Ä‰∏≤token„ÄÇ</p>
<ul>
<li>
<p>Áõ∏ÊØîdecoderÊù•ËØ¥ÔºåÂíåËæìÂÖ•ËæìÂá∫tokenÁöÑÂÖ≥Á≥ªÊõ¥‰∏çÁ¥ßÂØÜÔºåËøôÁ±ªÊ®°ÂûãÁî®‰∫éÂú®‰∏çÂêåÁ±ªÂûãÁöÑÊ†áËÆ∞‰πãÈó¥ËøõË°åÊò†Â∞Ñ</p>
</li>
<li>
<p>Êú∫Âô®ÁøªËØëÁ≠â</p>
</li>
</ul>
</li>
</ul>
<p>‰ªñ‰ª¨ÈÉΩÊòØÂü∫‰∫éÁ•ûÁªèÁΩëÁªúÊûÑÂª∫ÁöÑ„ÄÇ</p>
<h3 id="conditional-generation-of-text-the-intuition">Conditional Generation of Text: The Intuition
</h3><p>‰∏çÁÆ°‰ªÄ‰πà‰ªªÂä°ÔºåÈÉΩÂèØ‰ª•ÁÆÄÂåñ‰∏∫ÁªôÂÆöpromptÁöÑ‰∏ã‰∏Ä‰∏™ËØçÁöÑÈ¢ÑÊµã‰ªªÂä°„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222035606.png"
	width="813"
	height="346"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222035606_hu_b84c5560c85d0ad0.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222035606_hu_e5d863ff7f7b20c8.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="234"
		data-flex-basis="563px"
	
></p>
<h3 id="prompting">Prompting
</h3><p>Êåá‰ª§ÂæÆË∞É</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222047947.png"
	width="815"
	height="386"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222047947_hu_6ad9dfc9690a84c8.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222047947_hu_8595ec17a22bd37b.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="211"
		data-flex-basis="506px"
	
></p>
<p>prompt</p>
<ul>
<li>Demonstrations</li>
<li>Few-shot prompting</li>
<li>Zero-shot prompting</li>
</ul>
<blockquote>
<p>ËøôÁßçDemonstrationsÂèØ‰ª•ÊâãÂä®Á≠õÈÄâÔºå‰πüÂèØ‰ª•Áî±‰ºòÂåñÂô®Â¶ÇDSPyÊù•Ëá™Âä®ÈÄâÊã©„ÄÇÊ≠§Â§ñÔºåDemonstrations‰ºº‰πéÂπ∂‰∏çÊòØ‰∏ÄÂÆöË¶ÅÁªôÊ≠£Á°ÆÁöÑÈóÆÈ¢òÂíåÁ≠îÊ°àÔºåÈîôËØØÁöÑ‰πüË°åÔºå‰∏ªË¶Å‰ΩúÁî®ÊòØÊ†ºÂºè„ÄÇ</p></blockquote>
<p>promptÔºöÂèØ‰ª•ÁúãÂÅö‰∏Ä‰∏™Â≠¶‰π†‰ø°Âè∑„ÄÇ<strong>ÊèêÁ§∫ËØç‰∏ç‰ºöÊõ¥Êñ∞Ê®°ÂûãÁöÑÊùÉÈáçÔºåÂÖ∂ÊîπÂèòÁöÑ‰ªÖ‰ªÖÊòØÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ‰ª•ÂèäÁΩëÁªú‰∏≠ÁöÑÊøÄÊ¥ªÁä∂ÊÄÅ„ÄÇ</strong></p>
<ul>
<li>in-context learningÔºöÂèÇÊï∞Ê≤°ÊúâÊîπÂèòÁöÑÂ≠¶‰π†</li>
<li>System promptÔºöÂΩ±ÂìçÂÖ®Â±ÄÁöÑ‰∏Ä‰∏™ÊñáÊú¨promptÔºåË¢´Ê∑ªÂä†Âà∞ÊâÄÊúâÁî®Êà∑promptÊàñÊü•ËØ¢ÁöÑÂâçÈù¢</li>
</ul>
<h3 id="generation-and-sampling">Generation and Sampling
</h3><p>ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜÖÈÉ®ÁΩëÁªú‰ºöÁîüÊàêlogitsÔºåÂÜçÁî±softmaxËÆ°ÁÆóÂæóÂà∞Ê¶ÇÁéáÔºåÈöèÂêéÂú®Ëøô‰∫õtoken‰∏≠ËøõË°åÈááÊ†∑„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222105152.png"
	width="812"
	height="329"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222105152_hu_16ad1c8697e23a8f.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222105152_hu_fff46d8496576fff.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="246"
		data-flex-basis="592px"
	
></p>
<p>decodingÔºöÂü∫‰∫éÊ¶ÇÁéáÈÄâÊã©tokenÁîüÊàêÁöÑËøáÁ®ãÂ∏∏Áß∞‰∏∫decoding</p>
<p>Ëá™ÂõûÂΩíÁîüÊàê
D</p>
<h4 id="greedy-decoding">Greedy decoding
</h4><p>Ë¥™ÂøÉËß£Á†ÅÔºöÈÄâÊã©Ê¶ÇÁéáÊúÄÈ´òÁöÑÈÇ£‰∏™tokenÁîüÊàêÔºàargmaxÔºâ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222114008.png"
	width="830"
	height="334"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222114008_hu_98ac761f730aacb7.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222114008_hu_49ef26f8c34a2658.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="248"
		data-flex-basis="596px"
	
></p>
<p>ÊïàÊûú‰∏çÂ•Ω‚Äî‚ÄîËæìÂÖ•ÊñáÊú¨Â¶ÇÊûúÁõ∏ÂêåÔºåÁªìÊûúÊòØÂõ∫ÂÆöÁöÑ„ÄÇ</p>
<p><strong>ÊùüÊêúÁ¥¢</strong></p>
<h4 id="random-sampling">Random Sampling
</h4><p>ÊåâÁÖßÂàÜÂ∏ÉÈááÊ†∑ÔºåÁõ¥Âà∞ÈááÊ†∑Âà∞EOS„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222124629.png"
	width="811"
	height="326"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222124629_hu_11963dc8db1e3582.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222124629_hu_4629a2f05c285dce.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="248"
		data-flex-basis="597px"
	
></p>
<p>ÊïàÊûú‰πü‰∏çÂ•Ω‚Äî‚ÄîÊúâ‰∫õtokenËôΩÁÑ∂Âç†ÊØîÂ∞èÔºå‰ΩÜÊòØËøô‰∫õtokenÂæàÂ§öÔºåÂØºËá¥Âç†ÊØî‰πü‰∏çÂ∞è„ÄÇÂ¶ÇÊûúË¢´ÈááÊ†∑Âà∞‰∫ÜÔºåÂè•Â≠ê‰ºöÂèòÂæóÂæàÂ•áÊÄ™</p>
<h4 id="temperature-sampling">Temperature Sampling
</h4><p>logitsËΩ¨Âåñ‰∏∫probabilityÁöÑÊó∂ÂÄôÁî®Â∏¶ÊúâtemperatureÁöÑsoftmaxËÆ°ÁÆó„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222132977.png"
	width="958"
	height="388"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222132977_hu_3596e2556cd62f8b.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222132977_hu_41d0f2085c2134bd.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="246"
		data-flex-basis="592px"
	
></p>
<p>ÂΩì$\tau \le 1$ÁöÑÊó∂ÂÄôÔºå‰ºöÂÄæÂêëÂ∞ÜÈ´òÊ¶ÇÁéáÊãâÂæóÊõ¥È´òÔºå‰ΩéÊ¶ÇÁéáÊãâÂæóÊõ¥‰ΩéÔºåÂèç‰πãÂàôÊõ¥ÂÆπÊòìÈÄâÊã©Âà∞‰ΩéÊ¶ÇÁéá‰∫ã‰ª∂„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222140906.png"
	width="799"
	height="603"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222140906_hu_bf742fb76c7ff9ec.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222140906_hu_bfff8415c93c103b.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="132"
		data-flex-basis="318px"
	
></p>
<h3 id="training-large-language-models">Training Large Language Models
</h3><p>‰∏ÄËà¨ÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºö</p>
<ul>
<li>È¢ÑËÆ≠ÁªÉ</li>
<li>Êåá‰ª§ÂæÆË∞É</li>
<li>ÂÅèÂ•ΩÂØπÈΩê</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222151929.png"
	width="800"
	height="514"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222151929_hu_25e107b931e1b56a.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222151929_hu_bf627f72fb75e27d.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="155"
		data-flex-basis="373px"
	
></p>
<h4 id="self-supervised-training-algorithm-for-pretraining">Self-supervised Training Algorithm for Pretraining
</h4><p>Teacher forcingÔºöÊ∞∏ËøúÁªôÊ®°ÂûãÊ≠£Á°ÆÁöÑÂ∫èÂàóÔºåËÄå‰∏çÊòØÊåâÁÖßÊ®°ÂûãÁöÑÈ¢ÑÊµãÊé•ÁùÄÂæÄ‰∏ãÊù•È¢ÑÊµã‰∏ã‰∏Ä‰∏™ËØç</p>
<p>Â¶Ç‰∏ãÂõæÔºåÁΩëÁªú‰∏≠ÁöÑÊùÉÈáç‰ºöÈÄöËøáÊ¢ØÂ∫¶‰∏ãÈôçËøõË°åË∞ÉÊï¥Ôºå‰ª•ÊúÄÂ∞èÂåñËØ•ÊâπÊ¨°‰∏äÁöÑÂπ≥Âùá‰∫§ÂèâÁÜµÊçüÂ§±„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903222201965.png"
	width="808"
	height="469"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903222201965_hu_571b6370d9971e97.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903222201965_hu_b722f8598f57cfd6.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="172"
		data-flex-basis="413px"
	
></p>
<p>Ëøô‰∫õÊùÉÈáçÂåÖÊã¨ÂµåÂÖ•Áü©Èòµ E„ÄÇÁî±Ê≠§Â≠¶‰π†Âà∞ÁöÑÂµåÂÖ•ÔºåÂ∞ÜËÉΩÊúÄÊúâÊïàÂú∞È¢ÑÊµãÂêéÁª≠ËØçËØ≠„ÄÇ</p>
<h4 id="pretraining-corpora-for-large-language-models">Pretraining Corpora for Large Language Models
</h4><p>ËÆ≠ÁªÉÊï∞ÊçÆÂèØ‰ª•Áî®ÁΩëÁªúÊï∞ÊçÆÔºåÂπ∂‰∏îÂä†‰∏ä‰∏Ä‰∫õÁ≤æÂøÉÁ≠õÈÄâÁöÑÊï∞ÊçÆ„ÄÇ</p>
<ul>
<li>common crawl</li>
<li>Colossal CLean Crawled Corpus</li>
<li>The Pile</li>
<li>Dolma</li>
</ul>
<p>ÈÅøÂÖç‰∏™‰∫∫‰ø°ÊÅØPIIÔºåÂéªÈáçÔºåÂÆâÂÖ®Á≠õÈÄâÔºåtoxicity detection„ÄÇÊ≥®ÊÑèÁâàÊùÉ„ÄÅÊï∞ÊçÆÂêåÊÑè„ÄÅÈöêÁßÅÂíåÂÅèÂ∑ÆÁ≠âÈóÆÈ¢ò„ÄÇ</p>
<h4 id="finetuning">Finetuning
</h4><p>ÂØπÂ∑≤Áªè‰∏éËÆ≠ÁªÉËøáÁöÑÊ®°ÂûãÂä†ÂÖ•‰∏Ä‰∫õÊñ∞ÁöÑÁü•ËØÜËøõË°åÂæÆË∞É„ÄÇÂ¶ÇÊûúÊñ∞ÁöÑÊï∞ÊçÆÊòØÈ¢ÑËÆ≠ÁªÉÁöÑÂêéÈù¢Ôºå‰πüÂèØ‰ª•Âè´continued pretraining„ÄÇ</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903194702282.png"
	width="704"
	height="363"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903194702282_hu_ef5bfaca2f0c0697.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903194702282_hu_5326aa4cc5696be2.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="193"
		data-flex-basis="465px"
	
></p>
<h3 id="evaluating-large-language-models">Evaluating Large Language Models
</h3><h4 id="perplexity">Perplexity
</h4><p>Áî±‰∫éÈìæÂºèÊ≥ïÂàôÔºå‰ΩøÁî®<strong>ÂØπÊï∞‰ººÁÑ∂</strong>Êù•‰Ωú‰∏∫Ë°°ÈáèËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÊåáÊ†áÁöÑËØùÔºåÊµãËØïÈõÜÁöÑÊ¶ÇÁéáÂ§ßÂ∞è‰ºöÂèóÂà∞tokenÊï∞ÈáèÁöÑÂΩ±Âìç„ÄÇÊñáÊú¨Ë∂äÈïøÔºåÊµãËØïÈõÜÁöÑÊ¶ÇÁéáË∂äÂ∞è„ÄÇ</p>
<p>PerplexityÔºöÂõ∞ÊÉëÂ∫¶ÔºåÈïøÂ∫¶ÂΩí‰∏ÄÂåñÁöÑÊåáÊ†á„ÄÇÂõ∞ÊÉëÂ∫¶ÁöÑÂÖ∑‰ΩìÂÖ¨ÂºèÊòØÊµãËØïÈõÜÊ¶ÇÁéáÁöÑÂÄíÊï∞ÔºåÂÜçÊåâÊ†áËÆ∞Êï∞ÈáèËøõË°åÂΩí‰∏ÄÂåñ„ÄÇ
</p>
$$\begin{aligned}
\text{Perplexity}_{\boldsymbol{\theta}}(w_{1:n}) & =P_{\boldsymbol{\theta}}(w_{1:n})^{-\frac{1}{n}} \\
 & =\sqrt[n]{\frac{1}{P_{\boldsymbol{\theta}}(w_{1:n})}}
\end{aligned}$$<p>
<strong>Âõ∞ÊÉëÂ∫¶Ë∂ä‰ΩéÔºåÊ®°ÂûãË∂äÂ•Ω„ÄÇ</strong></p>
<blockquote>
<p>Âõ∞ÊÉëÂ∫¶ÈùûÂ∏∏‰æùËµñtokensÁöÑÊï∞ÈáèÔºåÂõ†Ê≠§‰∏çËÉΩÂØπ‰∏§‰∏™‰ΩøÁî®‰∏çÂêåtokenizerÁöÑÊ®°ÂûãËøõË°åÊØîËæÉÔºåËÄåÊòØÂè™ËÉΩÂØπ‰ΩøÁî®Âêå‰∏Ä‰∏™tokenizerÁöÑÊ®°ÂûãÊØîËæÉ„ÄÇ</p></blockquote>
<h4 id="downstream-tasks-reasoniong-and-world-knowledge">Downstream Tasks: Reasoniong and World Knowledge
</h4><p>ÂáÜÁ°ÆÁéáÔºöÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®‰∏ãÊ∏∏‰ªªÂä°Êù•Ë°°Èáè„ÄÇ</p>
<ul>
<li>MMLU
<img src="/2025/speech-and-language-processing/assets/IMG-20250903214854478.png"
	width="805"
	height="289"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903214854478_hu_f3db381a926f2221.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903214854478_hu_7d93c033b6db94ba.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="278"
		data-flex-basis="668px"
	
>
<img src="/2025/speech-and-language-processing/assets/IMG-20250903214915252.png"
	width="806"
	height="475"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903214915252_hu_101f6843eda66160.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903214915252_hu_d2314743aaea0c7f.png 1024w"
	loading="lazy"
	
		alt="‰∏Ä‰∏™2-shot promptÁöÑMMLUÈ´ò‰∏≠Êï∞Â≠¶È¢ò"
	
	
		class="gallery-image" 
		data-flex-grow="169"
		data-flex-basis="407px"
	
></li>
</ul>
<p>‰ΩÜÊòØÈóÆÈ¢òÊòØ<strong>Êï∞ÊçÆÊ≥ÑÈú≤</strong>„ÄÇ</p>
<h4 id="other-factors-for-evaluating-language-models">Other Factors for Evaluating Language Models
</h4><p>Ê®°ÂûãÂ§ßÂ∞èÔºåËÆ≠ÁªÉÊó∂Èó¥ÔºåÊé®ÁêÜÊó∂Èó¥ÔºåGPUÊï∞Èáè</p>
<p>ÂÖ¨Âπ≥</p>
<p>leaderboards</p>
<h3 id="ethical-and-safety-issues-with-language-models">Ethical and Safety Issues with Language Models
</h3><p>Â§ßÊ®°ÂûãÂπªËßâÈóÆÈ¢òÔºöRAG</p>
<p>ÂÆâÂÖ®ÈóÆÈ¢òÔºöÂÆâÂÖ®ÂæÆË∞ÉÂíåÂØπÈΩê</p>
<p>representational harms</p>
<p>ÈöêÁßÅÈóÆÈ¢ò</p>
<p>ÊÉÖÊÑü‰æùËµñ</p>
<p>Â¢ûÈïøË∞éË®Ä„ÄÅÂÆ£‰º†„ÄÅËôöÂÅá‰ø°ÊÅØÁ≠âÊñáÊú¨ÁîüÊàê</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/llm/">LLM</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>All rights reserved.</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    


<aside class="related-content--wrapper">
    <h2 class="section-title">Áõ∏ÂÖ≥ÊñáÁ´†</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/2025/mcts/">
        
        

        <div class="article-details">
            <h2 class="article-title">ËÆ∫ÊñáÈòÖËØª| MCTS</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/2024/langchain-learning/">
        
        
            <div class="article-image">
                <img src="/2024/langchain-learning/cover.ddb755e13e74b4dc9c3215937020b858_hu_f1f250f84bcd151e.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post Â≠¶‰π†Á¨îËÆ∞ | LangChainÂ≠¶‰π†Á¨îËÆ∞"
                        data-key="langchain-learning" 
                        data-hash="md5-3bdV4T50tNycMhWTcCC4WA==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Â≠¶‰π†Á¨îËÆ∞ | LangChainÂ≠¶‰π†Á¨îËÆ∞</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/2025/rbdr/">
        
        

        <div class="article-details">
            <h2 class="article-title">AIDR | Reasoning-based Drug Repurposing</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 ionfeather&#39;Log
    </section>
    
    <section class="powerby">
        ‰ΩøÁî® <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> ÊûÑÂª∫ <br />
        ‰∏ªÈ¢ò <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.29.0">Stack</a></b> Áî± <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> ËÆæËÆ°
    </section>
</footer>


<script>
    (function(u, c) {
      var d = document, t = 'script', o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function(e) { c(e); }); }
      s.parentNode.insertBefore(o, s);
    })('//cdn.bootcss.com/pangu/3.3.0/pangu.min.js', function() {
      pangu.spacingPage();
    });
</script>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>
<script src="https://npm.elemecdn.com/nprogress@0.2.0/nprogress.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://npm.elemecdn.com/nprogress@0.2.0/nprogress.css" crossorigin="anonymous" />
<script>
    NProgress.start();
    document.addEventListener("readystatechange", () => {
        if (document.readyState === "interactive") NProgress.inc(0.8);
        if (document.readyState === "complete") NProgress.done();
    });
</script>


    </body>
</html>
