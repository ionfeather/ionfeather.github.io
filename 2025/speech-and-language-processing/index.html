<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Introduction\ré˜…è¯»Speech and Language Processingè¿™æœ¬ä¹¦çš„ä¸€äº›ç¬”è®°ã€‚\nWords and Tokens\ræˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸œè¥¿æ¥å»ºæ¨¡è¯­è¨€ï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬çš„é€‰æ‹©ï¼š\nWords\rä¸ºä»€ä¹ˆä¸ç”¨è¯ï¼Ÿ\næœ‰äº›è¯­è¨€æ²¡æœ‰orthographic words è¯çš„æ•°é‡ä¼šéšç€æ–‡ç« å¢é•¿ï¼Œè¯æ±‡è¡¨æ°¸è¿œéƒ½ä¼šè¦†ç›–ä¸è¶³ Morphemes\rè¯­ç´ ç±»å‹\n">
<title>ä¹¦ç±é˜…è¯» | Speech and Language Processing</title>

<link rel='canonical' href='https://ionfeather.github.io/2025/speech-and-language-processing/'>

<link rel="stylesheet" href="/scss/style.min.cdd95828ca8971b17ccb14112222a60d19d84ea3f4e5b525c8c68fb4d2a4535d.css"><meta property='og:title' content="ä¹¦ç±é˜…è¯» | Speech and Language Processing">
<meta property='og:description' content="Introduction\ré˜…è¯»Speech and Language Processingè¿™æœ¬ä¹¦çš„ä¸€äº›ç¬”è®°ã€‚\nWords and Tokens\ræˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸œè¥¿æ¥å»ºæ¨¡è¯­è¨€ï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬çš„é€‰æ‹©ï¼š\nWords\rä¸ºä»€ä¹ˆä¸ç”¨è¯ï¼Ÿ\næœ‰äº›è¯­è¨€æ²¡æœ‰orthographic words è¯çš„æ•°é‡ä¼šéšç€æ–‡ç« å¢é•¿ï¼Œè¯æ±‡è¡¨æ°¸è¿œéƒ½ä¼šè¦†ç›–ä¸è¶³ Morphemes\rè¯­ç´ ç±»å‹\n">
<meta property='og:url' content='https://ionfeather.github.io/2025/speech-and-language-processing/'>
<meta property='og:site_name' content='ionfeather&#39;Log'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='LLM' /><meta property='article:published_time' content='2025-08-28T19:28:49&#43;08:00'/><meta property='article:modified_time' content='2025-08-28T19:28:49&#43;08:00'/>
<meta name="twitter:title" content="ä¹¦ç±é˜…è¯» | Speech and Language Processing">
<meta name="twitter:description" content="Introduction\ré˜…è¯»Speech and Language Processingè¿™æœ¬ä¹¦çš„ä¸€äº›ç¬”è®°ã€‚\nWords and Tokens\ræˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸œè¥¿æ¥å»ºæ¨¡è¯­è¨€ï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬çš„é€‰æ‹©ï¼š\nWords\rä¸ºä»€ä¹ˆä¸ç”¨è¯ï¼Ÿ\næœ‰äº›è¯­è¨€æ²¡æœ‰orthographic words è¯çš„æ•°é‡ä¼šéšç€æ–‡ç« å¢é•¿ï¼Œè¯æ±‡è¡¨æ°¸è¿œéƒ½ä¼šè¦†ç›–ä¸è¶³ Morphemes\rè¯­ç´ ç±»å‹\n">
    <link rel="shortcut icon" href="/ion.ico" />

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&display=swap"
    onload="this.media='all'" onError="this.media='none'">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@callmebill/lxgw-wenkai-web@latest/style.css"
    onload="this.media='all'" onError="this.media='none'">

  <style>
     
    :root {
      --sys-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --zh-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --base-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --code-font-family: 'Consolas', monospace; 
      --article-font-family: 'Noto Serif SC', serif; 
      --heading-font-family: 'LXGW WenKai', serif; 
    }

     
    body {
      font-family: var(--base-font-family);
      font-weight: normal;
    }
  </style>
</head>

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="åˆ‡æ¢èœå•">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_7d702df343b40e37.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">ğŸŒ³</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">ionfeather&#39;Log</a></h1>
            <h2 class="site-description">åå¹´é¥®å†°ï¼Œéš¾å‡‰çƒ­è¡€</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/ionfeather'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg t="1732597946058" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3746" width="24" height="24"><path d="M850.346667 155.008a42.666667 42.666667 0 0 0-22.741334-23.509333c-8.704-3.754667-85.717333-33.322667-200.32 39.168H396.714667c-114.773333-72.618667-191.701333-42.922667-200.32-39.168a42.88 42.88 0 0 0-22.741334 23.466666c-26.197333 66.218667-18.048 136.448-7.850666 176.896C134.272 374.016 128 413.098667 128 469.333333c0 177.877333 127.104 227.882667 226.730667 246.272a189.568 189.568 0 0 0-13.013334 46.549334A44.373333 44.373333 0 0 0 341.333333 768v38.613333c-19.498667-4.138667-41.002667-11.946667-55.168-26.112C238.08 732.416 188.330667 682.666667 128 682.666667v85.333333c25.002667 0 65.365333 40.362667 97.834667 72.832 51.029333 51.029333 129.066667 55.253333 153.386666 55.253333 3.114667 0 5.376-0.085333 6.528-0.128A42.666667 42.666667 0 0 0 426.666667 853.333333v-82.090666c4.266667-24.746667 20.224-49.621333 27.946666-56.362667a42.666667 42.666667 0 0 0-23.125333-74.581333C293.333333 624.554667 213.333333 591.488 213.333333 469.333333c0-53.12 5.632-70.741333 31.573334-99.285333 11.008-12.117333 14.08-29.568 7.978666-44.8-4.821333-11.904-18.773333-65.450667-6.485333-117.546667 20.650667-1.578667 59.904 4.565333 113.706667 40.96C367.104 253.44 375.466667 256 384 256h256a42.666667 42.666667 0 0 0 23.936-7.338667c54.016-36.522667 92.970667-41.770667 113.664-41.130666 12.330667 52.224-1.578667 105.770667-6.4 117.674666a42.666667 42.666667 0 0 0 8.021333 44.928C805.077333 398.464 810.666667 416.085333 810.666667 469.333333c0 122.581333-79.957333 155.52-218.069334 170.922667a42.666667 42.666667 0 0 0-23.125333 74.709333c19.797333 17.066667 27.861333 32.469333 27.861333 53.034667v128h85.333334v-128c0-20.437333-3.925333-38.101333-9.770667-53.12C769.92 695.765333 896 643.712 896 469.333333c0-56.362667-6.272-95.530667-37.76-137.514666 10.197333-40.405333 18.261333-110.506667-7.893333-176.810667z" fill="currentColor" p-id="3747"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='mailto:lizishadowmay@gmail.com'
                        target="_blank"
                        title="Email"
                        rel="me"
                    >
                        
                        
                            <svg t="1732597869588" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23464" width="24" height="24"><path d="M926.47619 355.644952V780.190476a73.142857 73.142857 0 0 1-73.142857 73.142857H170.666667a73.142857 73.142857 0 0 1-73.142857-73.142857V355.644952l73.142857 62.000762V780.190476h682.666666V417.645714l73.142857-62.000762zM853.333333 170.666667a74.044952 74.044952 0 0 1 26.087619 4.778666 72.704 72.704 0 0 1 30.622477 22.186667 73.508571 73.508571 0 0 1 10.678857 17.67619c3.169524 7.509333 5.12 15.652571 5.607619 24.210286L926.47619 243.809524v24.380952L559.469714 581.241905a73.142857 73.142857 0 0 1-91.306666 2.901333l-3.632762-2.925714L97.52381 268.190476v-24.380952a72.899048 72.899048 0 0 1 40.155428-65.292191A72.97219 72.97219 0 0 1 170.666667 170.666667h682.666666z m-10.971428 73.142857H181.638095L512 525.58019 842.361905 243.809524z" p-id="23465" fill="currentColor"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='index.xml'
                        target="_blank"
                        title="RSS"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon" viewBox="0 0 1024 1024" width="200" height="200">
  <path 
    d="M170.666667 426.666667c-25.6 0-42.666667 17.066667-42.666667 42.666666s17.066667 42.666667 42.666667 42.666667c187.733333 0 341.333333 153.6 341.333333 341.333333 0 25.6 17.066667 42.666667 42.666667 42.666667s42.666667-17.066667 42.666666-42.666667c0-234.666667-192-426.666667-426.666666-426.666666z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
  <path 
    d="M170.666667 128c-25.6 0-42.666667 17.066667-42.666667 42.666667s17.066667 42.666667 42.666667 42.666666c354.133333 0 640 285.866667 640 640 0 25.6 17.066667 42.666667 42.666666 42.666667s42.666667-17.066667 42.666667-42.666667c0-401.066667-324.266667-725.333333-725.333333-725.333333z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
  <path 
    d="M213.333333 810.666667m-85.333333 0a85.333333 85.333333 0 1 0 170.666667 0 85.333333 85.333333 0 1 0-170.666667 0Z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
</svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>ä¸»é¡µ</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>å…³äº</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>å½’æ¡£</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>æœç´¢</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%8F%8B%E9%93%BE/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>å‹é“¾</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            

                
                    <span id="dark-mode-toggle">
                        <svg  xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left"   width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828a4 4 0 1 0 -5.656 -5.656a4 4 0 0 0 5.656 5.656z" /><path d="M6.343 17.657l-1.414 1.414" /><path d="M6.343 6.343l-1.414 -1.414" /><path d="M17.657 6.343l1.414 -1.414" /><path d="M17.657 17.657l1.414 1.414" /><path d="M4 12h-2" /><path d="M12 4v-2" /><path d="M20 12h2" /><path d="M12 20v2" /></svg>
                        <svg  xmlns="http://www.w3.org/2000/svg"  class="icon icon-tabler icon-tabler-toggle-right"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-moon"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" /></svg>
                    </span>
                
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">ç›®å½•</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#words-and-tokens">Words and Tokens</a>
      <ol>
        <li><a href="#words">Words</a></li>
        <li><a href="#morphemes">Morphemes</a></li>
        <li><a href="#unicode">Unicode</a>
          <ol>
            <li><a href="#code-points">Code Points</a></li>
            <li><a href="#utf-8">UTF-8</a></li>
          </ol>
        </li>
        <li><a href="#subword-tokenization-byte-pair-encoding">Subword Tokenization: Byte-Pair Encoding</a>
          <ol>
            <li><a href="#bpe">BPE</a></li>
            <li><a href="#bpe-encoder">BPE encoder</a></li>
            <li><a href="#bpe-in-practice">BPE in practice</a></li>
          </ol>
        </li>
        <li><a href="#rule-based-tokenization">Rule-based tokenization</a>
          <ol>
            <li><a href="#sentence-segmentation">Sentence Segmentation</a></li>
          </ol>
        </li>
        <li><a href="#corpora">Corpora</a></li>
        <li><a href="#regular-expressions">Regular Expressions</a></li>
        <li><a href="#simple-unix-tools-for-word-tokenization">Simple Unix Tools for Word Tokenization</a></li>
        <li><a href="#minimum-edit-distance">Minimum Edit Distance</a>
          <ol>
            <li><a href="#the-minimum-edit-distance-algorithm">The Minimum Edit Distance Algorithm</a></li>
          </ol>
        </li>
        <li><a href="#exercies">Exercies</a>
          <ol>
            <li></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#n-gram-language-models">N-gram Language Models</a>
      <ol>
        <li><a href="#n-grams">N-Grams</a>
          <ol>
            <li><a href="#how-to-estimate-probabilities">How to estimate probabilities</a></li>
            <li><a href="#dealing-with-scale-in-large-n-gram-models">Dealing with scale in large n-gram models</a></li>
            <li><a href="#evaluating-language-models-training-and-test-sets">Evaluating Language Models: Training and Test Sets</a></li>
            <li><a href="#evaluating-language-models-perplexity">Evaluating Language Models: Perplexity</a></li>
          </ol>
        </li>
        <li><a href="#sampling-sentences-from-a-language-model">Sampling sentences from a language model</a></li>
        <li><a href="#generalizing-vs-overfitting-the-training-set">Generalizing vs. overfitting the training set</a></li>
        <li><a href="#smoothing-interpolation-and-backoff">Smoothing, Interpolation, and Backoff</a>
          <ol>
            <li><a href="#laplace-smoothing">Laplace Smoothing</a></li>
            <li><a href="#add-k-smoothing">Add-k Smoothing</a></li>
            <li><a href="#language-model-interpolation">Language Model Interpolation</a></li>
            <li><a href="#stupid-backoff">Stupid Backoff</a></li>
          </ol>
        </li>
        <li><a href="#advanced-perplexitys-relation-to-entropy">Advanced: Perplexity&rsquo;s Relation to Entropy</a></li>
        <li><a href="#excercies">Excercies</a>
          <ol>
            <li></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#logistic-regression-and-text-classification">Logistic Regression and Text Classification</a>
      <ol>
        <li><a href="#machine-learning-and-classification">Machine Learning and Classification</a></li>
        <li><a href="#the-sigmoid-function">The Sigmoid Function</a></li>
        <li><a href="#classification-with-logistic-regression">Classification with Logistic Regression</a>
          <ol>
            <li><a href="#sentiment-classification">Sentiment Classification</a></li>
            <li><a href="#other-classification-tasks-and-features">Other Classification Tasks and Features</a></li>
            <li><a href="#processing-many-examples-at-once">Processing many examples at once</a></li>
          </ol>
        </li>
        <li><a href="#multinomial-logistic-regression">Multinomial Logistic Regression</a>
          <ol>
            <li><a href="#softmax">Softmax</a></li>
            <li><a href="#applying-softmax-in-logistic-regression">Applying Softmax in Logistic Regression</a></li>
            <li><a href="#features-in-multinomial-logistic-regression">Features in Multinomial Logistic Regression</a></li>
          </ol>
        </li>
        <li><a href="#learning-in-logistic-regression">Learning in Logistic Regression</a></li>
        <li><a href="#the-cross-entropy-loss-function">The Cross-entropy Loss Function</a></li>
        <li><a href="#gradient-descent">Gradient Descent</a>
          <ol>
            <li><a href="#the-gradient-for-logistic-regression">The Gradient for Logistic Regression</a></li>
            <li><a href="#the-stochastic-gradient-descent-algorithm">The Stochastic Gradient Descent Algorithm</a></li>
            <li><a href="#mini-batch-training">Mini-batch Training</a></li>
          </ol>
        </li>
        <li><a href="#learning-in-multinomial-logistic-regression">Learning in Multinomial Logistic Regression</a></li>
        <li><a href="#evaluation-precision-recall-f-measure">Evaluation: Precision, Recall, F-measure</a></li>
        <li><a href="#test-sets-and-cross-validation">Test sets and Cross-validation</a></li>
        <li><a href="#statistical-significance-testing">Statistical Significance Testing</a>
          <ol>
            <li><a href="#the-paired-bootstrap-test">The Paired Bootstrap Test</a></li>
          </ol>
        </li>
        <li><a href="#avoiding-harms-in-classification">Avoiding Harms in Classification</a></li>
        <li><a href="#interpereting-models">Interpereting Models</a></li>
        <li><a href="#advanced-regularization">Advanced: Regularization</a></li>
      </ol>
    </li>
    <li><a href="#embeddings">Embeddings</a>
      <ol>
        <li><a href="#lexical-semantics">Lexical Semantics</a></li>
        <li><a href="#vector-semantics-the-intuition">Vector Semantics: The Intuition</a>
          <ol>
            <li><a href="#simple-count-based-embeddings">Simple Count-based Embeddings</a></li>
            <li><a href="#cosine-for-measuring-similarity">Cosine for Measuring Similarity</a></li>
            <li><a href="#word2vec">Word2vec</a></li>
            <li><a href="#learning-skip-gram-embeddings">Learning Skip-gram Embeddings</a></li>
            <li><a href="#other-kinds-of-static-embeddings">Other kinds of static embeddings</a></li>
          </ol>
        </li>
        <li><a href="#visualizing-embeddings">Visualizing Embeddings</a></li>
        <li><a href="#semantic-properites-of-embeddings">Semantic Properites of Embeddings</a>
          <ol>
            <li><a href="#embeddings-and-historical-semantics">Embeddings and Historical Semantics</a></li>
          </ol>
        </li>
        <li><a href="#bias-and-embeddings">Bias and Embeddings</a></li>
        <li><a href="#evaluating-vector-models">Evaluating Vector Models</a></li>
      </ol>
    </li>
    <li><a href="#neural-networks">Neural Networks</a>
      <ol>
        <li><a href="#units">Units</a></li>
        <li><a href="#the-xor-problem">The XOR Problem</a>
          <ol>
            <li><a href="#the-solution-neural-networks">The Solution: Neural Networks</a></li>
          </ol>
        </li>
        <li><a href="#feedforward-neural-networks">Feedforward Neural Networks</a>
          <ol>
            <li><a href="#more-details-on-feedforward-networks">More Details on Feedforward Networks</a></li>
          </ol>
        </li>
        <li><a href="#feedforward-networks-for-nlp-classification">Feedforward Networks for NLP: Classification</a>
          <ol>
            <li><a href="#neural-net-classifiers-with-hand-built-features">Neural Net Classifiers with Hand-built Features</a></li>
            <li><a href="#vectorizing-for-parallelizing-inference">Vectorizing for Parallelizing Inference</a></li>
          </ol>
        </li>
        <li><a href="#embeddings-as-the-input-to-neural-net-classifiers">Embeddings as the Input to Neural Net Classifiers</a></li>
        <li><a href="#training-neural-nets">Training Neural Nets</a>
          <ol>
            <li><a href="#loss-function">Loss Function</a></li>
            <li><a href="#computing-the-gradient">Computing the Gradient</a></li>
            <li><a href="#computation-graphs">Computation Graphs</a></li>
            <li><a href="#backward-differentiation-on-computation-graphs">Backward Differentiation on Computation Graphs</a></li>
            <li><a href="#more-details-on-learning">More Details on Learning</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#large-language-models">Large Language Models</a>
      <ol>
        <li><a href="#three-architecture-for-language-models">Three Architecture for Language Models</a></li>
        <li><a href="#conditional-generation-of-text-the-intuition">Conditional Generation of Text: The Intuition</a></li>
        <li><a href="#prompting">Prompting</a></li>
        <li><a href="#generation-and-sampling">Generation and Sampling</a>
          <ol>
            <li><a href="#greedy-decoding">Greedy decoding</a></li>
            <li><a href="#random-sampling">Random Sampling</a></li>
            <li><a href="#temperature-sampling">Temperature Sampling</a></li>
          </ol>
        </li>
        <li><a href="#training-large-language-models">Training Large Language Models</a>
          <ol>
            <li><a href="#self-supervised-training-algorithm-for-pretraining">Self-supervised Training Algorithm for Pretraining</a></li>
            <li><a href="#pretraining-corpora-for-large-language-models">Pretraining Corpora for Large Language Models</a></li>
            <li><a href="#finetuning">Finetuning</a></li>
          </ol>
        </li>
        <li><a href="#evaluating-large-language-models">Evaluating Large Language Models</a>
          <ol>
            <li><a href="#perplexity">Perplexity</a></li>
            <li><a href="#downstream-tasks-reasoniong-and-world-knowledge">Downstream Tasks: Reasoniong and World Knowledge</a></li>
            <li><a href="#other-factors-for-evaluating-language-models">Other Factors for Evaluating Language Models</a></li>
          </ol>
        </li>
        <li><a href="#ethical-and-safety-issues-with-language-models">Ethical and Safety Issues with Language Models</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="background-color: #5DB9AE; color: #fff;">
                å­¦ä¹ æœ­è®°
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/2025/speech-and-language-processing/">ä¹¦ç±é˜…è¯» | Speech and Language Processing</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-08-28</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    é˜…è¯»æ—¶é•¿: 23 åˆ†é’Ÿ
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="introduction">Introduction
</h2><p>é˜…è¯»<a class="link" href="https://web.stanford.edu/~jurafsky/slp3/"  target="_blank" rel="noopener"
    >Speech and Language Processing</a>è¿™æœ¬ä¹¦çš„ä¸€äº›ç¬”è®°ã€‚</p>
<h2 id="words-and-tokens">Words and Tokens
</h2><p>æˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸œè¥¿æ¥å»ºæ¨¡è¯­è¨€ï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬çš„é€‰æ‹©ï¼š</p>
<h3 id="words">Words
</h3><p>ä¸ºä»€ä¹ˆä¸ç”¨è¯ï¼Ÿ</p>
<ul>
<li>æœ‰äº›è¯­è¨€æ²¡æœ‰orthographic words</li>
<li>è¯çš„æ•°é‡ä¼šéšç€æ–‡ç« å¢é•¿ï¼Œè¯æ±‡è¡¨æ°¸è¿œéƒ½ä¼šè¦†ç›–ä¸è¶³</li>
</ul>
<h3 id="morphemes">Morphemes
</h3><p>è¯­ç´ ç±»å‹</p>
<ul>
<li>å±ˆæŠ˜è¯­ç´ ï¼šinflectional morphemes</li>
<li>æ´¾ç”Ÿè¯­ç´ ï¼šderivational morphemes</li>
<li>é™„ç€è¯­ç´ ï¼šclitic</li>
</ul>
<p>è¯­è¨€ç±»å‹</p>
<ul>
<li>Analytic</li>
<li>polysynthetic</li>
<li>fusional</li>
<li>agglutinative</li>
</ul>
<p>ä¸ºä»€ä¹ˆä¸ç”¨è¯­ç´ ï¼Ÿ</p>
<ul>
<li>è¯­ç´ å¾ˆå¤æ‚ï¼Œå¾ˆéš¾å®šä¹‰</li>
<li>ä¸åŒè¯­è¨€ä¸åŒä¸”éš¾ä»¥ç»Ÿä¸€</li>
</ul>
<h3 id="unicode">Unicode
</h3><p>Unicodeçš„å†å²</p>
<ul>
<li>ASCII</li>
<li>CJKV</li>
<li>ä¸æ–­æ›´æ–°ä¸­ï¼Œè¶Šæ¥è¶Šå¤šï¼ŒUnicode 16.0å·²ç»åŒ…å«è¶…è¿‡150000ä¸ªå­—ç¬¦</li>
</ul>
<h4 id="code-points">Code Points
</h4><ul>
<li>U+ï¼šè¡¨ç¤ºæ¥ä¸‹æ¥è¦ç”¨Unicodeåå…­è¿›åˆ¶è¡¨ç¤ºä¸€ä¸ªcode point</li>
<li>U+0061ï¼š0x0061ä¸€ä¸ªæ„æ€ï¼Œä¹Ÿå°±å°å†™å­—æ¯aã€‚</li>
</ul>
<h4 id="utf-8">UTF-8
</h4><p>ç›®å‰æœ€å¸¸ç”¨çš„encodingå­—ç¬¦çš„æ–¹å¼ã€‚ä¸­æ–‡å­—ç¬¦ â€œä¸­â€ çš„ Unicode ç ç‚¹æ˜¯<code>U+4E2D</code>ï¼ŒUTF-8 ç¼–ç åä¸º 3 ä¸ªå­—èŠ‚ï¼š<code>0xE4 0xB8 0xAD</code></p>
<p>UTF-8æ˜¯ä¸€ç§å˜é•¿ç¼–ç ï¼Œå…¼å®¹ASCIIã€‚</p>
<ul>
<li>å¦‚ã€Œä¸–ã€ï¼ŒUTF-8 ç¼–ç æ˜¯<code>0xE4 B8 96</code>ï¼Œå…¶ä¸­E4çš„äºŒè¿›åˆ¶ä¸º<code>11100110H</code>ï¼Œå¼€å¤´çš„<code>1110H</code>è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ª3å­—èŠ‚å­—ç¬¦çš„ç¬¬ä¸€ä¸ªå­—èŠ‚ã€‚</li>
</ul>
<h3 id="subword-tokenization-byte-pair-encoding">Subword Tokenization: Byte-Pair Encoding
</h3><p>ä¸Šé¢çš„ä¸‰ä¸ªå€™é€‰éƒ½ä¸è¡Œï¼Œwordå’Œmorphemeéš¾ä»¥è§„èŒƒå®šä¹‰ï¼Œcharacterå¯ä»¥é€šè¿‡unicodeæ¥å®šä¹‰ï¼Œä½†åˆå¯¹äºä½œä¸ºtokensæ¥è¯´å¤ªå°äº†ã€‚</p>
<p>ä¸ºä»€ä¹ˆè¦tokenizeè¾“å…¥ï¼Ÿ</p>
<ul>
<li>å°†è¾“å…¥è½¬æ¢ä¸ºä¸€ç»„ç¡®å®šçš„ã€å›ºå®šçš„å•å…ƒï¼ˆTokenï¼‰ï¼Œèƒ½è®©ä¸åŒçš„ç®—æ³•å’Œç³»ç»Ÿåœ¨ä¸€äº›ç®€å•é—®é¢˜ä¸Šè¾¾æˆå…±è¯†ã€‚ä¾‹å¦‚å›°æƒ‘åº¦çš„è®¡ç®—ã€‚</li>
<li>å¯¹å¯å¤ç°å¾ˆé‡è¦</li>
<li>ä¸ºäº†æ¶ˆé™¤unknown wordsçš„é—®é¢˜</li>
</ul>
<p>ä¸ºäº†æ¶ˆé™¤unknown wordsé—®é¢˜ï¼Œç°ä»£tokenizersè‡ªåŠ¨å¼•å…¥äº†tokenåŒ…å«é‚£äº›æ¯”wordså°çš„tokenï¼Œå«subwordã€‚</p>
<p>ä½¿ç”¨<a class="link" href="https://platform.openai.com/tokenizer"  target="_blank" rel="noopener"
    >Tokenizer - OpenAI API</a>ä¸­çš„<code>GPT-4o &amp;  GPT-4o mini</code>æ¥åˆ†è¯ä¸‹é¢è¿™ä¸€å¤§æ®µè¯ï¼š</p>
<blockquote>
<p>For example, if we had happened not to ever see the word lower, when it appears we could segment it successfully into low and er which we had already seen. In the worst case, a really unusual word (perhaps an acronym like GRPO) could be tokenized as a sequence of individual letters if necessary.</p></blockquote>
<p>æœ€ç»ˆå¾—åˆ°çš„æ˜¯
<img src="/2025/speech-and-language-processing/assets/IMG-20250828205105982.png"
	width="711"
	height="279"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250828205105982_hu_b760c9dcae731844.png 480w, /2025/speech-and-language-processing/assets/IMG-20250828205105982_hu_3812253da13ed9c0.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="254"
		data-flex-basis="611px"
	
>
ç°åœ¨æœ€æµè¡Œçš„tokenization algorithmæœ‰ä¸¤ä¸ªï¼š</p>
<ul>
<li>Byte-Pair Encoding(BPE)</li>
<li>Unigram Language modeling(ULM)</li>
</ul>
<h4 id="bpe">BPE
</h4><p>é€šè¿‡åˆ†æè®­ç»ƒè¯­æ–™ï¼Œè‡ªåŠ¨å­¦ä¹ å‡ºä¸€å¥—å­è¯é›†åˆï¼ˆè¯æ±‡è¡¨ï¼‰ï¼Œä½¿å¾—é«˜é¢‘å‡ºç°çš„å­—ç¬¦ / å­è¯ç»„åˆè¢«åˆå¹¶ä¸ºæ›´å¤§çš„å­è¯å•ä½ã€‚</p>
<p>è®­ç»ƒæ–¹æ³•ä»‹ç»ã€‚</p>
<h4 id="bpe-encoder">BPE encoder
</h4><h4 id="bpe-in-practice">BPE in practice
</h4><p>é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šå¯¹ UTF-8 ç¼–ç æ–‡æœ¬çš„<strong>å•ä¸ªå­—èŠ‚</strong>æ‰§è¡Œ BPE æ“ä½œã€‚BPE å¤„ç† â€œä¸­â€ æ—¶ï¼Œè¾“å…¥å¹¶é<code>U+4E2D</code>è¿™ä¸ªç ç‚¹ï¼Œè€Œæ˜¯<code>E4</code>ã€<code>B8</code>ã€<code>AD</code>è¿™ä¸‰ä¸ªç‹¬ç«‹å­—èŠ‚ã€‚</p>
<p>ä»…åœ¨<strong>é¢„å…ˆåˆ‡åˆ†å‡ºçš„å•è¯å†…éƒ¨</strong>æ‰§è¡Œ BPE æ“ä½œï¼Œæœ‰åŠ©äºé¿å…æ½œåœ¨é—®é¢˜ã€‚</p>
<p>ä¸€äº›è‹±è¯­é‡Œçš„å°å‘ç°ï¼š</p>
<ul>
<li>å¤§å¤šæ•°å•è¯çš„tokensæ˜¯ä»–ä»¬è‡ªå·±ï¼ŒåŒ…å«è¯å‰ç©ºæ ¼ã€‚è¿™æ ·å¯ä»¥é¿å…ç‹¬ç«‹å•è¯å’Œå•è¯å†…éƒ¨çš„subwordã€‚</li>
<li>é™„ç€è¯­ç´ Cliticsåœ¨åå­—åé¢åˆ†å¼€å•ç‹¬æˆtokenï¼Œä½†åœ¨å¸¸è§çš„è¯è¯­åé¢ä¼šæ˜¯tokençš„ä¸€éƒ¨åˆ†</li>
<li>æ•°å­—é€šå¸¸ä¸‰ä½ä¸€ç»„</li>
<li>ä¸€äº›è¯ï¼Œå¦‚Anyhowå’Œanyhowä¼šæœ‰ä¸åŒçš„åˆ†å‰²æ–¹æ³•</li>
</ul>
<p>è¿™ä¸ªå’Œé¢„å¤„ç†æœ‰å…³ç³»ã€‚</p>
<p>SuperBPEä¼šåˆå¹¶å¸¸è§„çš„BPEå­è¯åˆ†è¯ï¼Œæ•ˆç‡æ›´é«˜ã€‚</p>
<p>ç‰¹åˆ«åœ°ï¼Œä½èµ„æºè¯­è¨€çš„tokensæ›´ç¢ï¼Œå°±ä¼šè¾“å‡ºè¾¹é•¿ï¼Œæœ€ç»ˆLLMçš„æ•ˆç‡å˜ä½ã€‚</p>
<h3 id="rule-based-tokenization">Rule-based tokenization
</h3><p>Penn Treebank Tokenization Standardï¼‰ï¼šäº‹å®æ€§è§„èŒƒã€‚</p>
<ul>
<li>åˆ†å¼€é™„ç€è¯­ç´ </li>
<li>ä¿ç•™è¿å­—ç¬¦è¿æ¥çš„è¯</li>
<li>åˆ†å¼€æ‰€æœ‰çš„æ ‡ç‚¹ç¬¦å·</li>
</ul>
<h4 id="sentence-segmentation">Sentence Segmentation
</h4><p>sentence tokenizationå¯ä»¥å’Œword tokenizationè”åˆå¤„ç†ã€‚</p>
<h3 id="corpora">Corpora
</h3><p>è¯­æ–™åº“å’Œè¯­è¨€æ•°é‡ã€ä½¿ç”¨è€…çš„ç‰¹å¾éƒ½æœ‰å…³ã€‚</p>
<p>code switchingï¼šåœ¨ä¸€æ¬¡æŒç»­çš„äº¤æµï¼‰ä¸­ï¼Œè¯´è¯è€…æˆ–ä½œè€…äº¤æ›¿ä½¿ç”¨ä¸¤ç§æˆ–å¤šç§ â€œè¯­ç â€çš„ç°è±¡ã€‚</p>
<p>datasheetï¼šå­˜å‚¨ä¸€å¥è¯çš„ç‰¹å¾ï¼Œå¦‚æ—¶é—´ã€è¯´è¯äººæ€§æ ¼ã€é˜¶çº§&hellip;</p>
<h3 id="regular-expressions">Regular Expressions
</h3><p>æ­£åˆ™è¡¨è¾¾å¼çš„å…·ä½“å®ç°ã€‚åŒ…å«å­—ç¬¦æå–ã€è®¡æ•°ã€å¯é€‰æ€§ã€é€šé…ç¬¦ã€é”šç‚¹å’Œè¾¹ç•Œã€æ›¿æ¢å’Œæ•è·ç»„ã€å‰å‘æ–­è¨€ç­‰ã€‚</p>
<h3 id="simple-unix-tools-for-word-tokenization">Simple Unix Tools for Word Tokenization
</h3><p>å¯ä»¥åœ¨Unixã€Linuxç³»ç»Ÿä¸­ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ã€‚å¦‚<code>tr -sc 'A-Za-z' '\n' &lt; sh.txt</code>è¡¨ç¤ºä»Â <code>sh.txt</code>Â æ–‡ä»¶ä¸­æå–æ‰€æœ‰è‹±æ–‡å­—æ¯ï¼Œå¹¶å°†éå­—æ¯å­—ç¬¦æ›¿æ¢ä¸ºæ¢è¡Œç¬¦ï¼ŒåŒæ—¶å‹ç¼©è¿ç»­çš„éå­—æ¯å­—ç¬¦ä¸ºå•ä¸ªæ¢è¡Œç¬¦ã€‚</p>
<h3 id="minimum-edit-distance">Minimum Edit Distance
</h3><p><strong>æœ€å°ç¼–è¾‘è·ç¦»</strong>ï¼šå°†ä¸€ä¸ªå­—ç¬¦ä¸²é€šè¿‡ â€œæ’å…¥â€â€œåˆ é™¤â€â€œæ›¿æ¢â€ ä¸‰ç§åŸºæœ¬æ“ä½œè½¬æ¢ä¸ºå¦ä¸€ä¸ªå­—ç¬¦ä¸²æ‰€éœ€çš„æœ€å°‘æ“ä½œæ¬¡æ•°</p>
<h4 id="the-minimum-edit-distance-algorithm">The Minimum Edit Distance Algorithm
</h4><p>ä¸€ä¸ªç»å…¸çš„åŠ¨æ€è§„åˆ’é—®é¢˜ã€‚</p>
<p><strong>å­—ç¬¦å¯¹é½</strong>ï¼šé€šè¿‡å›æº¯ç¼–è¾‘è·ç¦»çŸ©é˜µä¸­çš„ â€œæœ€ä¼˜è·¯å¾„â€ï¼Œåå‘æ¨å¯¼å‡ºå°†ä¸€ä¸ªå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¦ä¸€ä¸ªå­—ç¬¦ä¸²çš„å…·ä½“æ“ä½œåºåˆ—ã€‚ä¹Ÿå°±æ˜¯<strong>è·¯å¾„å¯è§†åŒ–</strong>ã€‚</p>
<h3 id="exercies">Exercies
</h3><h5 id="21">2.1
</h5><p>Write regular expressions for the following languages.</p>
<ol>
<li>The set of all alphabetic strings.</li>
<li>The set of all lowercase alphabetic strings ending in &ldquo;b&rdquo;.</li>
<li>The set of all strings from the alphabet {a, b} such that each &ldquo;a&rdquo; is immediately preceded by and immediately followed by a &ldquo;b&rdquo;.</li>
</ol>
<h5 id="22">2.2
</h5><p>Write regular expressions for the following languages. By &ldquo;word&rdquo;, we mean an alphabetic string separated from other words by whitespace, relevant punctuation, line breaks, etc.</p>
<ol>
<li>The set of all strings with two consecutive repeated words (e.g., &ldquo;Humbert Humbert&rdquo; and &ldquo;the the&rdquo; but not &ldquo;the bug&rdquo; or &ldquo;the big bug&rdquo;).</li>
<li>All strings that start at the beginning of the line with an integer and end at the end of the line with a word.</li>
<li>All strings that have both the word &ldquo;grotto&rdquo; and the word &ldquo;raven&rdquo; in them (but not, e.g., words like &ldquo;grottos&rdquo; that merely contain &ldquo;grotto&rdquo;).</li>
<li>Write a pattern that places the first word of an English sentence in a register. Deal with punctuation.</li>
</ol>
<h5 id="23">2.3
</h5><p>Implement an ELIZA-like program, using substitutions such as those described on page 27. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your program can legitimately engage in a lot of simple repetition.</p>
<h5 id="24">2.4
</h5><p>Compute the edit distance (using insertion cost 1, deletion cost 1, substitution cost 1) of &ldquo;leda&rdquo; to &ldquo;deal&rdquo;. Show your work (using the edit distance grid).</p>
<h5 id="25">2.5
</h5><p>Figure out whether &ldquo;drive&rdquo; is closer to &ldquo;brief&rdquo; or to &ldquo;divers&rdquo; and what the edit distance is to each. You may use any version of distance that you like.</p>
<h5 id="26">2.6
</h5><p>Now implement a minimum edit distance algorithm and use your hand-computed results to check your code.</p>
<h5 id="27">2.7
</h5><p>Augment the minimum edit distance algorithm to output an alignment; you will need to store pointers and add a stage to compute the backtrace.</p>
<h2 id="n-gram-language-models">N-gram Language Models
</h2><p>æœ¬ç« ä»‹ç»æœ€ç®€å•çš„è¯­è¨€æ¨¡å‹ï¼š<strong>Nå…ƒè¯­æ³•è¯­è¨€æ¨¡å‹</strong>ã€‚</p>
<h3 id="n-grams">N-Grams
</h3><p><strong>æ¦‚ç‡é“¾å¼æ³•åˆ™</strong></p>
<h4 id="how-to-estimate-probabilities">How to estimate probabilities
</h4><p><strong>é©¬å°”ç§‘å¤«å‡è®¾</strong>ï¼šå‡è®¾ä¸€ä¸ªå•è¯çš„å‡ºç°æ¦‚ç‡åªå’Œå‰é¢çš„ä¸€ä¸ªå•è¯æœ‰å…³ã€‚é‚£ä¹ˆn-gramå³åªå’Œå‰é¢çš„$n-1$ä¸ªå•è¯æœ‰å…³ã€‚</p>
<p><strong>æœ€å¤§ä¼¼ç„¶ä¼°è®¡</strong>ï¼šå·²çŸ¥å‰ä¸€ä¸ªè¯$w_{nâˆ’1}$â€‹æ—¶ï¼Œå½“å‰è¯$w_n$â€‹çš„æ¦‚ç‡</p>
<p>ç»ˆæ­¢ç¬¦å·ï¼ˆend-symbolï¼‰ï¼šæ‰€æœ‰å¯èƒ½å¥å­çš„æ¦‚ç‡æ€»å’Œä¸º 1ï¼Œå¦åˆ™æ˜¯ç‰¹å®šé•¿åº¦çš„æ‰€æœ‰å¥å­æ¦‚ç‡ä¹‹å’Œä¸º 1ã€‚</p>
<h4 id="dealing-with-scale-in-large-n-gram-models">Dealing with scale in large n-gram models
</h4><p>Log probabilities</p>
<p>Nå…ƒè¯­æ³•çš„è®¡ç®—ç°åœ¨ç”šè‡³èƒ½è¾¾åˆ°æ— é™å…ƒã€‚</p>
<p>å¯¹Nå…ƒè¯­æ³•æ¨¡å‹è¿›è¡Œä¿®å‰ªä¹Ÿæ˜¯å¾ˆé‡è¦çš„ã€‚</p>
<h4 id="evaluating-language-models-training-and-test-sets">Evaluating Language Models: Training and Test Sets
</h4><p><strong>å†…éƒ¨è¯„ä¼°</strong>å’Œå¤–éƒ¨è¯„ä¼°ã€‚</p>
<p>è®­ç»ƒé›†ã€å¼€å‘é›†å’Œæµ‹è¯•é›†ã€‚</p>
<h4 id="evaluating-language-models-perplexity">Evaluating Language Models: Perplexity
</h4><p><strong>Perplexityï¼ˆPPLï¼‰</strong>ï¼šå›°æƒ‘åº¦è¶Šä½ï¼Œè¯´æ˜æ¨¡å‹å¯¹æ–‡æœ¬çš„é¢„æµ‹è¶Šå‡†ç¡®ï¼ˆå³æ¨¡å‹è¶Š â€œä¸å›°æƒ‘â€ï¼‰ã€‚</p>
<ul>
<li>å…·ä½“æ¥è¯´ï¼Œæ˜¯â€œè”åˆæ¦‚ç‡å€’æ•°çš„å‡ ä½•å¹³å‡å€¼â€ã€‚</li>
<li>åœ¨è®¡ç®—çš„æ—¶å€™å¸¸å¸¸ä¼šå–å¯¹æ•°æ¥å°†æ±‚ä¹˜ç§¯å˜ä¸ºæ±‚å’Œï¼Œé¿å…æ•°å€¼é—®é¢˜</li>
</ul>
<h5 id="perplexity-as-weighted-average-branching-factor">Perplexity as Weighted Average Branching Factor
</h5><p>å›°æƒ‘åº¦ä¹Ÿå¯ä»¥ç†è§£ä¸º<strong>åŠ æƒå¹³å‡åˆ†æ”¯ç³»æ•°</strong>ã€‚å…¶ä¸­ï¼Œè¯­è¨€çš„ â€œåˆ†æ”¯ç³»æ•°â€æŒ‡çš„æ˜¯ â€œä»»ä½•ä¸€ä¸ªè¯ä¹‹åå¯èƒ½å‡ºç°çš„ä¸‹ä¸€ä¸ªè¯çš„æ•°é‡â€ã€‚</p>
<h3 id="sampling-sentences-from-a-language-model">Sampling sentences from a language model
</h3><p>â€œ0-1 æ•°è½´ + åŒºé—´æ˜ å°„â€æ¥ç†è§£é‡‡æ ·çš„åŸºæœ¬åŸç†ã€‚</p>
<h3 id="generalizing-vs-overfitting-the-training-set">Generalizing vs. overfitting the training set
</h3><p>å¯¹äºèå£«æ¯”äºšæ–‡æœ¬å’Œåå°”è¡—æ—¥æŠ¥çš„æ–‡æœ¬ï¼Œä¸¤è€…å·®å¼‚è¿‡å¤§ä»¥è‡³äºä¸èƒ½åˆ†åˆ«ä½œä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚</p>
<p>æ‰€ä»¥è¯´è¦ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„é¢†åŸŸè¦ç›¸ä¼¼ã€‚</p>
<h3 id="smoothing-interpolation-and-backoff">Smoothing, Interpolation, and Backoff
</h3><p>zero probability n-gramsæœ‰ä¸¤ä¸ªé—®é¢˜ï¼š</p>
<ul>
<li>ä½ä¼°äº†è¯è¯­åºå¯èƒ½å‡ºç°çš„å¯èƒ½æ€§ï¼Œå¯¼è‡´æœ€ç»ˆçš„æ€§èƒ½å˜å·®</li>
<li>å›°æƒ‘åº¦æ— æ³•è®¡ç®—ï¼Œå› ä¸ºæ— æ³•é™¤ä»¥0</li>
</ul>
<p>å› æ­¤éœ€è¦Smoothingæˆ–è€…discounting</p>
<h4 id="laplace-smoothing">Laplace Smoothing
</h4><p>å…¶å®ä¹Ÿå°±æ˜¯add one smoothingï¼Œå°±æ˜¯å¯¹äºæ‰€æœ‰çš„Nå…ƒè¯­æ³•éƒ½åŠ ä¸€ã€‚</p>
<p>å¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œç»“æœå¹¶ä¸æ˜¯å¾ˆå¥½ã€‚å¯¹æ–‡æœ¬åˆ†ç±»æœ‰æ•ˆã€‚</p>
<h4 id="add-k-smoothing">Add-k Smoothing
</h4><p>ä¹Ÿå°±æ˜¯å¯¹æ‰€æœ‰çš„éƒ½åŠ Kã€‚</p>
<p>å¯¹è¯­è¨€æ¨¡å‹æ¥è¯´ä»ç„¶æ•ˆæœä¸€èˆ¬ã€‚</p>
<h4 id="language-model-interpolation">Language Model Interpolation
</h4><p><strong>n å…ƒè¯­æ³•æ’å€¼æ³•ï¼šåŠ æƒèåˆä¸åŒé˜¶æ•° n å…ƒè¯­æ³•çš„æ¦‚ç‡</strong>ï¼Œé¿å…é«˜é˜¶çš„nå…ƒè¯­æ³•é›¶æ¦‚ç‡å¯¼è‡´çš„é¢„æµ‹å¤±æ•ˆã€‚</p>
<p>åŠ æƒçš„$\lambda$åº”è¯¥è®¾ç½®æˆå¤šå°‘å‘¢ï¼Ÿå¯ä»¥ä»é¢„ç•™é›†held-out corpusä¸­å­¦ä¹ ã€‚ä½¿ç”¨EMï¼ˆæœŸæœ›æœ€å¤§åŒ–ï¼‰ç®—æ³•æ¥å­¦ä¹ ã€‚</p>
<h4 id="stupid-backoff">Stupid Backoff
</h4><p><strong>å›é€€æ¨¡å‹</strong>ï¼šé«˜é˜¶né˜¶çš„æ¨¡å‹æ— æ³•ä½¿ç”¨çš„æ—¶å€™ï¼Œå›é€€åˆ°ä½é˜¶æ¨¡å‹ã€‚</p>
<p><strong>Discount</strong>ï¼šè¦è®©å›é€€æ¨¡å‹ï¼ˆbackoff modelï¼‰è¾“å‡ºåˆç†çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¿…é¡»å¯¹é«˜é˜¶ n å…ƒè¯­æ³•çš„æ¦‚ç‡è¿›è¡Œ â€œæŠ˜æ‰£å¤„ç†â€ï¼ˆdiscountï¼‰ï¼Œä»è€Œé¢„ç•™å‡ºéƒ¨åˆ†æ¦‚ç‡ä½™é‡ï¼ˆprobability massï¼‰ï¼Œä¾›ä½é˜¶ n å…ƒè¯­æ³•ä½¿ç”¨ã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œäººä»¬å¸¸ä½¿ç”¨ä¸€ç§æ›´ç®€å•çš„ â€œæ— æŠ˜æ‰£å›é€€ç®—æ³•â€â€”â€” å³åä¸º<strong>Stupid Backoff</strong>ã€‚</p>
<h3 id="advanced-perplexitys-relation-to-entropy">Advanced: Perplexity&rsquo;s Relation to Entropy
</h3><p><strong>ç†µ</strong>ï¼šä¸ç¡®å®šæ€§çš„åº¦é‡æ–¹å¼ã€‚å¯ä»¥ç†è§£æ˜¯ç¼–ç æŸä¸ªå†³ç­–æˆ–æŸæ¡ä¿¡æ¯æ‰€éœ€çš„æœ€å°å¹³å‡æ¯”ç‰¹æ•°ã€‚è¶Šä¸ç¡®å®šï¼Œç†µè¶Šå¤§ã€‚</p>
<p><strong>ç†µç‡</strong>ï¼šå¹³å‡çš„ä¸ç¡®å®šæ€§ã€‚è‡ªç„¶è¯­è¨€çš„ç†µç‡å®šä¹‰ä¸º â€œ<strong>æ— é™é•¿åºåˆ—ä¸­ï¼Œæ¯ä¸ªè¯çš„å¹³å‡ç†µ</strong>â€ï¼Œåæ˜ è¯­è¨€çš„é•¿æœŸä¸ç¡®å®šæ€§ã€‚ä¾‹å¦‚ï¼Œè‹±æ–‡çš„ç†µç‡çº¦ 1-2 æ¯”ç‰¹ / è¯ï¼Œæ„å‘³ç€å¹³å‡æ¯ä¸ªè¯éœ€è¦ 1-2 æ¯”ç‰¹æ¥ç¼–ç ã€‚</p>
<p><strong>å¹³ç¨³æ€§</strong>ï¼šåºåˆ—æ¦‚ç‡ä¸éšç€æ—¶é—´æ”¹å˜ã€‚è‡ªç„¶è¯­è¨€ä¸æ˜¯ï¼Œä½†æ˜¯Nå…ƒè¯­æ³•æ˜¯å¹³ç¨³çš„ã€‚</p>
<p><strong>éå†æ€§</strong>ï¼šé•¿åºåˆ—ä¸­åŒ…å«äº†æ‰€æœ‰çš„çŸ­åºåˆ—ã€‚</p>
<p><strong>Shannon-McMillan-Breiman theorem</strong>ï¼šå¦‚æœè¯­è¨€æ»¡è¶³æŸäº›æ­£åˆ™æ¡ä»¶ï¼ˆå‡†ç¡®åœ°è¯´ï¼Œæ˜¯å¹³ç¨³ä¸”éå†çš„ï¼‰ï¼Œ<strong>åºåˆ—é•¿åº¦è¶‹è¿‘äºæ— ç©·å¤§æ—¶ï¼Œâ€œåºåˆ—çš„å¹³å‡å¯¹æ•°æ¦‚ç‡çš„è´Ÿå€¼â€ ï¼Œå³ç»éªŒç†µç‡ä¼šä»¥æ¦‚ç‡1æ”¶æ•›æ•›åˆ°è¯¥è¿‡ç¨‹çš„ç†è®ºç†µç‡</strong>ã€‚</p>
<p><strong>äº¤å‰ç†µï¼ˆCross-Entropyï¼‰</strong>ï¼šæˆ‘ä»¬è™½ç„¶ä¸çŸ¥é“æ•°æ®çš„çœŸå®æ¦‚ç‡åˆ†å¸ƒpï¼Œä½†æ˜¯å¯ä»¥ç”¨æ¨¡å‹mæ¥è¿‘ä¼¼pã€‚ï¼ˆå³æˆ‘ä»¬è™½ç„¶ä¸çŸ¥é“è‡ªç„¶è¯­è¨€çš„çœŸå®æƒ…å†µï¼Œä½†æ˜¯å¯ä»¥ç”¨Nå…ƒè¯­æ³•æ¥è¿‘ä¼¼ã€‚ï¼‰<strong>äº¤å‰ç†µè¶Šå°ï¼Œæ¨¡å‹è¶Šæ¥è¿‘çœŸå®åˆ†å¸ƒ</strong>ã€‚</p>
<p><strong>å›°æƒ‘åº¦</strong>ï¼š<strong>å›°æƒ‘åº¦æ˜¯ç†µçš„æŒ‡æ•°å½¢å¼</strong>ã€‚æ¯”è¾ƒç›´è§‚ã€‚</p>
<h3 id="excercies">Excercies
</h3><h5 id="31">3.1
</h5><p>Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 40.</p>
<h5 id="32">3.2
</h5><p>Calculate the probability of the sentence <code>i want chinese food</code>. Give two probabilities, one using Fig. 3.2 and the â€˜useful probabilitiesâ€™ just below it on page 42, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(i|&lt;s&gt;) = 0.19$ and $P(&lt;/s&gt;|food) = 0.40$.</p>
<h5 id="33">3.3
</h5><p>Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why.</p>
<h5 id="34">3.4
</h5><p>We are given the following corpus, modified from the one in the chapter:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; Sam I am &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;
</span></span></code></pre></div><p>Using a bigram language model with add-one smoothing, what is $P(Sam | am)$? Include $&lt;s&gt;$ and $&lt;/s&gt;$ in your counts just like any other token.</p>
<h5 id="35">3.5
</h5><p>Suppose we didnâ€™t use the end-symbol $&lt;/s&gt;$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $&lt;/s&gt;$:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; a b  
</span></span><span class="line"><span class="cl">&lt;s&gt; b b  
</span></span><span class="line"><span class="cl">&lt;s&gt; b a  
</span></span><span class="line"><span class="cl">&lt;s&gt; a a
</span></span></code></pre></div><p>Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet a,b is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet a,b is also 1.0.</p>
<h5 id="36">3.6
</h5><p>Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating $P(w3|w1,w2)$, where $w3$ is a word which follows the bigram$ (w1,w2)$, in terms of various n-gram counts and V. Use the notation $c(w1,w2,w3)$ to denote the number of times that trigram $(w1,w2,w3)$ occurs in the corpus, and so on for bigrams and unigrams.</p>
<h5 id="37">3.7
</h5><p>We are given the following corpus, modified from the one in the chapter:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; Sam I am &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;
</span></span></code></pre></div><p>If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $Î»â‚ = 1/2$ and $Î»â‚‚ = 1/2,$ what is $P(Sam|am)$? Include $&lt;s&gt;$ and $&lt;/s&gt;$ in your counts just like any other token.</p>
<h5 id="38">3.8
</h5><p>Write a program to compute unsmoothed unigrams and bigrams.</p>
<h5 id="39">3.9
</h5><p>Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?</p>
<h5 id="310">3.10
</h5><p>Add an option to your program to generate random sentences.</p>
<h5 id="311">3.11
</h5><p>Add an option to your program to compute the perplexity of a test set.</p>
<h5 id="312">3.12
</h5><p>You are given a training set of 100 numbers that consists of 91 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?</p>
<h2 id="logistic-regression-and-text-classification">Logistic Regression and Text Classification
</h2><p>ç»å…¸ä»»åŠ¡ï¼š</p>
<ul>
<li>sentiment analysis</li>
<li>spam detection</li>
<li>language id</li>
<li>authorship attribution</li>
</ul>
<h3 id="machine-learning-and-classification">Machine Learning and Classification
</h3><ul>
<li>äººå·¥è§„åˆ™å¾ˆè„†å¼±ï¼Œæ•°æ®ä¸€å˜åŒ–å°±æ— æ³•ä½¿ç”¨</li>
<li>LLMçš„å¼±ç‚¹ï¼šå¹»è§‰ã€æ— æ³•è§£é‡Šã€‚</li>
</ul>
<p>å› æ­¤æœ€å¸¸è§çš„åˆ†ç±»æ–¹æ³•æ˜¯<strong>æœ‰ç›‘ç£æœºå™¨å­¦ä¹ </strong>ã€‚</p>
<ul>
<li>æ¦‚ç‡åˆ†ç±»å™¨ï¼šè¾“å‡º<strong>æ ·æœ¬å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡</strong>è€Œä¸æ˜¯ç±»åˆ«æ ‡ç­¾ï¼Œä¿è¯åœ¨åˆå¹¶çš„ç³»ç»Ÿé‡Œä¸è¿‡æ—©åœ°è¾“å‡ºç»“æœã€‚</li>
</ul>
<p>åˆ†ç±»å™¨çš„æ ¸å¿ƒç»„ä»¶ï¼š</p>
<ul>
<li>A feature representation of the input</li>
<li>A classificaition function that computes $\hat{y}$</li>
<li>An objective funcion that we want to potimize for learning
<ul>
<li>loss function</li>
</ul>
</li>
<li>An algorithm for optimizing the objective function
<ul>
<li>stochastic gradient descent algorithm</li>
</ul>
</li>
</ul>
<h3 id="the-sigmoid-function">The Sigmoid Function
</h3><p>äºŒåˆ†ç±»é€»è¾‘å›å½’çš„ç›®æ ‡æ˜¯ï¼šè®¡ç®—æ ·æœ¬å±äºæ­£ç±»çš„æ¦‚ç‡ã€‚</p>
<ul>
<li>ç¬¬ä¸€æ­¥ï¼šè®¡ç®—çº¿æ€§å¾—åˆ†$z=wâ‹…x+b$ï¼Œå€¼åŸŸä¸º$[-\infty, +\infty ]$</li>
<li>ç¬¬äºŒæ­¥ï¼š<strong>é€šè¿‡ Sigmoid å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡</strong>ï¼š  å°†çº¿æ€§å¾—åˆ†Â zÂ æ˜ å°„åˆ°Â $[0,1]Â $åŒºé—´</li>
</ul>
<p>$z$å¸¸å¸¸è¢«ç§°ä½œLogitï¼ˆå¯¹æ•°å‡ ç‡ï¼‰ã€‚Logitå°±æ˜¯Sigmoidçš„åå‡½æ•°ã€‚å¯ä»¥æé†’æˆ‘ä»¬åç»­è¦åŠ ä¸ŠSigmoidè¿›è¡Œè½¬æ¢ï¼Œå› ä¸º$z$å¹¶ä¸æ˜¯ä¸€ä¸ªçœŸå®çš„å€¼ã€‚</p>
<p>ç‰¹åˆ«åœ°ï¼Œ<strong>â€œæ­£ç±»çš„å¯¹æ•°å‡ ç‡â€ ä¸ç‰¹å¾å‘ˆçº¿æ€§å…³ç³»</strong>ã€‚ä¹Ÿå°±æ˜¯å½“å…¶ä»–æ¡ä»¶ä¸å˜æ—¶ï¼Œç‰¹å¾$x_1$æ¯å¢åŠ 1ï¼ŒLogitå°±å¢åŠ $w_1$ã€‚è¿™éå¸¸æœ‰å¯è§£é‡Šæ€§ã€‚</p>
<h3 id="classification-with-logistic-regression">Classification with Logistic Regression
</h3><p>å½“æ¦‚ç‡å¤§äº0.5çš„æ—¶å€™ï¼Œå°±æŠŠå®ƒåˆ†ç±»åˆ°æ­£ç±»é‡Œã€‚</p>
<h4 id="sentiment-classification">Sentiment Classification
</h4><p>ä¸¾äº†ä¸€ä¸ªä¾‹å­ã€‚</p>
<h4 id="other-classification-tasks-and-features">Other Classification Tasks and Features
</h4><p>Period disambiguationï¼šç¡®å®šå¥å·æ˜¯EOSè¿˜æ˜¯å…¶ä»–ã€‚</p>
<p><strong>Designing v.s. Learning featuresï¼š</strong></p>
<ul>
<li>åˆšåˆšçš„ä¾‹å­ï¼Œç‰¹å¾éƒ½æ˜¯äººå·¥è®¾è®¡çš„ã€‚æ­¤å¤–è¿˜æœ‰ï¼š
<ul>
<li>feaure interactionsï¼šåŸºç¡€ç‰¹å¾ç»„åˆæˆçš„å¤æ‚ç‰¹å¾</li>
<li>feature templatesï¼šæŠ½è±¡çš„ç‰¹å¾è§„èŒƒæ¥å®šä¹‰ç‰¹å¾ã€‚è¿™é‡Œçš„ç‰¹å¾ç©ºé—´æ˜¯ç¨€ç–çš„ï¼Œæ­¤å¤–ç‰¹å¾ä¸€èˆ¬æ˜¯å­—ç¬¦ä¸²æè¿°çš„Hashå€¼ã€‚</li>
</ul>
</li>
<li>äººå·¥è®¾è®¡å¤ªå¤æ‚äº†ã€‚å› æ­¤ç°ä»£çš„NLPç³»ç»Ÿéƒ½æ˜¯ç”¨Representation Learningæ¥è§£å†³ã€‚</li>
</ul>
<p>standardizeå’Œnormalizeã€‚</p>
<h4 id="processing-many-examples-at-once">Processing many examples at once
</h4><p>å¦‚æœæœ‰è®¸å¤šçš„å€¼è¦è®¡ç®—ï¼Œå¯ä»¥ä½¿ç”¨matrix arithmeticæ¥ä¸€æ¬¡è®¡ç®—å®Œã€‚</p>
<h3 id="multinomial-logistic-regression">Multinomial Logistic Regression
</h3><p>å¤šé¡¹é€»è¾‘å›å½’ä¹Ÿç§°softmax regressionï¼Œè€çš„æ•™æä¸Šä¹Ÿå«maxent clasifierã€‚</p>
<p>åœ¨å¤šé¡¹é€»è¾‘å›å½’ä¸­ï¼Œç›´æ¥è¾“å‡ºç»“æœè€Œä¸æ˜¯ä¸€ä¸ªæ¦‚ç‡å€¼ã€‚</p>
<ul>
<li>hard classification</li>
</ul>
<h4 id="softmax">Softmax
</h4><p>Sigmoidå‡½æ•°åœ¨å¤šåˆ†ç±»æƒ…å†µä¸‹çš„æ¨å¹¿ã€‚</p>
<h4 id="applying-softmax-in-logistic-regression">Applying Softmax in Logistic Regression
</h4><p>å¯ä»¥ä½¿ç”¨çŸ©é˜µè¿ç®—æ–¹å¼åŠ å¿«è®¡ç®—ã€‚</p>
$$\hat{y}=softmax(Wx+b)$$<blockquote>
<p><a class="link" href="https://arxiv.org/abs/2506.11035"  target="_blank" rel="noopener"
    >Doumbouya et al., 2025</a>æ˜¯è¿™ä¹ˆè®¤ä¸ºçš„ï¼šé€»è¾‘å›å½’å°†çŸ©é˜µçš„æ¯ä¸€è¡ŒÂ $w_k$è§†ä¸º<strong>ç¬¬Â $k$Â ç±»çš„åŸå‹ï¼ˆprototypeï¼‰</strong>ï¼Œç”±äºä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œå®ƒä»¬çš„ç‚¹ç§¯ï¼ˆdot productï¼‰å€¼å°±è¶Šå¤§ï¼Œå› æ­¤ç‚¹ç§¯å¯ä½œä¸ºè¡¡é‡å‘é‡ç›¸ä¼¼åº¦çš„å‡½æ•°ã€‚æ¨¡å‹æœ€ç»ˆå°†è¾“å…¥åˆ†é…ç»™ç›¸ä¼¼åº¦æœ€é«˜çš„ç±»åˆ«ã€‚</p></blockquote>
<h4 id="features-in-multinomial-logistic-regression">Features in Multinomial Logistic Regression
</h4><p>ç‰¹å¾æƒé‡åŒæ—¶ä¾èµ–äºè¾“å…¥æ–‡æœ¬å’Œè¾“å‡ºç±»åˆ«ã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250830163440597.png"
	width="744"
	height="837"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830163440597_hu_dcd37b06fe097e65.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830163440597_hu_87f8d0d9a05a6baf.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="88"
		data-flex-basis="213px"
	
></p>
<h3 id="learning-in-logistic-regression">Learning in Logistic Regression
</h3><p>é€»è¾‘å›å½’æ˜¯å¦‚ä½•å®ç°å­¦ä¹ çš„ï¼Ÿ</p>
<ul>
<li>ä½¿system outputï¼ˆclassifier outputï¼‰å’Œgold outputï¼ˆcorrect outputï¼‰è¶Šæ¥è¿‘è¶Šå¥½ã€‚ä¸¤è€…ä¹‹é—´çš„è·ç¦»å¯ä»¥ç§°ä½œ<strong>æŸå¤±å‡½æ•°</strong>æˆ–è€…<strong>ä»£ä»·å‡½æ•°</strong>ã€‚ä¸‹é¢ä»‹ç»äº¤å‰ç†µã€‚</li>
<li>éœ€è¦ä¸€ä¸ªç®—æ³•æ¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚ä¸‹é¢ä»‹ç»éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚</li>
</ul>
<h3 id="the-cross-entropy-loss-function">The Cross-entropy Loss Function
</h3><p>æ¡ä»¶æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼šåœ¨ç»™å®š$x$ä¸‹ï¼Œé€‰æ‹©å‚æ•°$w$å’Œ$b$ä½¿å¾—$y$çš„å¯¹æ•°æ¦‚ç‡æœ€å¤§ã€‚</p>
<p>è¿™é‡ŒæŸå¤±å‡½æ•°æ˜¯<strong>è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆnegative log likelihood lossï¼‰</strong>ï¼Œé€šå¸¸ä¹Ÿè¢«ç§°ä¸º<strong>äº¤å‰ç†µæŸå¤±ï¼ˆcross-entropy lossï¼‰</strong>ã€‚</p>
<p>ä»‹ç»äº†ä¸€ä¸‹ä¸ºä»€ä¹ˆ<strong>æœ€å°åŒ–äº¤å‰ç†µæŸå¤±å¯ä»¥ä½¿å¾—çœŸå®åˆ†å¸ƒå’Œé¢„æµ‹åˆ†å¸ƒæ›´åŠ æ¥è¿‘</strong>ã€‚</p>
<h3 id="gradient-descent">Gradient Descent
</h3><p>æ¢¯åº¦ä¸‹é™ç®—æ³•çš„åŸç†ã€‚ä»‹ç»äº†æ¢¯åº¦ã€å­¦ä¹ ç‡ã€‚</p>
<h4 id="the-gradient-for-logistic-regression">The Gradient for Logistic Regression
</h4><p>é€»è¾‘å›å½’çš„æ¢¯åº¦å°±æ˜¯
</p>
$$\frac{\partial L_{\mathrm{CE}}(\hat{y},y)}{\partial w_{j}}=-(y-\hat{y})x_{j}$$<p>
ä¹Ÿå°±æ˜¯é¢„æµ‹å€¼$\hat{y}$å’Œå®é™…å€¼$y$ä¹‹é—´çš„å·®ä¹˜è¾“å…¥å€¼$x_j$ã€‚</p>
<h4 id="the-stochastic-gradient-descent-algorithm">The Stochastic Gradient Descent Algorithm
</h4><p>éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•æ˜¯ä¸€ç§åœ¨çº¿ç®—æ³•ï¼Œå¯ä»¥è¾¹æ¥æ”¶æ•°æ®è¾¹å­¦ä¹ ã€‚</p>
<p>SGDæ¯æ¬¡ç”¨<strong>å•ä¸ªéšæœºæ ·æœ¬</strong>è®¡ç®—æ¢¯åº¦ã€‚</p>
<h4 id="mini-batch-training">Mini-batch Training
</h4><p>batch trainingå’Œmini-batch trainingçš„åŒºåˆ«ã€‚</p>
<ul>
<li>batch gradientï¼šæ‰€æœ‰çš„éšæœºæ ·æœ¬è®¡ç®—æ¢¯åº¦ã€‚</li>
<li>mini-batch gradientï¼šå°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚æ¯æ¬¡é€‰æ‹©<strong>ä¸€å°æ‰¹éšæœºæ ·æœ¬</strong>è®¡ç®—æ¢¯åº¦ã€‚</li>
</ul>
<h3 id="learning-in-multinomial-logistic-regression">Learning in Multinomial Logistic Regression
</h3><p>å¤šé¡¹å¼é€»è¾‘å›å½’å…¶å®å’ŒäºŒé¡¹å¼é€»è¾‘å›å½’å·®ä¸å¤šã€‚</p>
<ul>
<li>æœ¬è´¨æ˜¯ä½¿ç”¨ç‹¬çƒ­æ ‡ç­¾+æ¦‚ç‡å‘é‡çš„å½¢å¼è¿›è¡Œè®¡ç®—ã€‚</li>
<li>æ ¸å¿ƒæ˜¯ â€œå¯¹æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡å–è´Ÿå¯¹æ•°â€ï¼Œå¾—åˆ°äº¤å‰ç†µæŸå¤±ï¼Œå…¶è¶Šå°åˆ™é¢„æµ‹æ¦‚ç‡è¶Šé«˜ã€‚</li>
</ul>
<h3 id="evaluation-precision-recall-f-measure">Evaluation: Precision, Recall, F-measure
</h3><ul>
<li>confusion matrix</li>
<li>accuracy</li>
<li>precision</li>
<li>recall</li>
<li>F-measure
<ul>
<li>F1</li>
<li>a weighted harmonic mean of precision and recall.</li>
</ul>
</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250830172049893.png"
	width="928"
	height="394"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830172049893_hu_5b835770261a7097.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830172049893_hu_9163edd8010643ed.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="235"
		data-flex-basis="565px"
	
></p>
<p>microaveraging v.s. macroaveraging</p>
<ul>
<li>å¾®è§‚å¹³å‡ï¼šæ›´å…³æ³¨ â€œæ•´ä½“æ ·æœ¬çš„é¢„æµ‹å‡†ç¡®æ€§â€ï¼Œ<strong>å°‘æ•°ç±»é”™åˆ¤ä»£ä»·ä½äºå¤šæ•°ç±»</strong></li>
<li>å®è§‚å¹³å‡ï¼šæ›´å…³æ³¨æ‰€æœ‰çš„ç±»çš„é”™åˆ¤ä»£ä»·çš„å…¬å¹³ï¼Œ<strong>å°‘æ•°ç±»å’Œå¤šæ•°ç±»çš„ä»£ä»·ç›¸ç­‰</strong>
<img src="/2025/speech-and-language-processing/assets/IMG-20250830173257005.png"
	width="1084"
	height="468"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830173257005_hu_bca6ffdfb72f8971.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830173257005_hu_899e55b55133737a.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="231"
		data-flex-basis="555px"
	
></li>
</ul>
<h3 id="test-sets-and-cross-validation">Test sets and Cross-validation
</h3><p>Cross-validationï¼šè§£å†³æµ‹è¯•é›†ä¸è¶³çš„é—®é¢˜ã€‚</p>
<ul>
<li>å›ºå®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚</li>
<li>è®­ç»ƒé›†ä¸­è¿›è¡Œåˆ†å‰²ã€‚
<img src="/2025/speech-and-language-processing/assets/IMG-20250830173844382.png"
	width="895"
	height="462"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830173844382_hu_4558bdf3b4166cbe.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830173844382_hu_e53d98bd5ba967c8.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="193"
		data-flex-basis="464px"
	
></li>
</ul>
<h3 id="statistical-significance-testing">Statistical Significance Testing
</h3><p><strong>ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ</strong></p>
<ul>
<li>ä¸åªæ˜¯ç®€å•åœ°æ£€æŸ¥Aåœ¨æµ‹è¯•é›†ä¸Šçš„ç»“æœ$M(A,x)$å¥½äºBåœ¨æµ‹è¯•é›†ä¸Šçš„ç»“æœ$M(B,x)$ã€‚å¦‚æœå·®å¾ˆå°çš„è¯ï¼Œå…¶å®ä¸ä¸€å®šèƒ½è¯æ˜Açš„ç»“æœæ¯”Bå°ï¼Œä¸å…·æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ€§ã€‚</li>
<li>è®¾è®¡ä¸€ä¸ªæ•ˆåº”é‡$\delta (x)=M(A,x)-M(B,x)$ï¼ŒåŸå‡è®¾æ˜¯$H_0 :\delta (x)\leq 0$ã€‚è¿™æ ·è®¡ç®—på€¼æ˜¯å¦å°äºé˜ˆå€¼å¯ä»¥å¾—å‡ºæ˜¯å¦æ˜¾è‘—æ€§åœ°Aæ¯”Bè¦å¥½ã€‚</li>
</ul>
<p>åœ¨NLPä¸­ï¼Œä¸€èˆ¬ä¸ç”¨ANOVAsæˆ–è€…tæ£€éªŒï¼Œè€Œæ˜¯ä½¿ç”¨éå‚æ•°æ£€éªŒï¼š</p>
<ul>
<li><strong>è¿‘ä¼¼éšæœºåŒ–æ£€éªŒ</strong>ï¼ˆApproximate Randomization Testï¼‰</li>
<li><strong>bootstrap æ£€éªŒ</strong>ï¼ˆBootstrap Testï¼‰</li>
</ul>
<blockquote>
<p>æ–¹å·®åˆ†æå’Œtæ£€éªŒéƒ½éœ€è¦æœ‰å‡è®¾ï¼šæ–¹å·®é½æ€§æˆ–æ•°æ®æœä»æ­£æ€åˆ†å¸ƒã€‚æ‰€ä»¥åªèƒ½ç”¨éå‚æ•°æ£€éªŒã€‚</p></blockquote>
<h4 id="the-paired-bootstrap-test">The Paired Bootstrap Test
</h4><p>Bootstrap æ£€éªŒçš„æ ¸å¿ƒæ˜¯ â€œé‡æŠ½æ ·â€â€”â€” ä»åŸå§‹æµ‹è¯•é›†<code>x</code>ä¸­<strong>æœ‰æ”¾å›åœ°éšæœºæŠ½å–</strong>ç”Ÿæˆæ–°çš„æµ‹è¯•é›†ã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250830181020970.png"
	width="1097"
	height="490"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250830181020970_hu_8f406568f15b6414.png 480w, /2025/speech-and-language-processing/assets/IMG-20250830181020970_hu_c48413deb765f0e1.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="223"
		data-flex-basis="537px"
	
>
åœ¨æ–°çš„æµ‹è¯•é›†ä¸Šï¼ŒP å€¼ç­‰äº â€œé‡æŠ½æ ·æµ‹è¯•é›†ä¸­ï¼Œ$d(x^{(i)})â‰¥2d(x)$çš„æ•°é‡å æ€»é‡æŠ½æ ·æ¬¡æ•°bçš„æ¯”ä¾‹â€ã€‚<strong>åˆ¤æ–­æ­¤æ—¶çš„på€¼æ˜¯å¦ä½äºé˜ˆå€¼å°±å¯ä»¥åˆ¤æ–­å‡ºæ˜¯å¦æ˜¯æ•°æ®é›†æœ¬èº«çš„åå·®å¯¼è‡´äº†Aæ¯”Bè¦ç»“æœå¥½ã€‚</strong></p>
<blockquote>
<p>LLMçš„è§£é‡Šï¼šBootstrap é‡æŠ½æ ·ï¼Œå°±æ˜¯åœ¨ â€œä¸æ”¹å˜å¤©å¹³åˆå§‹å€¾æ–œï¼ˆæµ‹è¯•é›†åå‘ï¼‰â€ çš„å‰æä¸‹ï¼Œåå¤æ”¾ â€œéšæœºé‡é‡ï¼ˆé‡æŠ½æ ·çš„æ ·æœ¬ï¼‰â€ï¼Œçœ‹å·¦è¾¹ä¼šæ¯”å³è¾¹å¤šä½å¤šå°‘æ ¼ â€”â€” å¦‚æœåªæ˜¯å¤©å¹³æœ¬èº«æ­ªäº†ï¼Œéšæœºæ”¾é‡é‡æ—¶ï¼Œå·¦è¾¹æœ€å¤šä½ 2 æ ¼å·¦å³ï¼ˆå¸¸è§„æ³¢åŠ¨ï¼‰ï¼›å¦‚æœå·¦è¾¹çœŸçš„æ›´é‡ï¼Œå°±å¯èƒ½ä½ 4 æ ¼ä»¥ä¸Šï¼ˆæç«¯æƒ…å†µï¼‰ã€‚è¿™ç§æç«¯æƒ…å†µå¤šäº†ï¼Œè¶…è¿‡äº†é˜ˆå€¼ï¼Œæˆ‘ä»¬å°±æ›´ç›¸ä¿¡æ˜¯å¤©å¹³æœ¬èº«çš„é—®é¢˜ã€‚</p></blockquote>
<h3 id="avoiding-harms-in-classification">Avoiding Harms in Classification
</h3><p>representational harmsï¼šç”±äºå¯¹ç‰¹å®šç¤¾ä¼šç¾¤ä½“çš„è´¬ä½æˆ–åˆ»æ¿å°è±¡å¯¼è‡´çš„ä¼¤å®³ã€‚</p>
<p>toxic detection</p>
<p>model card</p>
<h3 id="interpereting-models">Interpereting Models
</h3><p>æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¹Ÿæ˜¯å¾ˆé‡è¦çš„ã€‚é€»è¾‘å›å½’å°±æ˜¯æ¯”è¾ƒå¥½çš„å¯è§£é‡Šçš„æ¨¡å‹ã€‚</p>
<h3 id="advanced-regularization">Advanced: Regularization
</h3><p>regularizationæ¥è§£å†³è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</p>
<ul>
<li>L2 regulaization</li>
<li>L1 regulaization</li>
<li>lasso</li>
<li>ridge</li>
</ul>
<h2 id="embeddings">Embeddings
</h2><p>åˆ†å¸ƒå‡è¯´ï¼šç›¸ä¼¼çš„ä¸Šä¸‹æ–‡æ€»ä¼šè¡¨ç°å‡ºç›¸ä¼¼çš„æ„æ€ã€‚</p>
<p>Embeddingsåˆ†ç±»</p>
<ul>
<li>static embeddings</li>
<li>contextualized embeddings</li>
</ul>
<p>å­¦ä¹ embeddingså’Œå®ƒçš„æ„ä¹‰çš„ç†è®ºç§°ä¸ºå‘é‡è¯­ä¹‰ã€‚</p>
<ul>
<li>è‡ªç›‘ç£æ¨¡å‹</li>
<li>representation learningçš„ä¸€ç§</li>
<li>æ— éœ€é€šè¿‡ç‰¹å¾å·¥ç¨‹çš„äººå·¥åˆ¶é€ representations</li>
</ul>
<h3 id="lexical-semantics">Lexical Semantics
</h3><ul>
<li>lemmaï¼šcitation formï¼Œè¯å…ƒï¼Œå¼•ç”¨å½¢å¼ï¼Œä¸€ä¸ªè¯çš„åŸºæœ¬å½¢å¼ã€‚</li>
<li>wordformï¼šè¯å½¢ï¼Œä¸€ä¸ªè¯çš„å…·ä½“ä½¿ç”¨å½¢æ€ã€‚</li>
<li>word senseï¼šè¯ä¹‰ã€‚</li>
<li>synonymyï¼šåŒä¹‰å…³ç³»ã€‚</li>
<li>word similarityï¼šè¯è¯­ç›¸ä¼¼åº¦ã€‚
<ul>
<li><a class="link" href="https://arxiv.org/abs/1408.3456"  target="_blank" rel="noopener"
    >SimLex-999</a>ä¸­å°±è®©äººä»¬ç»™ä¸€ä¸ªè¯å’Œå¦ä¸€ä¸ªè¯çš„ç›¸ä¼¼åº¦æ‰“åˆ†ã€‚</li>
</ul>
</li>
<li>word relatedness/associationï¼šè¯æ±‡å…³è”æ€§ï¼Œæ‰€æœ‰èƒ½è®©è¯æ±‡æœ‰å…³è”æ„Ÿçš„å…³ç³»ã€‚
<ul>
<li>semantic fields</li>
<li>topic modelsï¼šç‰¹åˆ«åœ°æœ‰Latent Dirichlet Allocationï¼ŒLDA</li>
<li>æœ€å¸¸è§çš„å…³ç³»
<ul>
<li>hypernymy or IS-A</li>
<li>antonymy</li>
<li>mernoymy</li>
</ul>
</li>
</ul>
</li>
<li>connotation</li>
</ul>
<h3 id="vector-semantics-the-intuition">Vector Semantics: The Intuition
</h3><p>ä¸€ä¸ªå•è¯å¯ä»¥è¡¨ç¤ºä¸º<strong>å¤šç»´è¯­ä¹‰ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹</strong>ã€‚è€Œè¿™ä¸ªå¤šç»´è¯­ä¹‰ç©ºé—´æ˜¯ä»å•è¯é‚»å±…çš„åˆ†å¸ƒè§„å¾‹ä¸­æ¨å¯¼è€Œæ¥çš„ã€‚</p>
<ul>
<li>tf-idf</li>
<li>word2vec</li>
<li>cosine</li>
</ul>
<h4 id="simple-count-based-embeddings">Simple Count-based Embeddings
</h4><ul>
<li>è¯æ±‡è¡¨ä¸€èˆ¬åœ¨1-5ä¸‡ä¹‹é—´</li>
<li>ç¨€ç–å‘é‡è¡¨ç¤ºï¼šå¤šæ•°æ•°å€¼ä¸º0ï¼Œç›®å‰æœ‰æœ‰æ•ˆç®—æ³•æ¥æœ‰æ•ˆå­˜å‚¨å’Œè®¡ç®—</li>
<li>æƒé‡å‡½æ•°
<ul>
<li>è®¡æ•°çš„æ—¶å€™å¯ä»¥ç”¨æƒé‡å‡½æ•°</li>
<li>ç›®å‰æœ€æµè¡Œçš„æ–¹æ³•æ˜¯tf-idf</li>
<li>è¿˜æœ‰ä¸€äº›å†å²æƒé‡æ–¹å¼</li>
</ul>
</li>
</ul>
<h4 id="cosine-for-measuring-similarity">Cosine for Measuring Similarity
</h4><p>ä½¿ç”¨ä½™å¼¦è®¡ç®—ç›¸ä¼¼åº¦ã€‚é€‚ç”¨äºç¨€ç–é•¿å‘é‡ã€‚</p>
<h4 id="word2vec">Word2vec
</h4><p>embeddingsè¦åŒºåˆ«äºåŸæ¥çš„ç¨€ç–é•¿å‘é‡ï¼Œé€šå¸¸æŒ‡çŸ­è€Œç¨ å¯†çš„å‘é‡ã€‚</p>
<ul>
<li>å­¦ä¹ çš„æƒé‡å˜å°‘ï¼Œå­¦ä¹ æ›´å¿«</li>
<li>æœ‰åŠ©äºæ³›åŒ–å’Œé¿å…è¿‡æ‹Ÿåˆ</li>
<li>èƒ½å¤Ÿæ›´å¥½æ•æ‰åŒä¹‰æ€§</li>
</ul>
<p>skip-gram with negative samplingï¼ˆSGNSï¼‰æ˜¯word2vecä¸¤ç§æ–¹æ³•çš„ä¸€ç§ã€‚word2vecæ˜¯ä¸€ç§<strong>é™æ€embedding</strong>æ–¹æ³•ï¼ŒåŒºåˆ«äºåŠ¨æ€embeddingï¼Œå¦‚BERTè¡¨ç¤ºã€‚</p>
<p>è¿™é‡Œæœ‰ä¸€ä¸ªæå…·åˆ›æ–°æ€§çš„æƒ³æ³• â€”â€”<strong>ä¸ç›´æ¥è®¡ç®— â€œè¯ä¸è¯çš„å…³è”â€ï¼ˆå…±ç°çŸ©é˜µï¼‰ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ª â€œé¢„æµ‹ä»»åŠ¡â€ è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ è¿™ç§å…³è”ï¼Œå†å°†å­¦ä¹ æˆæœï¼ˆæƒé‡ï¼‰ä½œä¸ºè¯åµŒå…¥</strong>ã€‚</p>
<ul>
<li>è¿™è¢«ç§°ä¸º<strong>è‡ªç›‘ç£æ–¹æ³•</strong></li>
</ul>
<p>Skip-gram æ¨¡å‹çš„æ ¸å¿ƒæ€è·¯å¦‚ä¸‹ï¼š</p>
<ol>
<li>å°†ç›®æ ‡è¯ä¸å…¶ç›¸é‚»çš„è¯­å¢ƒè¯è§†ä¸º<strong>æ­£ä¾‹</strong>ã€‚</li>
<li>ä»è¯æ±‡è¡¨ä¸­éšæœºé€‰å–å…¶ä»–è¯è¯­ï¼Œä½œä¸º<strong>è´Ÿä¾‹</strong>ã€‚</li>
<li>åˆ©ç”¨<strong>é€»è¾‘å›å½’è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œä½¿å…¶èƒ½å¤ŸåŒºåˆ†ä¸Šè¿°ä¸¤ç§æƒ…å†µ</strong>ï¼ˆå³åŒºåˆ† â€œç›®æ ‡è¯ä¸è¯­å¢ƒè¯æ˜¯ç›¸é‚»å…³ç³»â€ å’Œ â€œç›®æ ‡è¯ä¸éšæœºè¯æ— ç›¸é‚»å…³ç³»â€ï¼‰ã€‚</li>
<li>å°†è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„æƒé‡ä½œä¸ºembeddingã€‚</li>
</ol>
<h5 id="the-classifier">The Classifier
</h5><p>Skip-gramç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè®¡ç®—è¿™ä¸ªåœ°æ–¹å¡«è¿™ä¸ªè¯çš„æ¦‚ç‡ã€‚</p>
<ul>
<li><strong>æ ¸å¿ƒæ€è·¯</strong>ï¼šä¸€ä¸ªè¯æ˜¯å¦å¯èƒ½å‡ºç°åœ¨ç›®æ ‡è¯é™„è¿‘ï¼Œå–å†³äºå®ƒçš„åµŒå…¥å‘é‡ä¸ç›®æ ‡è¯çš„åµŒå…¥å‘é‡æ˜¯å¦ç›¸ä¼¼ã€‚</li>
<li><strong>ç›¸ä¼¼åº¦è®¡ç®—</strong>ï¼šç‚¹ç§¯</li>
<li>ç‚¹ç§¯ç»“æœå¹¶éæ¦‚ç‡å€¼ï¼Œè¿˜éœ€è¦ç»è¿‡Sigmoidå‡½æ•°è¿ç®—â€˜</li>
</ul>
<p>Skip-gramæ¨¡å‹å…¶å®æ˜¯å­˜å‚¨äº†æ¯ä¸ªå•è¯çš„ä¸¤ä¸ªembeddingsï¼Œä¸€ä¸ªä½œä¸ºç›®æ ‡è¯ï¼Œä¸€ä¸ªä½œä¸ºä¸Šä¸‹æ–‡ã€‚åˆ†åˆ«ä»target matrix Wå’Œcontext matrix CçŸ©é˜µä¸­å­¦ä¹ ã€‚</p>
<h4 id="learning-skip-gram-embeddings">Learning Skip-gram Embeddings
</h4><p><strong>positive examples</strong>ï¼šä¸Šä¸‹æ–‡æ»‘åŠ¨çª—å£</p>
<p><strong>negative examples</strong>ï¼šè¯æ±‡è¡¨ä¸­éšæœºæŠ½å–ï¼Œä¸€èˆ¬æ˜¯positive examplesçš„kå€ï¼Œç”±ç›®æ ‡è¯$w$å’Œå™ªå£°è¯ç»„æˆã€‚</p>
<p>åœ¨æŠ½æ ·çš„æ—¶å€™ï¼Œä¼šè®¾ç½®ä¸€ä¸ªæƒé‡ç³»æ•°$\alpha =0.75$æ¥è°ƒæ•´æ¦‚ç‡$P(w)$é¿å…æ€»æ˜¯é€‰æ‹©é«˜é¢‘è¯ã€‚</p>
<p>å­¦ä¹ ç›®æ ‡ï¼š</p>
<ul>
<li>æœ€å¤§åŒ–positive examplesä¸­å­¦ä¹ çš„ç›¸ä¼¼åº¦</li>
<li>æœ€å°åŒ–negative examplesä¸­çš„ç›¸ä¼¼åº¦</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250831142750258.png"
	width="880"
	height="558"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250831142750258_hu_b96cde190aab3329.png 480w, /2025/speech-and-language-processing/assets/IMG-20250831142750258_hu_1050abc09f7158b.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="157"
		data-flex-basis="378px"
	
>
å¦‚å›¾ï¼Œç›®æ ‡æ˜¯ä½¿å¾—apricotå’Œjamçš„ç‚¹ç§¯æ›´å¤§ï¼Œå’Œmatrixã€Tolstoyçš„ç‚¹ç§¯æ›´å°ã€‚</p>
<p>åœ¨å­¦ä¹ å®Œäº†ä¹‹åï¼Œä¸€èˆ¬åªä½¿ç”¨Wæ¥è¡¨ç¤ºã€‚</p>
<h4 id="other-kinds-of-static-embeddings">Other kinds of static embeddings
</h4><p>fasttext</p>
<ul>
<li>è§£å†³äº†word2vecçš„æœªçŸ¥è¯é—®é¢˜</li>
<li>subword models</li>
</ul>
<p>GloVe</p>
<ul>
<li>Global Vectors</li>
<li>åŸºäºè¯è¯å…±ç°çŸ©é˜µçš„æ¦‚ç‡</li>
</ul>
<p>word2vecå¯ä»¥çœ‹æˆ<strong>é—´æ¥ä¼˜åŒ–ä¸€ä¸ª â€œå¸¦PPMIï¼ˆPositive Pointwise Mutual Informationï¼‰æƒé‡çš„å…±ç°çŸ©é˜µâ€ çš„å‡½æ•°</strong>ã€‚</p>
<blockquote>
<p>æˆ‘çš„ä¸€äº›æƒ³æ³•ï¼šGNNæ˜¯å¦ä¹Ÿå¯ä»¥åšç±»ä¼¼çš„å·¥ä½œï¼ŸGNNç›®å‰çš„æ–¹æ³•å°±æ˜¯ç›´æ¥å¤„ç†å›¾ç»“æ„ï¼Œè¿›è¡Œå±€éƒ¨å›¾ç»“æ„å­¦ä¹ ã€‚ä¸ä¸“é—¨è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œè€Œæ˜¯æŠŠè¿™ä¸ªè®­ç»ƒçš„æ¨¡å‹çš„å‚æ•°ç›´æ¥ä½œä¸ºä¸€ä¸ªå‘é‡ä½¿ç”¨ï¼Œåè€Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åˆ°éšå«å…³ç³»ã€‚</p>
<p>è¿™ç§æƒ³æ³•æ˜¯å¦å¯ä»¥å°†LLMå’ŒGNNæ›´å¥½åœ°è¿æ¥èµ·æ¥ï¼Ÿ</p></blockquote>
<h3 id="visualizing-embeddings">Visualizing Embeddings
</h3><ul>
<li>ç›´æ¥åˆ—å‡ºæœ€ç›¸ä¼¼çš„å•è¯</li>
<li>èšç±»ç®—æ³•</li>
<li>t-SNEï¼šè®©ä½ç»´ç©ºé—´ä¸­ â€œç‚¹ä¸ç‚¹çš„ç›¸ä¼¼æ¦‚ç‡â€ï¼Œå°½é‡å’Œé«˜ç»´ç©ºé—´ä¸­ â€œç‚¹ä¸ç‚¹çš„ç›¸ä¼¼æ¦‚ç‡â€ ä¸€è‡´</li>
</ul>
<h3 id="semantic-properites-of-embeddings">Semantic Properites of Embeddings
</h3><p>å…³è”å’Œç›¸ä¼¼çš„åŒºåˆ«ï¼šè¶ŠçŸ­çš„ä¸Šä¸‹æ–‡å¾—åˆ°çš„å‘é‡ï¼Œç›¸ä¼¼è¶Šå¥½æ‰¾ï¼Œå…³è”è¶Šéš¾ï¼›è¶Šé•¿æ­£å¥½ç›¸åã€‚</p>
<ul>
<li>ä¸€é˜¶å…±ç°ï¼šç»„åˆå…³ç³»ï¼Œä¸€èµ·ç»„åˆå‡ºç°ï¼Œå¦‚writeå’Œpoem</li>
<li>äºŒé˜¶å…±äº«ï¼šèšåˆå…³ç³»ï¼Œç›´æ¥ç›¸å…³ï¼Œå¦‚writeå’Œsay</li>
</ul>
<p>ç±»æ¯”å…³ç³»ï¼šå¹³è¡Œå››è¾¹å½¢æ¨¡å‹
<img src="/2025/speech-and-language-processing/assets/IMG-20250831150132974.png"
	width="1134"
	height="469"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250831150132974_hu_363723ff4c0d8ebb.png 480w, /2025/speech-and-language-processing/assets/IMG-20250831150132974_hu_327a3c1c3636cedb.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="241"
		data-flex-basis="580px"
	
>
é—®é¢˜æ˜¯åªèƒ½ç”¨åœ¨æ˜ç¡®çš„å…³ç³»ã€çŸ­çš„è·ç¦»å’Œé¢‘ç¹çš„å•è¯ä¸Šã€‚</p>
<h4 id="embeddings-and-historical-semantics">Embeddings and Historical Semantics
</h4><p>åµŒå…¥çš„åº”ç”¨ã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250831150603967.png"
	width="839"
	height="505"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250831150603967_hu_baa0cb87e66ee17e.png 480w, /2025/speech-and-language-processing/assets/IMG-20250831150603967_hu_76e49fbca324cd4d.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="166"
		data-flex-basis="398px"
	
></p>
<p>å¾ˆæœ‰æ„æ€ã€‚</p>
<ul>
<li>gayï¼šæ„‰æ‚¦çš„-&gt;æ˜äº®çš„-&gt;ç”·åŒæ€§æ‹</li>
<li>broadcastï¼šæ•£æ’­-&gt;æŠ¥çº¸-&gt;BBC</li>
<li>awfulï¼šåº„ä¸¥çš„-&gt;å¯æ€•çš„</li>
</ul>
<h3 id="bias-and-embeddings">Bias and Embeddings
</h3><p>allocational harm</p>
<p>embeddingä¸ä»…åæ˜ è¾“å…¥ï¼Œè¿˜æ”¾å¤§åè§ã€‚</p>
<h3 id="evaluating-vector-models">Evaluating Vector Models
</h3><p>ç›¸ä¼¼åº¦åº¦é‡ï¼š</p>
<ul>
<li>ä¸å«ä¸Šä¸‹æ–‡
<ul>
<li>WordSim-353</li>
<li>SimLex-999</li>
<li>TOEFL dataset</li>
</ul>
</li>
<li>å«ä¸Šä¸‹æ–‡
<ul>
<li>SCWS</li>
<li>WiC</li>
<li>semantic textual similarity task</li>
</ul>
</li>
</ul>
<p>ç±»æ¯”åº¦é‡ï¼š</p>
<ul>
<li>SemEval-2012 Task 2 dataset</li>
</ul>
<p>æ‰€æœ‰çš„Embeddingç®—æ³•éƒ½ä¼šå­˜åœ¨å›ºæœ‰çš„å˜å¼‚æ€§ã€‚å»ºè®®ä½¿ç”¨bootstrapé‡‡æ ·åçš„æ–‡æ¡£ä¸­è®­ç»ƒå¤šä¸ªembeddingså¹¶å¹³å‡ã€‚</p>
<h2 id="neural-networks">Neural Networks
</h2><p>McCulloch-Pitts neuron</p>
<p>feedforward</p>
<p>deep learning</p>
<h3 id="units">Units
</h3><p>bias term</p>
<p>activationï¼šä½¿ç”¨non-linear functionsï¼Œå¦‚sigmoidã€tanhã€ReLUç­‰</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250901195216978.png"
	width="833"
	height="422"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250901195216978_hu_e98d5b59e57b93ca.png 480w, /2025/speech-and-language-processing/assets/IMG-20250901195216978_hu_75d0b4b410c52d3a.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="197"
		data-flex-basis="473px"
	
></p>
<ul>
<li>sigmoidå’Œtanhçš„é—®é¢˜ï¼šç‰¹åˆ«å®¹æ˜“å‡ºç°é¥±å’Œçš„ç°è±¡ï¼Œå¦‚å½“ç‰¹åˆ«é è¿‘1çš„æ—¶å€™ï¼Œæ­¤æ—¶çš„å¯¼æ•°æ¥è¿‘äº0ï¼Œè¾“å…¥çš„å¾®å°æ”¹å˜å°†æ— æ³•å¼•èµ·è¾“å‡ºçš„ä»»ä½•å˜åŒ–ã€‚è¿™ç§ç°è±¡ç§°ä¸º<strong>æ¢¯åº¦æ¶ˆå¤±</strong>ã€‚</li>
</ul>
<h3 id="the-xor-problem">The XOR Problem
</h3><p>Minsky and Papertï¼šä¸€å±‚ç¥ç»å…ƒæ— æ³•è§£å†³å¼‚æˆ–é—®é¢˜ã€‚å¦‚æ„ŸçŸ¥æœºã€‚</p>
<h4 id="the-solution-neural-networks">The Solution: Neural Networks
</h4><p>åŒ…å«éšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºMLPè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</p>
<p>ä¸‹é¢è¿™ä¸ªå›¾ï¼Œä¸ç®¡å¯¹äºåŸæ¥çš„[0,1]è¿˜æ˜¯[1,0]éƒ½ä¼šå°†ä»–ä»¬åœ¨éšå«å±‚é‡Œè½¬åŒ–ä¸º[1,0]ã€‚è€ŒåŸæ¥çš„[0,0]å’Œ[1,1]åˆ™å˜ä¸º[2,1]ï¼Œæ­¤æ—¶ä¾¿çº¿æ€§å¯åˆ†ã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250901200315265.png"
	width="762"
	height="337"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250901200315265_hu_820ad2365c82284f.png 480w, /2025/speech-and-language-processing/assets/IMG-20250901200315265_hu_8ed92ef255ef500a.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="226"
		data-flex-basis="542px"
	
></p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250901201345068.png"
	width="686"
	height="446"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250901201345068_hu_a56fa62e2362e855.png 480w, /2025/speech-and-language-processing/assets/IMG-20250901201345068_hu_c12f6c72632725f1.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="153"
		data-flex-basis="369px"
	
></p>
<h3 id="feedforward-neural-networks">Feedforward Neural Networks
</h3><p>ä¸RNNå¯¹åº”ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFNNï¼‰ä¸å¸¦æœ‰å¾ªç¯ã€‚</p>
<ul>
<li>ç”±äºå†å²åŸå› ï¼ŒFNNä¹Ÿç§°ä¸ºå¤šå±‚æ„ŸçŸ¥æœºï¼ŒMLPsï¼Œäº‹å®ä¸Šç°åœ¨çš„å¤šå±‚ç½‘ç»œä¸­å·²ç»ä¸æ˜¯æ„ŸçŸ¥æœºäº†ã€‚</li>
<li>æ„ŸçŸ¥æœºä½¿ç”¨é˜¶è·ƒå‡½æ•°ï¼Œç°åœ¨çš„ç¥ç»ç½‘ç»œç”¨çš„æ˜¯å¤šç§éçº¿æ€§çš„å•å…ƒå¦‚ReLUsæˆ–è€…Sigmoidç­‰</li>
</ul>
<p>å‰é¦ˆç¥ç»ç½‘ç»œçš„æ ‡å‡†ç»“æ„æ˜¯å…¨è¿æ¥çš„ã€‚
<img src="/2025/speech-and-language-processing/assets/IMG-20250902182112223.png"
	width="726"
	height="429"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902182112223_hu_28bfdb5ad0f61a8a.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902182112223_hu_d4c96ab1783e2a89.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="169"
		data-flex-basis="406px"
	
>
åœ¨è¿™é‡Œï¼ŒWä½œä¸ºè¾“å…¥å±‚åˆ°éšå«å±‚çš„æƒé‡çŸ©é˜µï¼ŒUåˆ™æ˜¯éšå«å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µã€‚
</p>
$$h=\sigma(Wx+b)$$<p>
</p>
$$z=Uh$$<p>
åœ¨å¾—åˆ°è¾“å‡ºç»“æœ$z$ä¹‹åï¼Œç”±äº$z$æ˜¯ä¸€ä¸ªå®æ•°å€¼å‘é‡ï¼ˆå…¶å®å°±æ˜¯logitsï¼‰ï¼Œè€Œåˆ†ç±»éœ€è¦çš„æ˜¯æ¦‚ç‡åˆ†å¸ƒå‘é‡ï¼Œæ‰€ä»¥è¦å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ï¼ˆnormalizingï¼‰ã€‚è¿™é‡Œä½¿ç”¨çš„æ˜¯softmaxå‡½æ•°ã€‚
</p>
$$y = \text{softmax}(z)$$<p>ç¥ç»ç½‘ç»œå’Œå¤šé¡¹é€»è¾‘å›å½’çš„åŒºåˆ«ï¼š</p>
<ul>
<li>æœ‰è®¸å¤šå±‚</li>
<li>ä¸­é—´å±‚çš„æ¿€æ´»å‡½æ•°ä¸åªä½¿ç”¨sigmoid</li>
<li>ç‰¹å¾å¯ä»¥ä¸åªæ˜¯ç”±äººå·¥çš„ç‰¹å¾æ¨¡æ¿è®¾è®¡ï¼Œè€Œå¯ä»¥ç”±ç½‘ç»œè‡ªèº«å¾—åˆ°</li>
</ul>
<p>é€»è¾‘å›å½’å¯ä»¥ç†è§£æˆä¸€å±‚çš„ç¥ç»ç½‘ç»œã€‚</p>
<h4 id="more-details-on-feedforward-networks">More Details on Feedforward Networks
</h4><p>ä¸ºä»€ä¹ˆæ¿€æ´»å‡½æ•°è¦éçº¿æ€§</p>
<p>æ›¿æ¢åç½®é¡¹ï¼šä½¿ç”¨dummy nodeæ¥ä»£æ›¿åŸæ¥çš„åç½®é¡¹ã€‚ä¹Ÿå°±æ˜¯ä¸‹å›¾ä¸­æŠŠbæ¢æˆä¸€ä¸ªæ–°çš„å›ºå®šä¸º1çš„$x_0$ã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902185044344.png"
	width="692"
	height="372"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902185044344_hu_22233632699e1e11.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902185044344_hu_f917792a9f3a40e1.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="186"
		data-flex-basis="446px"
	
></p>
<h3 id="feedforward-networks-for-nlp-classification">Feedforward Networks for NLP: Classification
</h3><p>åµŒå…¥çŸ©é˜µã€è¡¨ç¤ºæ± åŒ–ã€è¡¨ç¤ºå­¦ä¹ å…ˆä¸è®²ï¼Œå…ˆå­¦ä¸€ä¸‹ç”¨å‰é¦ˆç¥ç»ç½‘ç»œè§£å†³åˆ†ç±»é—®é¢˜ã€‚</p>
<h4 id="neural-net-classifiers-with-hand-built-features">Neural Net Classifiers with Hand-built Features
</h4><p>äººå·¥è®¾è®¡çš„ç‰¹å¾ï¼Œé™¤äº†æŠŠMLPæ¢æˆFNNä¹‹å¤–æ²¡å˜åŒ–ã€‚
<img src="/2025/speech-and-language-processing/assets/IMG-20250902185551826.png"
	width="688"
	height="399"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902185551826_hu_e7290fd892ec1524.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902185551826_hu_7954655261211383.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="172"
		data-flex-basis="413px"
	
></p>
<h4 id="vectorizing-for-parallelizing-inference">Vectorizing for Parallelizing Inference
</h4><p>å•ä¸ªæ ·æœ¬ç‰¹å¾ç»´åº¦æ˜¯dï¼Œæœ‰mä¸ªæ ·æœ¬ï¼Œå°±å¯ä»¥å°†è¾“å…¥å†™æˆä¸€ä¸ªçŸ©é˜µXä¸ºmÃ—dç»´ã€‚</p>
<p>å…¶ä»–çš„åç½®é¡¹ç­‰ä¹Ÿå¯ä»¥å†™æˆçŸ©é˜µçš„å½¢å¼ï¼Œæœ‰åŠ©äºæœ€åçš„è¿ç®—ã€‚æ­¤æ—¶æœ‰
</p>
$$H=\sigma(XW^T+\textbf{b})$$$$Z=HU^T$$$$\hat{Y}=\text{softmax}(Z)$$<blockquote>
<p>è¿™é‡Œçš„Xä¸­è¡Œå‘é‡è¡¨ç¤ºä¸€ä¸ªæ ·æœ¬çš„å®Œæ•´ç‰¹å¾ã€‚æœ‰çš„æ—¶å€™ä¼šå†™æˆ$WX+b$ï¼Œæœ‰çš„æ—¶å€™æ˜¯$XW+b$ã€‚æ³¨æ„Xçš„å½¢çŠ¶æœ‰æ‰€ä¸åŒã€‚</p></blockquote>
<h3 id="embeddings-as-the-input-to-neural-net-classifiers">Embeddings as the Input to Neural Net Classifiers
</h3><p>static embeddingsä»£æ›¿hand-designed featuresã€‚</p>
<ul>
<li>å­˜å‚¨static embeddingsçš„è¯å…¸ç§°ä¸ºembedding matrix <strong>E</strong></li>
</ul>
<p>one-hot vectorï¼šä»embedding matrixé€‰å–token embeddingçš„æ–¹æ³•
<img src="/2025/speech-and-language-processing/assets/IMG-20250902192356608.png"
	width="688"
	height="215"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902192356608_hu_35c57fe6c6d3b00d.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902192356608_hu_c416dc54a98e3898.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="320"
		data-flex-basis="768px"
	
></p>
<p>åˆ†ç±»å™¨ï¼š</p>
<ul>
<li>concatenationï¼šé€‚åˆå¯¹tokené¡ºåºå’Œç»†èŠ‚æ•æ„Ÿçš„ä»»åŠ¡ï¼Œå¦‚è¯­è¨€å»ºæ¨¡</li>
<li>poolingï¼šé€‚åˆå¯¹æ•´ä½“è¯­ä¹‰æ•æ„Ÿçš„ä»»åŠ¡ï¼Œå¦‚æƒ…æ„Ÿåˆ†æ
<ul>
<li>mean-pooling</li>
<li>max-pooling</li>
</ul>
</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902192912076.png"
	width="807"
	height="712"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902192912076_hu_e6ec73621f005a0d.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902192912076_hu_c30b59ad826afcc3.png 1024w"
	loading="lazy"
	
		alt="å‰é¦ˆç¥ç»ç½‘ç»œç”¨äºæƒ…æ„Ÿåˆ†æï¼Œå¸¦æœ‰æ± åŒ–å±‚çš„ç»“æ„"
	
	
		class="gallery-image" 
		data-flex-grow="113"
		data-flex-basis="272px"
	
></p>
<p>ç¥ç»è¯­è¨€æ¨¡å‹å’ŒNå…ƒè¯­æ³•æ¨¡å‹çš„åŒºåˆ«ï¼š</p>
<ul>
<li>å¯ä»¥å¤„ç†æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼Œæ›´åŠ æ³›åŒ–ï¼Œé¢„æµ‹æ›´å‡†ç¡®ï¼›é€Ÿåº¦æ›´æ…¢ï¼Œè®­ç»ƒæ›´éº»çƒ¦</li>
<li>ä½¿ç”¨embeddingsè¡¨ç¤ºè¯è€Œä¸æ˜¯word identity</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902193413335.png"
	width="717"
	height="645"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902193413335_hu_aab9c40f89390eef.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902193413335_hu_98641d26ebbce7c.png 1024w"
	loading="lazy"
	
		alt="embedding layer eè¿™é‡Œæ˜¯æ‹¼æ¥èµ·æ¥çš„"
	
	
		class="gallery-image" 
		data-flex-grow="111"
		data-flex-basis="266px"
	
></p>
<h3 id="training-neural-nets">Training Neural Nets
</h3><p>æŸå¤±å‡½æ•°ã€äº¤å‰ç†µ</p>
<p>è¯¯å·®åå‘ä¼ æ’­/åå‘å¾®åˆ†</p>
<h4 id="loss-function">Loss Function
</h4><p>äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼šè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ã€‚ç”¨äº<strong>è¾“å‡ºä¸º â€œç±»åˆ«æ¦‚ç‡â€ çš„ä»»åŠ¡</strong>ã€‚</p>
<h4 id="computing-the-gradient">Computing the Gradient
</h4><p>ä¸€å±‚çš„æ—¶å€™å¯ä»¥ç”¨æŸå¤±çš„å¯¼æ•°ï¼Œä½†æ˜¯å¤šå±‚çš„æ—¶å€™éœ€è¦ç”¨åå‘ä¼ æ’­ç®—æ³•ã€‚</p>
<h4 id="computation-graphs">Computation Graphs
</h4><p>ä»‹ç»äº†ä»€ä¹ˆæ˜¯è®¡ç®—å›¾ã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902201310848.png"
	width="697"
	height="263"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902201310848_hu_9525e3130f2a9740.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902201310848_hu_d15c38716a3d7624.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="265"
		data-flex-basis="636px"
	
></p>
<h4 id="backward-differentiation-on-computation-graphs">Backward Differentiation on Computation Graphs
</h4><p><strong>åå‘ä¼ æ’­</strong>ï¼šç”¨é“¾å¼æ³•åˆ™ä¸€æ¬¡æ€§è®¡ç®—å‡ºæ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚
<img src="/2025/speech-and-language-processing/assets/IMG-20250902213405406.png"
	width="829"
	height="432"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902213405406_hu_aea55f690f9591d7.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902213405406_hu_e6e577c00244a646.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="191"
		data-flex-basis="460px"
	
>
å…ˆæ­£å‘ä¼ æ’­ä¸€æ¬¡ï¼Œç„¶ååå‘ä¼ æ’­è®¡ç®—ã€‚</p>
<ul>
<li>æ­£å‘ä¼ æ’­æ˜¯ä¸ºäº†å¾—åˆ°æŸå¤±å€¼</li>
<li>åå‘ä¼ æ’­æ˜¯ä¸ºäº†å¾—åˆ°æ¢¯åº¦ï¼Œä»è€Œè¿›è¡Œå‚æ•°ä¼˜åŒ–</li>
</ul>
<p>å½“ç„¶åœ¨çœŸæ­£çš„ç¥ç»ç½‘ç»œä¸­ï¼Œè®¡ç®—ä¼šæ›´åŠ å¤æ‚ã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250902213726628.png"
	width="851"
	height="528"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250902213726628_hu_86b4e3d87a5a98fe.png 480w, /2025/speech-and-language-processing/assets/IMG-20250902213726628_hu_508ea0186fde9736.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="161"
		data-flex-basis="386px"
	
>
ä½†æ˜¯æ–¹æ³•ä¸‡å˜ä¸ç¦»å…¶å®—ã€‚</p>
<h4 id="more-details-on-learning">More Details on Learning
</h4><p>ç¥ç»ç½‘ç»œä¼˜åŒ–é—®é¢˜æ˜¯ä¸€ä¸ªéå‡¸ä¼˜åŒ–é—®é¢˜ã€‚ç›®å‰æœ‰äº†å¾ˆå¤šå¥½çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼š</p>
<ul>
<li>åˆå§‹å€¼ä¸è®¾ä¸º0ï¼Œè€Œæ˜¯éšæœºçš„ä¸€äº›å°çš„æ•°</li>
<li>dropout</li>
<li>è¶…å‚æ•°ï¼šAdamç­‰</li>
</ul>
<p>GPUsç­‰è®¡ç®—åŠ é€Ÿ</p>
<h2 id="large-language-models">Large Language Models
</h2><p>ELIZA</p>
<p>Distributional hypothesis</p>
<p>Pretraining</p>
<p>è¯­è¨€æ¨¡å‹ï¼šä¾æ®å‰æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„åˆ†å¸ƒ</p>
<p>å‘å±•ï¼š</p>
<ul>
<li>Nå…ƒè¯­æ³•</li>
<li>LSA/LSIï¼Œéšå«è¯­ä¹‰åˆ†æï¼Œå¼€å§‹ç”¨å‘é‡è¡¨ç¤ºè¯</li>
<li>ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹neural language model</li>
<li>RNNè¯­è¨€æ¨¡å‹</li>
<li>word2vec</li>
<li>é¢„è®­ç»ƒæŠ€æœ¯</li>
<li>Transformeræå‡º</li>
<li>æ©ç è¯­è¨€æ¨¡å‹</li>
<li>è‡ªå›å½’è¯­è¨€æ¨¡å‹</li>
</ul>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903221442377.png"
	width="817"
	height="367"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903221442377_hu_625cc627b168f68a.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903221442377_hu_9ee53070fd85a32c.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="222"
		data-flex-basis="534px"
	
></p>
<p>å¦‚æœå¯ä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒï¼Œé‚£ä¹ˆå°±å¯ä»¥ä»æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œç”Ÿæˆä¸‹ä¸€ä¸ªè¯ã€‚è¿™æ ·å°±ä»é¢„æµ‹æ¨¡å‹å˜æˆäº†ç”Ÿæˆæ¨¡å‹ã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903221449983.png"
	width="800"
	height="551"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903221449983_hu_3354a2ee29195159.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903221449983_hu_38463e6c14e8a47d.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="145"
		data-flex-basis="348px"
	
></p>
<ul>
<li>å› æœè¯­è¨€æ¨¡å‹/è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼šä»å·¦åˆ°å³ä¾æ¬¡ç”Ÿæˆ</li>
<li>æ©ç è¯­è¨€æ¨¡å‹ï¼šBERTç­‰ï¼Œå¯ä»¥åŒæ—¶åˆ©ç”¨å·¦å³ä¸¤ä¾§çš„ä¿¡æ¯</li>
</ul>
<p>ç”Ÿæˆå¼AI</p>
<h3 id="three-architecture-for-language-models">Three Architecture for Language Models
</h3><p>ä¸‰ç§ç»“æ„ï¼š</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903221500783.png"
	width="842"
	height="383"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903221500783_hu_5e7d45143603e021.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903221500783_hu_756c75157cc64aa.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="219"
		data-flex-basis="527px"
	
></p>
<ul>
<li>
<p>decoderï¼šä¸Šæ–‡ä¸­çš„æ¶æ„ã€‚è¾“å…¥ä¸ºä¸€ç³»åˆ—çš„tokenï¼Œ<strong>ä¾æ¬¡è¿­ä»£</strong>ç”Ÿæˆè¾“å‡ºçš„tokenã€‚ç”¨äºè‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚</p>
<ul>
<li>ç”Ÿæˆæ–‡æœ¬ç­‰ï¼Œå¦‚GPT</li>
</ul>
</li>
<li>
<p>encoderï¼šç”¨äºæ©ç è¯­è¨€æ¨¡å‹ã€‚è¾“å…¥ä¸ºæ–‡æœ¬ï¼Œè¾“å‡ºä¸ºæ ‡ç­¾ã€‚</p>
<ul>
<li>åˆ†ç±»æ–‡æœ¬ç­‰ï¼Œå¦‚BERT</li>
</ul>
</li>
<li>
<p>encoder-decoderï¼šè¾“å…¥ä¸ºä¸€ä¸²tokenï¼Œè¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸²tokenã€‚</p>
<ul>
<li>
<p>ç›¸æ¯”decoderæ¥è¯´ï¼Œå’Œè¾“å…¥è¾“å‡ºtokençš„å…³ç³»æ›´ä¸ç´§å¯†ï¼Œè¿™ç±»æ¨¡å‹ç”¨äºåœ¨ä¸åŒç±»å‹çš„æ ‡è®°ä¹‹é—´è¿›è¡Œæ˜ å°„</p>
</li>
<li>
<p>æœºå™¨ç¿»è¯‘ç­‰</p>
</li>
</ul>
</li>
</ul>
<p>ä»–ä»¬éƒ½æ˜¯åŸºäºç¥ç»ç½‘ç»œæ„å»ºçš„ã€‚</p>
<h3 id="conditional-generation-of-text-the-intuition">Conditional Generation of Text: The Intuition
</h3><p>ä¸ç®¡ä»€ä¹ˆä»»åŠ¡ï¼Œéƒ½å¯ä»¥ç®€åŒ–ä¸ºç»™å®špromptçš„ä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ä»»åŠ¡ã€‚</p>
<p><img src="/assets/Pasted%20image%2020250903221522.png"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="prompting">Prompting
</h3><p>æŒ‡ä»¤å¾®è°ƒ</p>
<p><img src="/assets/Pasted%20image%2020250903221532.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>prompt</p>
<ul>
<li>Demonstrations</li>
<li>Few-shot prompting</li>
<li>Zero-shot prompting</li>
</ul>
<blockquote>
<p>è¿™ç§Demonstrationså¯ä»¥æ‰‹åŠ¨ç­›é€‰ï¼Œä¹Ÿå¯ä»¥ç”±ä¼˜åŒ–å™¨å¦‚DSPyæ¥è‡ªåŠ¨é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒDemonstrationsä¼¼ä¹å¹¶ä¸æ˜¯ä¸€å®šè¦ç»™æ­£ç¡®çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œé”™è¯¯çš„ä¹Ÿè¡Œï¼Œä¸»è¦ä½œç”¨æ˜¯æ ¼å¼ã€‚</p></blockquote>
<p>promptï¼šå¯ä»¥çœ‹åšä¸€ä¸ªå­¦ä¹ ä¿¡å·ã€‚<strong>æç¤ºè¯ä¸ä¼šæ›´æ–°æ¨¡å‹çš„æƒé‡ï¼Œå…¶æ”¹å˜çš„ä»…ä»…æ˜¯æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥åŠç½‘ç»œä¸­çš„æ¿€æ´»çŠ¶æ€ã€‚</strong></p>
<ul>
<li>in-context learningï¼šå‚æ•°æ²¡æœ‰æ”¹å˜çš„å­¦ä¹ </li>
<li>System promptï¼šå½±å“å…¨å±€çš„ä¸€ä¸ªæ–‡æœ¬promptï¼Œè¢«æ·»åŠ åˆ°æ‰€æœ‰ç”¨æˆ·promptæˆ–æŸ¥è¯¢çš„å‰é¢</li>
</ul>
<h3 id="generation-and-sampling">Generation and Sampling
</h3><p>è¯­è¨€æ¨¡å‹çš„å†…éƒ¨ç½‘ç»œä¼šç”Ÿæˆlogitsï¼Œå†ç”±softmaxè®¡ç®—å¾—åˆ°æ¦‚ç‡ï¼Œéšååœ¨è¿™äº›tokenä¸­è¿›è¡Œé‡‡æ ·ã€‚</p>
<p><img src="/assets/Pasted%20image%2020250903221559.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>decodingï¼šåŸºäºæ¦‚ç‡é€‰æ‹©tokenç”Ÿæˆçš„è¿‡ç¨‹å¸¸ç§°ä¸ºdecoding</p>
<p>è‡ªå›å½’ç”Ÿæˆ</p>
<h4 id="greedy-decoding">Greedy decoding
</h4><p>è´ªå¿ƒè§£ç ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„é‚£ä¸ªtokenç”Ÿæˆï¼ˆargmaxï¼‰</p>
<p><img src="/assets/Pasted%20image%2020250903221610.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>æ•ˆæœä¸å¥½â€”â€”è¾“å…¥æ–‡æœ¬å¦‚æœç›¸åŒï¼Œç»“æœæ˜¯å›ºå®šçš„ã€‚</p>
<p><strong>æŸæœç´¢</strong></p>
<h4 id="random-sampling">Random Sampling
</h4><p>æŒ‰ç…§åˆ†å¸ƒé‡‡æ ·ï¼Œç›´åˆ°é‡‡æ ·åˆ°EOSã€‚</p>
<p><img src="/assets/Pasted%20image%2020250903221651.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>æ•ˆæœä¹Ÿä¸å¥½â€”â€”æœ‰äº›tokenè™½ç„¶å æ¯”å°ï¼Œä½†æ˜¯è¿™äº›tokenå¾ˆå¤šï¼Œå¯¼è‡´å æ¯”ä¹Ÿä¸å°ã€‚å¦‚æœè¢«é‡‡æ ·åˆ°äº†ï¼Œå¥å­ä¼šå˜å¾—å¾ˆå¥‡æ€ª</p>
<h4 id="temperature-sampling">Temperature Sampling
</h4><p>logitsè½¬åŒ–ä¸ºprobabilityçš„æ—¶å€™ç”¨å¸¦æœ‰temperatureçš„softmaxè®¡ç®—ã€‚</p>
<p><img src="/assets/Pasted%20image%2020250903221700.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>å½“$\tau \le 1$çš„æ—¶å€™ï¼Œä¼šå€¾å‘å°†é«˜æ¦‚ç‡æ‹‰å¾—æ›´é«˜ï¼Œä½æ¦‚ç‡æ‹‰å¾—æ›´ä½ï¼Œåä¹‹åˆ™æ›´å®¹æ˜“é€‰æ‹©åˆ°ä½æ¦‚ç‡äº‹ä»¶ã€‚</p>
<p><img src="/assets/Pasted%20image%2020250903221710.png"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="training-large-language-models">Training Large Language Models
</h3><p>ä¸€èˆ¬åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š</p>
<ul>
<li>é¢„è®­ç»ƒ</li>
<li>æŒ‡ä»¤å¾®è°ƒ</li>
<li>åå¥½å¯¹é½</li>
</ul>
<p><img src="/assets/Pasted%20image%2020250903221718.png"
	
	
	
	loading="lazy"
	
	
></p>
<h4 id="self-supervised-training-algorithm-for-pretraining">Self-supervised Training Algorithm for Pretraining
</h4><p>Teacher forcingï¼šæ°¸è¿œç»™æ¨¡å‹æ­£ç¡®çš„åºåˆ—ï¼Œè€Œä¸æ˜¯æŒ‰ç…§æ¨¡å‹çš„é¢„æµ‹æ¥ç€å¾€ä¸‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯</p>
<p>å¦‚ä¸‹å›¾ï¼Œç½‘ç»œä¸­çš„æƒé‡ä¼šé€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œè°ƒæ•´ï¼Œä»¥æœ€å°åŒ–è¯¥æ‰¹æ¬¡ä¸Šçš„å¹³å‡äº¤å‰ç†µæŸå¤±ã€‚</p>
<p><img src="/assets/Pasted%20image%2020250903221730.png"
	
	
	
	loading="lazy"
	
	
></p>
<p>è¿™äº›æƒé‡åŒ…æ‹¬åµŒå…¥çŸ©é˜µ Eã€‚ç”±æ­¤å­¦ä¹ åˆ°çš„åµŒå…¥ï¼Œå°†èƒ½æœ€æœ‰æ•ˆåœ°é¢„æµ‹åç»­è¯è¯­ã€‚</p>
<h4 id="pretraining-corpora-for-large-language-models">Pretraining Corpora for Large Language Models
</h4><p>è®­ç»ƒæ•°æ®å¯ä»¥ç”¨ç½‘ç»œæ•°æ®ï¼Œå¹¶ä¸”åŠ ä¸Šä¸€äº›ç²¾å¿ƒç­›é€‰çš„æ•°æ®ã€‚</p>
<ul>
<li>common crawl</li>
<li>Colossal CLean Crawled Corpus</li>
<li>The Pile</li>
<li>Dolma</li>
</ul>
<p>é¿å…ä¸ªäººä¿¡æ¯PIIï¼Œå»é‡ï¼Œå®‰å…¨ç­›é€‰ï¼Œtoxicity detectionã€‚æ³¨æ„ç‰ˆæƒã€æ•°æ®åŒæ„ã€éšç§å’Œåå·®ç­‰é—®é¢˜ã€‚</p>
<h4 id="finetuning">Finetuning
</h4><p>å¯¹å·²ç»ä¸è®­ç»ƒè¿‡çš„æ¨¡å‹åŠ å…¥ä¸€äº›æ–°çš„çŸ¥è¯†è¿›è¡Œå¾®è°ƒã€‚å¦‚æœæ–°çš„æ•°æ®æ˜¯é¢„è®­ç»ƒçš„åé¢ï¼Œä¹Ÿå¯ä»¥å«continued pretrainingã€‚</p>
<p><img src="/2025/speech-and-language-processing/assets/IMG-20250903194702282.png"
	width="704"
	height="363"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903194702282_hu_ef5bfaca2f0c0697.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903194702282_hu_5326aa4cc5696be2.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="193"
		data-flex-basis="465px"
	
></p>
<h3 id="evaluating-large-language-models">Evaluating Large Language Models
</h3><h4 id="perplexity">Perplexity
</h4><p>ç”±äºé“¾å¼æ³•åˆ™ï¼Œä½¿ç”¨<strong>å¯¹æ•°ä¼¼ç„¶</strong>æ¥ä½œä¸ºè¡¡é‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡çš„è¯ï¼Œæµ‹è¯•é›†çš„æ¦‚ç‡å¤§å°ä¼šå—åˆ°tokenæ•°é‡çš„å½±å“ã€‚æ–‡æœ¬è¶Šé•¿ï¼Œæµ‹è¯•é›†çš„æ¦‚ç‡è¶Šå°ã€‚</p>
<p>Perplexityï¼šå›°æƒ‘åº¦ï¼Œé•¿åº¦å½’ä¸€åŒ–çš„æŒ‡æ ‡ã€‚å›°æƒ‘åº¦çš„å…·ä½“å…¬å¼æ˜¯æµ‹è¯•é›†æ¦‚ç‡çš„å€’æ•°ï¼Œå†æŒ‰æ ‡è®°æ•°é‡è¿›è¡Œå½’ä¸€åŒ–ã€‚
</p>
$$\begin{aligned}
\text{Perplexity}_{\boldsymbol{\theta}}(w_{1:n}) & =P_{\boldsymbol{\theta}}(w_{1:n})^{-\frac{1}{n}} \\
 & =\sqrt[n]{\frac{1}{P_{\boldsymbol{\theta}}(w_{1:n})}}
\end{aligned}$$<p>
<strong>å›°æƒ‘åº¦è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½ã€‚</strong></p>
<blockquote>
<p>å›°æƒ‘åº¦éå¸¸ä¾èµ–tokensçš„æ•°é‡ï¼Œå› æ­¤ä¸èƒ½å¯¹ä¸¤ä¸ªä½¿ç”¨ä¸åŒtokenizerçš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œè€Œæ˜¯åªèƒ½å¯¹ä½¿ç”¨åŒä¸€ä¸ªtokenizerçš„æ¨¡å‹æ¯”è¾ƒã€‚</p></blockquote>
<h4 id="downstream-tasks-reasoniong-and-world-knowledge">Downstream Tasks: Reasoniong and World Knowledge
</h4><p>å‡†ç¡®ç‡ï¼šå¯ä»¥ç›´æ¥ä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡æ¥è¡¡é‡ã€‚</p>
<ul>
<li>MMLU
<img src="/2025/speech-and-language-processing/assets/IMG-20250903214854478.png"
	width="805"
	height="289"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903214854478_hu_f3db381a926f2221.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903214854478_hu_7d93c033b6db94ba.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="278"
		data-flex-basis="668px"
	
>
<img src="/2025/speech-and-language-processing/assets/IMG-20250903214915252.png"
	width="806"
	height="475"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250903214915252_hu_101f6843eda66160.png 480w, /2025/speech-and-language-processing/assets/IMG-20250903214915252_hu_d2314743aaea0c7f.png 1024w"
	loading="lazy"
	
		alt="ä¸€ä¸ª2-shot promptçš„MMLUé«˜ä¸­æ•°å­¦é¢˜"
	
	
		class="gallery-image" 
		data-flex-grow="169"
		data-flex-basis="407px"
	
></li>
</ul>
<p>ä½†æ˜¯é—®é¢˜æ˜¯<strong>æ•°æ®æ³„éœ²</strong>ã€‚</p>
<h4 id="other-factors-for-evaluating-language-models">Other Factors for Evaluating Language Models
</h4><p>æ¨¡å‹å¤§å°ï¼Œè®­ç»ƒæ—¶é—´ï¼Œæ¨ç†æ—¶é—´ï¼ŒGPUæ•°é‡</p>
<p>å…¬å¹³</p>
<p>leaderboards</p>
<h3 id="ethical-and-safety-issues-with-language-models">Ethical and Safety Issues with Language Models
</h3><p>å¤§æ¨¡å‹å¹»è§‰é—®é¢˜ï¼šRAG</p>
<p>å®‰å…¨é—®é¢˜ï¼šå®‰å…¨å¾®è°ƒå’Œå¯¹é½</p>
<p>representational harms</p>
<p>éšç§é—®é¢˜</p>
<p>æƒ…æ„Ÿä¾èµ–</p>
<p>å¢é•¿è°è¨€ã€å®£ä¼ ã€è™šå‡ä¿¡æ¯ç­‰æ–‡æœ¬ç”Ÿæˆ</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/llm/">LLM</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>All rights reserved.</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    


<aside class="related-content--wrapper">
    <h2 class="section-title">ç›¸å…³æ–‡ç« </h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/2024/langchain-learning/">
        
        
            <div class="article-image">
                <img src="/2024/langchain-learning/cover.ddb755e13e74b4dc9c3215937020b858_hu_f1f250f84bcd151e.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post å­¦ä¹ ç¬”è®° | LangChainå­¦ä¹ ç¬”è®°"
                        data-key="langchain-learning" 
                        data-hash="md5-3bdV4T50tNycMhWTcCC4WA==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">å­¦ä¹ ç¬”è®° | LangChainå­¦ä¹ ç¬”è®°</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/2025/rbdr/">
        
        

        <div class="article-details">
            <h2 class="article-title">AIDR | Reasoning-based Drug Repurposing</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/2025/aidr-network-method/">
        
        

        <div class="article-details">
            <h2 class="article-title">AIDR | åŸºäºç½‘ç»œçš„æ–¹æ³•</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 ionfeather&#39;Log
    </section>
    
    <section class="powerby">
        ä½¿ç”¨ <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> æ„å»º <br />
        ä¸»é¢˜ <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.29.0">Stack</a></b> ç”± <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> è®¾è®¡
    </section>
</footer>


<script>
    (function(u, c) {
      var d = document, t = 'script', o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function(e) { c(e); }); }
      s.parentNode.insertBefore(o, s);
    })('//cdn.bootcss.com/pangu/3.3.0/pangu.min.js', function() {
      pangu.spacingPage();
    });
</script>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>
<script src="https://npm.elemecdn.com/nprogress@0.2.0/nprogress.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://npm.elemecdn.com/nprogress@0.2.0/nprogress.css" crossorigin="anonymous" />
<script>
    NProgress.start();
    document.addEventListener("readystatechange", () => {
        if (document.readyState === "interactive") NProgress.inc(0.8);
        if (document.readyState === "complete") NProgress.done();
    });
</script>


    </body>
</html>
