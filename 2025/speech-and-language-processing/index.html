<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Introduction\rÈòÖËØªSpeech and Language ProcessingËøôÊú¨‰π¶ÁöÑ‰∏Ä‰∫õÁ¨îËÆ∞Ôºå‰ª•‰æõÂêéÊù•ÁöÑËá™Â∑±ÂèÇËÄÉ„ÄÇ\nWords and Tokens\rÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™‰∏úË•øÊù•Âª∫Ê®°ËØ≠Ë®ÄÔºå‰∏ãÈù¢ÊòØÊàë‰ª¨ÁöÑÈÄâÊã©Ôºö\nWords\r‰∏∫‰ªÄ‰πà‰∏çÁî®ËØçÔºü\nÊúâ‰∫õËØ≠Ë®ÄÊ≤°Êúâorthographic words ËØçÁöÑÊï∞Èáè‰ºöÈöèÁùÄÊñáÁ´†Â¢ûÈïøÔºåËØçÊ±áË°®Ê∞∏ËøúÈÉΩ‰ºöË¶ÜÁõñ‰∏çË∂≥ Morphemes\rËØ≠Á¥†Á±ªÂûã\n">
<title>‰π¶Á±çÈòÖËØª | Speech and Language Processing</title>

<link rel='canonical' href='https://ionfeather.github.io/2025/speech-and-language-processing/'>

<link rel="stylesheet" href="/scss/style.min.cdd95828ca8971b17ccb14112222a60d19d84ea3f4e5b525c8c68fb4d2a4535d.css"><meta property='og:title' content="‰π¶Á±çÈòÖËØª | Speech and Language Processing">
<meta property='og:description' content="Introduction\rÈòÖËØªSpeech and Language ProcessingËøôÊú¨‰π¶ÁöÑ‰∏Ä‰∫õÁ¨îËÆ∞Ôºå‰ª•‰æõÂêéÊù•ÁöÑËá™Â∑±ÂèÇËÄÉ„ÄÇ\nWords and Tokens\rÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™‰∏úË•øÊù•Âª∫Ê®°ËØ≠Ë®ÄÔºå‰∏ãÈù¢ÊòØÊàë‰ª¨ÁöÑÈÄâÊã©Ôºö\nWords\r‰∏∫‰ªÄ‰πà‰∏çÁî®ËØçÔºü\nÊúâ‰∫õËØ≠Ë®ÄÊ≤°Êúâorthographic words ËØçÁöÑÊï∞Èáè‰ºöÈöèÁùÄÊñáÁ´†Â¢ûÈïøÔºåËØçÊ±áË°®Ê∞∏ËøúÈÉΩ‰ºöË¶ÜÁõñ‰∏çË∂≥ Morphemes\rËØ≠Á¥†Á±ªÂûã\n">
<meta property='og:url' content='https://ionfeather.github.io/2025/speech-and-language-processing/'>
<meta property='og:site_name' content='ionfeather&#39;Log'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='LLM' /><meta property='article:published_time' content='2025-08-28T19:28:49&#43;08:00'/><meta property='article:modified_time' content='2025-08-28T19:28:49&#43;08:00'/>
<meta name="twitter:title" content="‰π¶Á±çÈòÖËØª | Speech and Language Processing">
<meta name="twitter:description" content="Introduction\rÈòÖËØªSpeech and Language ProcessingËøôÊú¨‰π¶ÁöÑ‰∏Ä‰∫õÁ¨îËÆ∞Ôºå‰ª•‰æõÂêéÊù•ÁöÑËá™Â∑±ÂèÇËÄÉ„ÄÇ\nWords and Tokens\rÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™‰∏úË•øÊù•Âª∫Ê®°ËØ≠Ë®ÄÔºå‰∏ãÈù¢ÊòØÊàë‰ª¨ÁöÑÈÄâÊã©Ôºö\nWords\r‰∏∫‰ªÄ‰πà‰∏çÁî®ËØçÔºü\nÊúâ‰∫õËØ≠Ë®ÄÊ≤°Êúâorthographic words ËØçÁöÑÊï∞Èáè‰ºöÈöèÁùÄÊñáÁ´†Â¢ûÈïøÔºåËØçÊ±áË°®Ê∞∏ËøúÈÉΩ‰ºöË¶ÜÁõñ‰∏çË∂≥ Morphemes\rËØ≠Á¥†Á±ªÂûã\n">
    <link rel="shortcut icon" href="/ion.ico" />

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&display=swap"
    onload="this.media='all'" onError="this.media='none'">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@callmebill/lxgw-wenkai-web@latest/style.css"
    onload="this.media='all'" onError="this.media='none'">

  <style>
     
    :root {
      --sys-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --zh-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --base-font-family: 'LXGW WenKai', 'Noto Serif SC', serif;
      --code-font-family: 'Consolas', monospace; 
      --article-font-family: 'Noto Serif SC', serif; 
      --heading-font-family: 'LXGW WenKai', serif; 
    }

     
    body {
      font-family: var(--base-font-family);
      font-weight: normal;
    }
  </style>
</head>

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="ÂàáÊç¢ËèúÂçï">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_7d702df343b40e37.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">üå≥</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">ionfeather&#39;Log</a></h1>
            <h2 class="site-description">ÂçÅÂπ¥È•ÆÂÜ∞ÔºåÈöæÂáâÁÉ≠Ë°Ä</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/ionfeather'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg t="1732597946058" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3746" width="24" height="24"><path d="M850.346667 155.008a42.666667 42.666667 0 0 0-22.741334-23.509333c-8.704-3.754667-85.717333-33.322667-200.32 39.168H396.714667c-114.773333-72.618667-191.701333-42.922667-200.32-39.168a42.88 42.88 0 0 0-22.741334 23.466666c-26.197333 66.218667-18.048 136.448-7.850666 176.896C134.272 374.016 128 413.098667 128 469.333333c0 177.877333 127.104 227.882667 226.730667 246.272a189.568 189.568 0 0 0-13.013334 46.549334A44.373333 44.373333 0 0 0 341.333333 768v38.613333c-19.498667-4.138667-41.002667-11.946667-55.168-26.112C238.08 732.416 188.330667 682.666667 128 682.666667v85.333333c25.002667 0 65.365333 40.362667 97.834667 72.832 51.029333 51.029333 129.066667 55.253333 153.386666 55.253333 3.114667 0 5.376-0.085333 6.528-0.128A42.666667 42.666667 0 0 0 426.666667 853.333333v-82.090666c4.266667-24.746667 20.224-49.621333 27.946666-56.362667a42.666667 42.666667 0 0 0-23.125333-74.581333C293.333333 624.554667 213.333333 591.488 213.333333 469.333333c0-53.12 5.632-70.741333 31.573334-99.285333 11.008-12.117333 14.08-29.568 7.978666-44.8-4.821333-11.904-18.773333-65.450667-6.485333-117.546667 20.650667-1.578667 59.904 4.565333 113.706667 40.96C367.104 253.44 375.466667 256 384 256h256a42.666667 42.666667 0 0 0 23.936-7.338667c54.016-36.522667 92.970667-41.770667 113.664-41.130666 12.330667 52.224-1.578667 105.770667-6.4 117.674666a42.666667 42.666667 0 0 0 8.021333 44.928C805.077333 398.464 810.666667 416.085333 810.666667 469.333333c0 122.581333-79.957333 155.52-218.069334 170.922667a42.666667 42.666667 0 0 0-23.125333 74.709333c19.797333 17.066667 27.861333 32.469333 27.861333 53.034667v128h85.333334v-128c0-20.437333-3.925333-38.101333-9.770667-53.12C769.92 695.765333 896 643.712 896 469.333333c0-56.362667-6.272-95.530667-37.76-137.514666 10.197333-40.405333 18.261333-110.506667-7.893333-176.810667z" fill="currentColor" p-id="3747"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='mailto:lizishadowmay@gmail.com'
                        target="_blank"
                        title="Email"
                        rel="me"
                    >
                        
                        
                            <svg t="1732597869588" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="23464" width="24" height="24"><path d="M926.47619 355.644952V780.190476a73.142857 73.142857 0 0 1-73.142857 73.142857H170.666667a73.142857 73.142857 0 0 1-73.142857-73.142857V355.644952l73.142857 62.000762V780.190476h682.666666V417.645714l73.142857-62.000762zM853.333333 170.666667a74.044952 74.044952 0 0 1 26.087619 4.778666 72.704 72.704 0 0 1 30.622477 22.186667 73.508571 73.508571 0 0 1 10.678857 17.67619c3.169524 7.509333 5.12 15.652571 5.607619 24.210286L926.47619 243.809524v24.380952L559.469714 581.241905a73.142857 73.142857 0 0 1-91.306666 2.901333l-3.632762-2.925714L97.52381 268.190476v-24.380952a72.899048 72.899048 0 0 1 40.155428-65.292191A72.97219 72.97219 0 0 1 170.666667 170.666667h682.666666z m-10.971428 73.142857H181.638095L512 525.58019 842.361905 243.809524z" p-id="23465" fill="currentColor"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='index.xml'
                        target="_blank"
                        title="RSS"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon" viewBox="0 0 1024 1024" width="200" height="200">
  <path 
    d="M170.666667 426.666667c-25.6 0-42.666667 17.066667-42.666667 42.666666s17.066667 42.666667 42.666667 42.666667c187.733333 0 341.333333 153.6 341.333333 341.333333 0 25.6 17.066667 42.666667 42.666667 42.666667s42.666667-17.066667 42.666666-42.666667c0-234.666667-192-426.666667-426.666666-426.666666z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
  <path 
    d="M170.666667 128c-25.6 0-42.666667 17.066667-42.666667 42.666667s17.066667 42.666667 42.666667 42.666666c354.133333 0 640 285.866667 640 640 0 25.6 17.066667 42.666667 42.666666 42.666667s42.666667-17.066667 42.666667-42.666667c0-401.066667-324.266667-725.333333-725.333333-725.333333z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
  <path 
    d="M213.333333 810.666667m-85.333333 0a85.333333 85.333333 0 1 0 170.666667 0 85.333333 85.333333 0 1 0-170.666667 0Z"
    stroke="currentColor"
    fill="currentColor"
    stroke-width="0"
  ></path>
</svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>‰∏ªÈ°µ</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>ÂÖ≥‰∫é</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>ÂΩíÊ°£</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>ÊêúÁ¥¢</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%8F%8B%E9%93%BE/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>ÂèãÈìæ</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            

                
                    <span id="dark-mode-toggle">
                        <svg  xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left"   width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828a4 4 0 1 0 -5.656 -5.656a4 4 0 0 0 5.656 5.656z" /><path d="M6.343 17.657l-1.414 1.414" /><path d="M6.343 6.343l-1.414 -1.414" /><path d="M17.657 6.343l1.414 -1.414" /><path d="M17.657 17.657l1.414 1.414" /><path d="M4 12h-2" /><path d="M12 4v-2" /><path d="M20 12h2" /><path d="M12 20v2" /></svg>
                        <svg  xmlns="http://www.w3.org/2000/svg"  class="icon icon-tabler icon-tabler-toggle-right"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-moon"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" /></svg>
                    </span>
                
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">ÁõÆÂΩï</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#words-and-tokens">Words and Tokens</a>
      <ol>
        <li><a href="#words">Words</a></li>
        <li><a href="#morphemes">Morphemes</a></li>
        <li><a href="#unicode">Unicode</a>
          <ol>
            <li><a href="#code-points">Code Points</a></li>
            <li><a href="#utf-8">UTF-8</a></li>
          </ol>
        </li>
        <li><a href="#subword-tokenization-byte-pair-encoding">Subword Tokenization: Byte-Pair Encoding</a>
          <ol>
            <li><a href="#bpe">BPE</a></li>
            <li><a href="#bpe-encoder">BPE encoder</a></li>
            <li><a href="#bpe-in-practice">BPE in practice</a></li>
          </ol>
        </li>
        <li><a href="#rule-based-tokenization">Rule-based tokenization</a>
          <ol>
            <li><a href="#sentence-segmentation">Sentence Segmentation</a></li>
          </ol>
        </li>
        <li><a href="#corpora">Corpora</a></li>
        <li><a href="#regular-expressions">Regular Expressions</a></li>
        <li><a href="#simple-unix-tools-for-word-tokenization">Simple Unix Tools for Word Tokenization</a></li>
        <li><a href="#minimum-edit-distance">Minimum Edit Distance</a>
          <ol>
            <li><a href="#the-minimum-edit-distance-algorithm">The Minimum Edit Distance Algorithm</a></li>
          </ol>
        </li>
        <li><a href="#exercies">Exercies</a>
          <ol>
            <li></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#n-gram-language-models">N-gram Language Models</a>
      <ol>
        <li><a href="#n-grams">N-Grams</a>
          <ol>
            <li><a href="#how-to-estimate-probabilities">How to estimate probabilities</a></li>
            <li><a href="#dealing-with-scale-in-large-n-gram-models">Dealing with scale in large n-gram models</a></li>
            <li><a href="#evaluating-language-models-training-and-test-sets">Evaluating Language Models: Training and Test Sets</a></li>
            <li><a href="#evaluating-language-models-perplexity">Evaluating Language Models: Perplexity</a></li>
          </ol>
        </li>
        <li><a href="#sampling-sentences-from-a-language-model">Sampling sentences from a language model</a></li>
        <li><a href="#generalizing-vs-overfitting-the-training-set">Generalizing vs. overfitting the training set</a></li>
        <li><a href="#smoothing-interpolation-and-backoff">Smoothing, Interpolation, and Backoff</a>
          <ol>
            <li><a href="#laplace-smoothing">Laplace Smoothing</a></li>
            <li><a href="#add-k-smoothing">Add-k Smoothing</a></li>
            <li><a href="#language-model-interpolation">Language Model Interpolation</a></li>
            <li><a href="#stupid-backoff">Stupid Backoff</a></li>
          </ol>
        </li>
        <li><a href="#advanced-perplexitys-relation-to-entropy">Advanced: Perplexity&rsquo;s Relation to Entropy</a></li>
        <li><a href="#excercies">Excercies</a>
          <ol>
            <li></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="background-color: #5DB9AE; color: #fff;">
                Â≠¶‰π†Êú≠ËÆ∞
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/2025/speech-and-language-processing/">‰π¶Á±çÈòÖËØª | Speech and Language Processing</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-08-28</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    ÈòÖËØªÊó∂Èïø: 8 ÂàÜÈíü
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="introduction">Introduction
</h2><p>ÈòÖËØª<a class="link" href="https://web.stanford.edu/~jurafsky/slp3/"  target="_blank" rel="noopener"
    >Speech and Language Processing</a>ËøôÊú¨‰π¶ÁöÑ‰∏Ä‰∫õÁ¨îËÆ∞Ôºå‰ª•‰æõÂêéÊù•ÁöÑËá™Â∑±ÂèÇËÄÉ„ÄÇ</p>
<h2 id="words-and-tokens">Words and Tokens
</h2><p>Êàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™‰∏úË•øÊù•Âª∫Ê®°ËØ≠Ë®ÄÔºå‰∏ãÈù¢ÊòØÊàë‰ª¨ÁöÑÈÄâÊã©Ôºö</p>
<h3 id="words">Words
</h3><p>‰∏∫‰ªÄ‰πà‰∏çÁî®ËØçÔºü</p>
<ul>
<li>Êúâ‰∫õËØ≠Ë®ÄÊ≤°Êúâorthographic words</li>
<li>ËØçÁöÑÊï∞Èáè‰ºöÈöèÁùÄÊñáÁ´†Â¢ûÈïøÔºåËØçÊ±áË°®Ê∞∏ËøúÈÉΩ‰ºöË¶ÜÁõñ‰∏çË∂≥</li>
</ul>
<h3 id="morphemes">Morphemes
</h3><p>ËØ≠Á¥†Á±ªÂûã</p>
<ul>
<li>Â±àÊäòËØ≠Á¥†Ôºöinflectional morphemes</li>
<li>Ê¥æÁîüËØ≠Á¥†Ôºöderivational morphemes</li>
<li>ÈôÑÁùÄËØ≠Á¥†Ôºöclitic</li>
</ul>
<p>ËØ≠Ë®ÄÁ±ªÂûã</p>
<ul>
<li>Analytic</li>
<li>polysynthetic</li>
<li>fusional</li>
<li>agglutinative</li>
</ul>
<p>‰∏∫‰ªÄ‰πà‰∏çÁî®ËØ≠Á¥†Ôºü</p>
<ul>
<li>ËØ≠Á¥†ÂæàÂ§çÊùÇÔºåÂæàÈöæÂÆö‰πâ</li>
<li>‰∏çÂêåËØ≠Ë®Ä‰∏çÂêå‰∏îÈöæ‰ª•Áªü‰∏Ä</li>
</ul>
<h3 id="unicode">Unicode
</h3><p>UnicodeÁöÑÂéÜÂè≤</p>
<ul>
<li>ASCII</li>
<li>CJKV</li>
<li>‰∏çÊñ≠Êõ¥Êñ∞‰∏≠ÔºåË∂äÊù•Ë∂äÂ§öÔºåUnicode 16.0Â∑≤ÁªèÂåÖÂê´Ë∂ÖËøá150000‰∏™Â≠óÁ¨¶</li>
</ul>
<h4 id="code-points">Code Points
</h4><ul>
<li>U+ÔºöË°®Á§∫Êé•‰∏ãÊù•Ë¶ÅÁî®UnicodeÂçÅÂÖ≠ËøõÂà∂Ë°®Á§∫‰∏Ä‰∏™code point</li>
<li>U+0061Ôºö0x0061‰∏Ä‰∏™ÊÑèÊÄùÔºå‰πüÂ∞±Â∞èÂÜôÂ≠óÊØça„ÄÇ</li>
</ul>
<h4 id="utf-8">UTF-8
</h4><p>ÁõÆÂâçÊúÄÂ∏∏Áî®ÁöÑencodingÂ≠óÁ¨¶ÁöÑÊñπÂºè„ÄÇ‰∏≠ÊñáÂ≠óÁ¨¶ ‚Äú‰∏≠‚Äù ÁöÑ Unicode Á†ÅÁÇπÊòØ<code>U+4E2D</code>ÔºåUTF-8 ÁºñÁ†ÅÂêé‰∏∫ 3 ‰∏™Â≠óËäÇÔºö<code>0xE4 0xB8 0xAD</code></p>
<p>UTF-8ÊòØ‰∏ÄÁßçÂèòÈïøÁºñÁ†ÅÔºåÂÖºÂÆπASCII„ÄÇ</p>
<ul>
<li>Â¶Ç„Äå‰∏ñ„ÄçÔºåUTF-8 ÁºñÁ†ÅÊòØ<code>0xE4 B8 96</code>ÔºåÂÖ∂‰∏≠E4ÁöÑ‰∫åËøõÂà∂‰∏∫<code>11100110H</code>ÔºåÂºÄÂ§¥ÁöÑ<code>1110H</code>Ë°®Á§∫ËøôÊòØ‰∏Ä‰∏™3Â≠óËäÇÂ≠óÁ¨¶ÁöÑÁ¨¨‰∏Ä‰∏™Â≠óËäÇ„ÄÇ</li>
</ul>
<h3 id="subword-tokenization-byte-pair-encoding">Subword Tokenization: Byte-Pair Encoding
</h3><p>‰∏äÈù¢ÁöÑ‰∏â‰∏™ÂÄôÈÄâÈÉΩ‰∏çË°åÔºåwordÂíåmorphemeÈöæ‰ª•ËßÑËåÉÂÆö‰πâÔºåcharacterÂèØ‰ª•ÈÄöËøáunicodeÊù•ÂÆö‰πâÔºå‰ΩÜÂèàÂØπ‰∫é‰Ωú‰∏∫tokensÊù•ËØ¥Â§™Â∞è‰∫Ü„ÄÇ</p>
<p>‰∏∫‰ªÄ‰πàË¶ÅtokenizeËæìÂÖ•Ôºü</p>
<ul>
<li>Â∞ÜËæìÂÖ•ËΩ¨Êç¢‰∏∫‰∏ÄÁªÑÁ°ÆÂÆöÁöÑ„ÄÅÂõ∫ÂÆöÁöÑÂçïÂÖÉÔºàTokenÔºâÔºåËÉΩËÆ©‰∏çÂêåÁöÑÁÆóÊ≥ïÂíåÁ≥ªÁªüÂú®‰∏Ä‰∫õÁÆÄÂçïÈóÆÈ¢ò‰∏äËææÊàêÂÖ±ËØÜ„ÄÇ‰æãÂ¶ÇÂõ∞ÊÉëÂ∫¶ÁöÑËÆ°ÁÆó„ÄÇ</li>
<li>ÂØπÂèØÂ§çÁé∞ÂæàÈáçË¶Å</li>
<li>‰∏∫‰∫ÜÊ∂àÈô§unknown wordsÁöÑÈóÆÈ¢ò</li>
</ul>
<p>‰∏∫‰∫ÜÊ∂àÈô§unknown wordsÈóÆÈ¢òÔºåÁé∞‰ª£tokenizersËá™Âä®ÂºïÂÖ•‰∫ÜtokenÂåÖÂê´ÈÇ£‰∫õÊØîwordsÂ∞èÁöÑtokenÔºåÂè´subword„ÄÇ</p>
<p>‰ΩøÁî®<a class="link" href="https://platform.openai.com/tokenizer"  target="_blank" rel="noopener"
    >Tokenizer - OpenAI API</a>‰∏≠ÁöÑ<code>GPT-4o &amp;  GPT-4o mini</code>Êù•ÂàÜËØç‰∏ãÈù¢Ëøô‰∏ÄÂ§ßÊÆµËØùÔºö</p>
<blockquote>
<p>For example, if we had happened not to ever see the word lower, when it appears we could segment it successfully into low and er which we had already seen. In the worst case, a really unusual word (perhaps an acronym like GRPO) could be tokenized as a sequence of individual letters if necessary.</p></blockquote>
<p>ÊúÄÁªàÂæóÂà∞ÁöÑÊòØ
<img src="/2025/speech-and-language-processing/assets/IMG-20250828205105982.png"
	width="711"
	height="279"
	srcset="/2025/speech-and-language-processing/assets/IMG-20250828205105982_hu_b760c9dcae731844.png 480w, /2025/speech-and-language-processing/assets/IMG-20250828205105982_hu_3812253da13ed9c0.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="254"
		data-flex-basis="611px"
	
>
Áé∞Âú®ÊúÄÊµÅË°åÁöÑtokenization algorithmÊúâ‰∏§‰∏™Ôºö</p>
<ul>
<li>Byte-Pair Encoding(BPE)</li>
<li>Unigram Language modeling(ULM)</li>
</ul>
<h4 id="bpe">BPE
</h4><p>ÈÄöËøáÂàÜÊûêËÆ≠ÁªÉËØ≠ÊñôÔºåËá™Âä®Â≠¶‰π†Âá∫‰∏ÄÂ•óÂ≠êËØçÈõÜÂêàÔºàËØçÊ±áË°®ÔºâÔºå‰ΩøÂæóÈ´òÈ¢ëÂá∫Áé∞ÁöÑÂ≠óÁ¨¶ / Â≠êËØçÁªÑÂêàË¢´ÂêàÂπ∂‰∏∫Êõ¥Â§ßÁöÑÂ≠êËØçÂçï‰Ωç„ÄÇ</p>
<p>ËÆ≠ÁªÉÊñπÊ≥ï‰ªãÁªç„ÄÇ</p>
<h4 id="bpe-encoder">BPE encoder
</h4><h4 id="bpe-in-practice">BPE in practice
</h4><p>ÈÄöÂ∏∏ÔºåÊàë‰ª¨‰ºöÂØπ UTF-8 ÁºñÁ†ÅÊñáÊú¨ÁöÑ<strong>Âçï‰∏™Â≠óËäÇ</strong>ÊâßË°å BPE Êìç‰Ωú„ÄÇBPE Â§ÑÁêÜ ‚Äú‰∏≠‚Äù Êó∂ÔºåËæìÂÖ•Âπ∂Èùû<code>U+4E2D</code>Ëøô‰∏™Á†ÅÁÇπÔºåËÄåÊòØ<code>E4</code>„ÄÅ<code>B8</code>„ÄÅ<code>AD</code>Ëøô‰∏â‰∏™Áã¨Á´ãÂ≠óËäÇ„ÄÇ</p>
<p>‰ªÖÂú®<strong>È¢ÑÂÖàÂàáÂàÜÂá∫ÁöÑÂçïËØçÂÜÖÈÉ®</strong>ÊâßË°å BPE Êìç‰ΩúÔºåÊúâÂä©‰∫éÈÅøÂÖçÊΩúÂú®ÈóÆÈ¢ò„ÄÇ</p>
<p>‰∏Ä‰∫õËã±ËØ≠ÈáåÁöÑÂ∞èÂèëÁé∞Ôºö</p>
<ul>
<li>Â§ßÂ§öÊï∞ÂçïËØçÁöÑtokensÊòØ‰ªñ‰ª¨Ëá™Â∑±ÔºåÂåÖÂê´ËØçÂâçÁ©∫Ê†º„ÄÇËøôÊ†∑ÂèØ‰ª•ÈÅøÂÖçÁã¨Á´ãÂçïËØçÂíåÂçïËØçÂÜÖÈÉ®ÁöÑsubword„ÄÇ</li>
<li>ÈôÑÁùÄËØ≠Á¥†CliticsÂú®ÂêçÂ≠óÂêéÈù¢ÂàÜÂºÄÂçïÁã¨ÊàêtokenÔºå‰ΩÜÂú®Â∏∏ËßÅÁöÑËØçËØ≠ÂêéÈù¢‰ºöÊòØtokenÁöÑ‰∏ÄÈÉ®ÂàÜ</li>
<li>Êï∞Â≠óÈÄöÂ∏∏‰∏â‰Ωç‰∏ÄÁªÑ</li>
<li>‰∏Ä‰∫õËØçÔºåÂ¶ÇAnyhowÂíåanyhow‰ºöÊúâ‰∏çÂêåÁöÑÂàÜÂâ≤ÊñπÊ≥ï</li>
</ul>
<p>Ëøô‰∏™ÂíåÈ¢ÑÂ§ÑÁêÜÊúâÂÖ≥Á≥ª„ÄÇ</p>
<p>SuperBPE‰ºöÂêàÂπ∂Â∏∏ËßÑÁöÑBPEÂ≠êËØçÂàÜËØçÔºåÊïàÁéáÊõ¥È´ò„ÄÇ</p>
<p>ÁâπÂà´Âú∞Ôºå‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑtokensÊõ¥Á¢éÔºåÂ∞±‰ºöËæìÂá∫ËæπÈïøÔºåÊúÄÁªàLLMÁöÑÊïàÁéáÂèò‰Ωé„ÄÇ</p>
<h3 id="rule-based-tokenization">Rule-based tokenization
</h3><p>Penn Treebank Tokenization StandardÔºâÔºö‰∫ãÂÆûÊÄßËßÑËåÉ„ÄÇ</p>
<ul>
<li>ÂàÜÂºÄÈôÑÁùÄËØ≠Á¥†</li>
<li>‰øùÁïôËøûÂ≠óÁ¨¶ËøûÊé•ÁöÑËØç</li>
<li>ÂàÜÂºÄÊâÄÊúâÁöÑÊ†áÁÇπÁ¨¶Âè∑</li>
</ul>
<h4 id="sentence-segmentation">Sentence Segmentation
</h4><p>sentence tokenizationÂèØ‰ª•Âíåword tokenizationËÅîÂêàÂ§ÑÁêÜ„ÄÇ</p>
<h3 id="corpora">Corpora
</h3><p>ËØ≠ÊñôÂ∫ìÂíåËØ≠Ë®ÄÊï∞Èáè„ÄÅ‰ΩøÁî®ËÄÖÁöÑÁâπÂæÅÈÉΩÊúâÂÖ≥„ÄÇ</p>
<p>code switchingÔºöÂú®‰∏ÄÊ¨°ÊåÅÁª≠ÁöÑ‰∫§ÊµÅÔºâ‰∏≠ÔºåËØ¥ËØùËÄÖÊàñ‰ΩúËÄÖ‰∫§Êõø‰ΩøÁî®‰∏§ÁßçÊàñÂ§öÁßç ‚ÄúËØ≠Á†Å‚ÄùÁöÑÁé∞Ë±°„ÄÇ</p>
<p>datasheetÔºöÂ≠òÂÇ®‰∏ÄÂè•ËØùÁöÑÁâπÂæÅÔºåÂ¶ÇÊó∂Èó¥„ÄÅËØ¥ËØù‰∫∫ÊÄßÊ†º„ÄÅÈò∂Á∫ß&hellip;</p>
<h3 id="regular-expressions">Regular Expressions
</h3><p>Ê≠£ÂàôË°®ËææÂºèÁöÑÂÖ∑‰ΩìÂÆûÁé∞„ÄÇÂåÖÂê´Â≠óÁ¨¶ÊûêÂèñ„ÄÅËÆ°Êï∞„ÄÅÂèØÈÄâÊÄß„ÄÅÈÄöÈÖçÁ¨¶„ÄÅÈîöÁÇπÂíåËæπÁïå„ÄÅÊõøÊç¢ÂíåÊçïËé∑ÁªÑ„ÄÅÂâçÂêëÊñ≠Ë®ÄÁ≠â„ÄÇ</p>
<h3 id="simple-unix-tools-for-word-tokenization">Simple Unix Tools for Word Tokenization
</h3><p>ÂèØ‰ª•Âú®Unix„ÄÅLinuxÁ≥ªÁªü‰∏≠‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºè„ÄÇÂ¶Ç<code>tr -sc 'A-Za-z' '\n' &lt; sh.txt</code>Ë°®Á§∫‰ªé¬†<code>sh.txt</code>¬†Êñá‰ª∂‰∏≠ÊèêÂèñÊâÄÊúâËã±ÊñáÂ≠óÊØçÔºåÂπ∂Â∞ÜÈùûÂ≠óÊØçÂ≠óÁ¨¶ÊõøÊç¢‰∏∫Êç¢Ë°åÁ¨¶ÔºåÂêåÊó∂ÂéãÁº©ËøûÁª≠ÁöÑÈùûÂ≠óÊØçÂ≠óÁ¨¶‰∏∫Âçï‰∏™Êç¢Ë°åÁ¨¶„ÄÇ</p>
<h3 id="minimum-edit-distance">Minimum Edit Distance
</h3><p><strong>ÊúÄÂ∞èÁºñËæëË∑ùÁ¶ª</strong>ÔºöÂ∞Ü‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÈÄöËøá ‚ÄúÊèíÂÖ•‚Äù‚ÄúÂà†Èô§‚Äù‚ÄúÊõøÊç¢‚Äù ‰∏âÁßçÂü∫Êú¨Êìç‰ΩúËΩ¨Êç¢‰∏∫Âè¶‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÊâÄÈúÄÁöÑÊúÄÂ∞ëÊìç‰ΩúÊ¨°Êï∞</p>
<h4 id="the-minimum-edit-distance-algorithm">The Minimum Edit Distance Algorithm
</h4><p>‰∏Ä‰∏™ÁªèÂÖ∏ÁöÑÂä®ÊÄÅËßÑÂàíÈóÆÈ¢ò„ÄÇ</p>
<p><strong>Â≠óÁ¨¶ÂØπÈΩê</strong>ÔºöÈÄöËøáÂõûÊ∫ØÁºñËæëË∑ùÁ¶ªÁü©Èòµ‰∏≠ÁöÑ ‚ÄúÊúÄ‰ºòË∑ØÂæÑ‚ÄùÔºåÂèçÂêëÊé®ÂØºÂá∫Â∞Ü‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ËΩ¨Êç¢‰∏∫Âè¶‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÁöÑÂÖ∑‰ΩìÊìç‰ΩúÂ∫èÂàó„ÄÇ‰πüÂ∞±ÊòØ<strong>Ë∑ØÂæÑÂèØËßÜÂåñ</strong>„ÄÇ</p>
<h3 id="exercies">Exercies
</h3><h5 id="21">2.1
</h5><p>Write regular expressions for the following languages.</p>
<ol>
<li>The set of all alphabetic strings.</li>
<li>The set of all lowercase alphabetic strings ending in &ldquo;b&rdquo;.</li>
<li>The set of all strings from the alphabet {a, b} such that each &ldquo;a&rdquo; is immediately preceded by and immediately followed by a &ldquo;b&rdquo;.</li>
</ol>
<h5 id="22">2.2
</h5><p>Write regular expressions for the following languages. By &ldquo;word&rdquo;, we mean an alphabetic string separated from other words by whitespace, relevant punctuation, line breaks, etc.</p>
<ol>
<li>The set of all strings with two consecutive repeated words (e.g., &ldquo;Humbert Humbert&rdquo; and &ldquo;the the&rdquo; but not &ldquo;the bug&rdquo; or &ldquo;the big bug&rdquo;).</li>
<li>All strings that start at the beginning of the line with an integer and end at the end of the line with a word.</li>
<li>All strings that have both the word &ldquo;grotto&rdquo; and the word &ldquo;raven&rdquo; in them (but not, e.g., words like &ldquo;grottos&rdquo; that merely contain &ldquo;grotto&rdquo;).</li>
<li>Write a pattern that places the first word of an English sentence in a register. Deal with punctuation.</li>
</ol>
<h5 id="23">2.3
</h5><p>Implement an ELIZA-like program, using substitutions such as those described on page 27. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your program can legitimately engage in a lot of simple repetition.</p>
<h5 id="24">2.4
</h5><p>Compute the edit distance (using insertion cost 1, deletion cost 1, substitution cost 1) of &ldquo;leda&rdquo; to &ldquo;deal&rdquo;. Show your work (using the edit distance grid).</p>
<h5 id="25">2.5
</h5><p>Figure out whether &ldquo;drive&rdquo; is closer to &ldquo;brief&rdquo; or to &ldquo;divers&rdquo; and what the edit distance is to each. You may use any version of distance that you like.</p>
<h5 id="26">2.6
</h5><p>Now implement a minimum edit distance algorithm and use your hand-computed results to check your code.</p>
<h5 id="27">2.7
</h5><p>Augment the minimum edit distance algorithm to output an alignment; you will need to store pointers and add a stage to compute the backtrace.</p>
<h2 id="n-gram-language-models">N-gram Language Models
</h2><p>Êú¨Á´†‰ªãÁªçÊúÄÁÆÄÂçïÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºö<strong>NÂÖÉËØ≠Ê≥ïËØ≠Ë®ÄÊ®°Âûã</strong>„ÄÇ</p>
<h3 id="n-grams">N-Grams
</h3><p><strong>Ê¶ÇÁéáÈìæÂºèÊ≥ïÂàô</strong></p>
<h4 id="how-to-estimate-probabilities">How to estimate probabilities
</h4><p><strong>È©¨Â∞îÁßëÂ§´ÂÅáËÆæ</strong>ÔºöÂÅáËÆæ‰∏Ä‰∏™ÂçïËØçÁöÑÂá∫Áé∞Ê¶ÇÁéáÂè™ÂíåÂâçÈù¢ÁöÑ‰∏Ä‰∏™ÂçïËØçÊúâÂÖ≥„ÄÇÈÇ£‰πàn-gramÂç≥Âè™ÂíåÂâçÈù¢ÁöÑ$n-1$‰∏™ÂçïËØçÊúâÂÖ≥„ÄÇ</p>
<p><strong>ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°</strong>ÔºöÂ∑≤Áü•Ââç‰∏Ä‰∏™ËØç$w_{n‚àí1}$‚ÄãÊó∂ÔºåÂΩìÂâçËØç$w_n$‚ÄãÁöÑÊ¶ÇÁéá</p>
<p>ÁªàÊ≠¢Á¨¶Âè∑Ôºàend-symbolÔºâÔºöÊâÄÊúâÂèØËÉΩÂè•Â≠êÁöÑÊ¶ÇÁéáÊÄªÂíå‰∏∫ 1ÔºåÂê¶ÂàôÊòØÁâπÂÆöÈïøÂ∫¶ÁöÑÊâÄÊúâÂè•Â≠êÊ¶ÇÁéá‰πãÂíå‰∏∫ 1„ÄÇ</p>
<h4 id="dealing-with-scale-in-large-n-gram-models">Dealing with scale in large n-gram models
</h4><p>Log probabilities</p>
<p>NÂÖÉËØ≠Ê≥ïÁöÑËÆ°ÁÆóÁé∞Âú®ÁîöËá≥ËÉΩËææÂà∞Êó†ÈôêÂÖÉ„ÄÇ</p>
<p>ÂØπNÂÖÉËØ≠Ê≥ïÊ®°ÂûãËøõË°å‰øÆÂâ™‰πüÊòØÂæàÈáçË¶ÅÁöÑ„ÄÇ</p>
<h4 id="evaluating-language-models-training-and-test-sets">Evaluating Language Models: Training and Test Sets
</h4><p><strong>ÂÜÖÈÉ®ËØÑ‰º∞</strong>ÂíåÂ§ñÈÉ®ËØÑ‰º∞„ÄÇ</p>
<p>ËÆ≠ÁªÉÈõÜ„ÄÅÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜ„ÄÇ</p>
<h4 id="evaluating-language-models-perplexity">Evaluating Language Models: Perplexity
</h4><p><strong>PerplexityÔºàPPLÔºâ</strong>ÔºöÂõ∞ÊÉëÂ∫¶Ë∂ä‰ΩéÔºåËØ¥ÊòéÊ®°ÂûãÂØπÊñáÊú¨ÁöÑÈ¢ÑÊµãË∂äÂáÜÁ°ÆÔºàÂç≥Ê®°ÂûãË∂ä ‚Äú‰∏çÂõ∞ÊÉë‚ÄùÔºâ„ÄÇ</p>
<ul>
<li>ÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊòØ‚ÄúËÅîÂêàÊ¶ÇÁéáÂÄíÊï∞ÁöÑÂá†‰ΩïÂπ≥ÂùáÂÄº‚Äù„ÄÇ</li>
<li>Âú®ËÆ°ÁÆóÁöÑÊó∂ÂÄôÂ∏∏Â∏∏‰ºöÂèñÂØπÊï∞Êù•Â∞ÜÊ±Ç‰πòÁßØÂèò‰∏∫Ê±ÇÂíåÔºåÈÅøÂÖçÊï∞ÂÄºÈóÆÈ¢ò</li>
</ul>
<h5 id="perplexity-as-weighted-average-branching-factor">Perplexity as Weighted Average Branching Factor
</h5><p>Âõ∞ÊÉëÂ∫¶‰πüÂèØ‰ª•ÁêÜËß£‰∏∫<strong>Âä†ÊùÉÂπ≥ÂùáÂàÜÊîØÁ≥ªÊï∞</strong>„ÄÇÂÖ∂‰∏≠ÔºåËØ≠Ë®ÄÁöÑ ‚ÄúÂàÜÊîØÁ≥ªÊï∞‚ÄùÊåáÁöÑÊòØ ‚Äú‰ªª‰Ωï‰∏Ä‰∏™ËØç‰πãÂêéÂèØËÉΩÂá∫Áé∞ÁöÑ‰∏ã‰∏Ä‰∏™ËØçÁöÑÊï∞Èáè‚Äù„ÄÇ</p>
<h3 id="sampling-sentences-from-a-language-model">Sampling sentences from a language model
</h3><p>‚Äú0-1 Êï∞ËΩ¥ + Âå∫Èó¥Êò†Â∞Ñ‚ÄùÊù•ÁêÜËß£ÈááÊ†∑ÁöÑÂü∫Êú¨ÂéüÁêÜ„ÄÇ</p>
<h3 id="generalizing-vs-overfitting-the-training-set">Generalizing vs. overfitting the training set
</h3><p>ÂØπ‰∫éËééÂ£´ÊØî‰∫öÊñáÊú¨ÂíåÂçéÂ∞îË°óÊó•Êä•ÁöÑÊñáÊú¨Ôºå‰∏§ËÄÖÂ∑ÆÂºÇËøáÂ§ß‰ª•Ëá≥‰∫é‰∏çËÉΩÂàÜÂà´‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ„ÄÇ</p>
<p>ÊâÄ‰ª•ËØ¥Ë¶ÅÁ°Æ‰øùËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÁöÑÈ¢ÜÂüüË¶ÅÁõ∏‰ºº„ÄÇ</p>
<h3 id="smoothing-interpolation-and-backoff">Smoothing, Interpolation, and Backoff
</h3><p>zero probability n-gramsÊúâ‰∏§‰∏™ÈóÆÈ¢òÔºö</p>
<ul>
<li>‰Ωé‰º∞‰∫ÜËØçËØ≠Â∫èÂèØËÉΩÂá∫Áé∞ÁöÑÂèØËÉΩÊÄßÔºåÂØºËá¥ÊúÄÁªàÁöÑÊÄßËÉΩÂèòÂ∑Æ</li>
<li>Âõ∞ÊÉëÂ∫¶Êó†Ê≥ïËÆ°ÁÆóÔºåÂõ†‰∏∫Êó†Ê≥ïÈô§‰ª•0</li>
</ul>
<p>Âõ†Ê≠§ÈúÄË¶ÅSmoothingÊàñËÄÖdiscounting</p>
<h4 id="laplace-smoothing">Laplace Smoothing
</h4><p>ÂÖ∂ÂÆû‰πüÂ∞±ÊòØadd one smoothingÔºåÂ∞±ÊòØÂØπ‰∫éÊâÄÊúâÁöÑNÂÖÉËØ≠Ê≥ïÈÉΩÂä†‰∏Ä„ÄÇ</p>
<p>ÂØπ‰∫éËØ≠Ë®ÄÊ®°ÂûãÊù•ËØ¥ÔºåÁªìÊûúÂπ∂‰∏çÊòØÂæàÂ•Ω„ÄÇÂØπÊñáÊú¨ÂàÜÁ±ªÊúâÊïà„ÄÇ</p>
<h4 id="add-k-smoothing">Add-k Smoothing
</h4><p>‰πüÂ∞±ÊòØÂØπÊâÄÊúâÁöÑÈÉΩÂä†K„ÄÇ</p>
<p>ÂØπËØ≠Ë®ÄÊ®°ÂûãÊù•ËØ¥‰ªçÁÑ∂ÊïàÊûú‰∏ÄËà¨„ÄÇ</p>
<h4 id="language-model-interpolation">Language Model Interpolation
</h4><p><strong>n ÂÖÉËØ≠Ê≥ïÊèíÂÄºÊ≥ïÔºöÂä†ÊùÉËûçÂêà‰∏çÂêåÈò∂Êï∞ n ÂÖÉËØ≠Ê≥ïÁöÑÊ¶ÇÁéá</strong>ÔºåÈÅøÂÖçÈ´òÈò∂ÁöÑnÂÖÉËØ≠Ê≥ïÈõ∂Ê¶ÇÁéáÂØºËá¥ÁöÑÈ¢ÑÊµãÂ§±Êïà„ÄÇ</p>
<p>Âä†ÊùÉÁöÑ$\lambda$Â∫îËØ•ËÆæÁΩÆÊàêÂ§öÂ∞ëÂë¢ÔºüÂèØ‰ª•‰ªéÈ¢ÑÁïôÈõÜheld-out corpus‰∏≠Â≠¶‰π†„ÄÇ‰ΩøÁî®EMÔºàÊúüÊúõÊúÄÂ§ßÂåñÔºâÁÆóÊ≥ïÊù•Â≠¶‰π†„ÄÇ</p>
<h4 id="stupid-backoff">Stupid Backoff
</h4><p><strong>ÂõûÈÄÄÊ®°Âûã</strong>ÔºöÈ´òÈò∂nÈò∂ÁöÑÊ®°ÂûãÊó†Ê≥ï‰ΩøÁî®ÁöÑÊó∂ÂÄôÔºåÂõûÈÄÄÂà∞‰ΩéÈò∂Ê®°Âûã„ÄÇ</p>
<p><strong>Discount</strong>ÔºöË¶ÅËÆ©ÂõûÈÄÄÊ®°ÂûãÔºàbackoff modelÔºâËæìÂá∫ÂêàÁêÜÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÊàë‰ª¨ÂøÖÈ°ªÂØπÈ´òÈò∂ n ÂÖÉËØ≠Ê≥ïÁöÑÊ¶ÇÁéáËøõË°å ‚ÄúÊäòÊâ£Â§ÑÁêÜ‚ÄùÔºàdiscountÔºâÔºå‰ªéËÄåÈ¢ÑÁïôÂá∫ÈÉ®ÂàÜÊ¶ÇÁéá‰ΩôÈáèÔºàprobability massÔºâÔºå‰æõ‰ΩéÈò∂ n ÂÖÉËØ≠Ê≥ï‰ΩøÁî®„ÄÇ‰ΩÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Ôºå‰∫∫‰ª¨Â∏∏‰ΩøÁî®‰∏ÄÁßçÊõ¥ÁÆÄÂçïÁöÑ ‚ÄúÊó†ÊäòÊâ£ÂõûÈÄÄÁÆóÊ≥ï‚Äù‚Äî‚Äî Âç≥Âêç‰∏∫<strong>Stupid Backoff</strong>„ÄÇ</p>
<h3 id="advanced-perplexitys-relation-to-entropy">Advanced: Perplexity&rsquo;s Relation to Entropy
</h3><p><strong>ÁÜµ</strong>Ôºö‰∏çÁ°ÆÂÆöÊÄßÁöÑÂ∫¶ÈáèÊñπÂºè„ÄÇÂèØ‰ª•ÁêÜËß£ÊòØÁºñÁ†ÅÊüê‰∏™ÂÜ≥Á≠ñÊàñÊüêÊù°‰ø°ÊÅØÊâÄÈúÄÁöÑÊúÄÂ∞èÂπ≥ÂùáÊØîÁâπÊï∞„ÄÇË∂ä‰∏çÁ°ÆÂÆöÔºåÁÜµË∂äÂ§ß„ÄÇ</p>
<p><strong>ÁÜµÁéá</strong>ÔºöÂπ≥ÂùáÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÁÜµÁéáÂÆö‰πâ‰∏∫ ‚Äú<strong>Êó†ÈôêÈïøÂ∫èÂàó‰∏≠ÔºåÊØè‰∏™ËØçÁöÑÂπ≥ÂùáÁÜµ</strong>‚ÄùÔºåÂèçÊò†ËØ≠Ë®ÄÁöÑÈïøÊúü‰∏çÁ°ÆÂÆöÊÄß„ÄÇ‰æãÂ¶ÇÔºåËã±ÊñáÁöÑÁÜµÁéáÁ∫¶ 1-2 ÊØîÁâπ / ËØçÔºåÊÑèÂë≥ÁùÄÂπ≥ÂùáÊØè‰∏™ËØçÈúÄË¶Å 1-2 ÊØîÁâπÊù•ÁºñÁ†Å„ÄÇ</p>
<p><strong>Âπ≥Á®≥ÊÄß</strong>ÔºöÂ∫èÂàóÊ¶ÇÁéá‰∏çÈöèÁùÄÊó∂Èó¥ÊîπÂèò„ÄÇËá™ÁÑ∂ËØ≠Ë®Ä‰∏çÊòØÔºå‰ΩÜÊòØNÂÖÉËØ≠Ê≥ïÊòØÂπ≥Á®≥ÁöÑ„ÄÇ</p>
<p><strong>ÈÅçÂéÜÊÄß</strong>ÔºöÈïøÂ∫èÂàó‰∏≠ÂåÖÂê´‰∫ÜÊâÄÊúâÁöÑÁü≠Â∫èÂàó„ÄÇ</p>
<p><strong>Shannon-McMillan-Breiman theorem</strong>ÔºöÂ¶ÇÊûúËØ≠Ë®ÄÊª°Ë∂≥Êüê‰∫õÊ≠£ÂàôÊù°‰ª∂ÔºàÂáÜÁ°ÆÂú∞ËØ¥ÔºåÊòØÂπ≥Á®≥‰∏îÈÅçÂéÜÁöÑÔºâÔºå<strong>Â∫èÂàóÈïøÂ∫¶Ë∂ãËøë‰∫éÊó†Á©∑Â§ßÊó∂Ôºå‚ÄúÂ∫èÂàóÁöÑÂπ≥ÂùáÂØπÊï∞Ê¶ÇÁéáÁöÑË¥üÂÄº‚Äù ÔºåÂç≥ÁªèÈ™åÁÜµÁéá‰ºö‰ª•Ê¶ÇÁéá1Êî∂ÊïõÊïõÂà∞ËØ•ËøáÁ®ãÁöÑÁêÜËÆ∫ÁÜµÁéá</strong>„ÄÇ</p>
<p><strong>‰∫§ÂèâÁÜµÔºàCross-EntropyÔºâ</strong>ÔºöÊàë‰ª¨ËôΩÁÑ∂‰∏çÁü•ÈÅìÊï∞ÊçÆÁöÑÁúüÂÆûÊ¶ÇÁéáÂàÜÂ∏ÉpÔºå‰ΩÜÊòØÂèØ‰ª•Áî®Ê®°ÂûãmÊù•Ëøë‰ººp„ÄÇÔºàÂç≥Êàë‰ª¨ËôΩÁÑ∂‰∏çÁü•ÈÅìËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÁúüÂÆûÊÉÖÂÜµÔºå‰ΩÜÊòØÂèØ‰ª•Áî®NÂÖÉËØ≠Ê≥ïÊù•Ëøë‰ºº„ÄÇÔºâ<strong>‰∫§ÂèâÁÜµË∂äÂ∞èÔºåÊ®°ÂûãË∂äÊé•ËøëÁúüÂÆûÂàÜÂ∏É</strong>„ÄÇ</p>
<p><strong>Âõ∞ÊÉëÂ∫¶</strong>Ôºö<strong>Âõ∞ÊÉëÂ∫¶ÊòØÁÜµÁöÑÊåáÊï∞ÂΩ¢Âºè</strong>„ÄÇÊØîËæÉÁõ¥ËßÇ„ÄÇ</p>
<h3 id="excercies">Excercies
</h3><h5 id="31">3.1
</h5><p>Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 40.</p>
<h5 id="32">3.2
</h5><p>Calculate the probability of the sentence <code>i want chinese food</code>. Give two probabilities, one using Fig. 3.2 and the ‚Äòuseful probabilities‚Äô just below it on page 42, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(i|&lt;s&gt;) = 0.19$ and $P(&lt;/s&gt;|food) = 0.40$.</p>
<h5 id="33">3.3
</h5><p>Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why.</p>
<h5 id="34">3.4
</h5><p>We are given the following corpus, modified from the one in the chapter:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; Sam I am &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;
</span></span></code></pre></div><p>Using a bigram language model with add-one smoothing, what is $P(Sam | am)$? Include $&lt;s&gt;$ and $&lt;/s&gt;$ in your counts just like any other token.</p>
<h5 id="35">3.5
</h5><p>Suppose we didn‚Äôt use the end-symbol $&lt;/s&gt;$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $&lt;/s&gt;$:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; a b  
</span></span><span class="line"><span class="cl">&lt;s&gt; b b  
</span></span><span class="line"><span class="cl">&lt;s&gt; b a  
</span></span><span class="line"><span class="cl">&lt;s&gt; a a
</span></span></code></pre></div><p>Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet a,b is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet a,b is also 1.0.</p>
<h5 id="36">3.6
</h5><p>Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating $P(w3|w1,w2)$, where $w3$ is a word which follows the bigram$ (w1,w2)$, in terms of various n-gram counts and V. Use the notation $c(w1,w2,w3)$ to denote the number of times that trigram $(w1,w2,w3)$ occurs in the corpus, and so on for bigrams and unigrams.</p>
<h5 id="37">3.7
</h5><p>We are given the following corpus, modified from the one in the chapter:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; Sam I am &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I am Sam &lt;/s&gt;  
</span></span><span class="line"><span class="cl">&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;
</span></span></code></pre></div><p>If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $Œª‚ÇÅ = 1/2$ and $Œª‚ÇÇ = 1/2,$ what is $P(Sam|am)$? Include $&lt;s&gt;$ and $&lt;/s&gt;$ in your counts just like any other token.</p>
<h5 id="38">3.8
</h5><p>Write a program to compute unsmoothed unigrams and bigrams.</p>
<h5 id="39">3.9
</h5><p>Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?</p>
<h5 id="310">3.10
</h5><p>Add an option to your program to generate random sentences.</p>
<h5 id="311">3.11
</h5><p>Add an option to your program to compute the perplexity of a test set.</p>
<h5 id="312">3.12
</h5><p>You are given a training set of 100 numbers that consists of 91 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/llm/">LLM</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>All rights reserved.</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    


<aside class="related-content--wrapper">
    <h2 class="section-title">Áõ∏ÂÖ≥ÊñáÁ´†</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/2024/langchain-learning/">
        
        
            <div class="article-image">
                <img src="/2024/langchain-learning/cover.ddb755e13e74b4dc9c3215937020b858_hu_f1f250f84bcd151e.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post Â≠¶‰π†Á¨îËÆ∞ | LangChainÂ≠¶‰π†Á¨îËÆ∞"
                        data-key="langchain-learning" 
                        data-hash="md5-3bdV4T50tNycMhWTcCC4WA==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Â≠¶‰π†Á¨îËÆ∞ | LangChainÂ≠¶‰π†Á¨îËÆ∞</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/2025/rbdr/">
        
        

        <div class="article-details">
            <h2 class="article-title">AIDR | Reasoning-based Drug Repurposing</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/2025/aidr-network-method/">
        
        

        <div class="article-details">
            <h2 class="article-title">AIDR | Âü∫‰∫éÁΩëÁªúÁöÑÊñπÊ≥ï</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 ionfeather&#39;Log
    </section>
    
    <section class="powerby">
        ‰ΩøÁî® <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> ÊûÑÂª∫ <br />
        ‰∏ªÈ¢ò <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.29.0">Stack</a></b> Áî± <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> ËÆæËÆ°
    </section>
</footer>


<script>
    (function(u, c) {
      var d = document, t = 'script', o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function(e) { c(e); }); }
      s.parentNode.insertBefore(o, s);
    })('//cdn.bootcss.com/pangu/3.3.0/pangu.min.js', function() {
      pangu.spacingPage();
    });
</script>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>
<script src="https://npm.elemecdn.com/nprogress@0.2.0/nprogress.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://npm.elemecdn.com/nprogress@0.2.0/nprogress.css" crossorigin="anonymous" />
<script>
    NProgress.start();
    document.addEventListener("readystatechange", () => {
        if (document.readyState === "interactive") NProgress.inc(0.8);
        if (document.readyState === "complete") NProgress.done();
    });
</script>


    </body>
</html>
