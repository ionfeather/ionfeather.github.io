[{"content":"上了研究生学术没有做多少，兴趣爱好培养了不少——台球、博客、摄影、健身…可能还打算学个吉他和乒乓球。忙不过来，实在是忙不过来了。还是得多放点时间在学习上呀。\n今天是情人节，这么一想，晚上的健身房应该会比较空。好羡慕甜甜的爱情。\n2025-02-14\n","date":"2025-02-14T14:36:39+08:00","permalink":"https://ionfeather.github.io/p/rainwater/","title":"雨水 | 北京的雨水是艳阳高照"},{"content":"春节快乐！\n工科研究生的假期有点短暂了。我已经回校了。\n到学校了感觉自己好多东西需要购置。Apple Watch的表带现在明显太松了，但是官方太贵，第三方又有点儿硬，计划看一下Bilibili上的测评，进行一波购置。还有看上了影视飓风的一款帽子，上面可以贴上你的mbti，这对我这个enfp根本无法拒绝。\n又来到了北方，又遇到了高铁上一望无际的平原，感觉是另一种大海，在这片海里，有冰封的河流，有枯黄的树枝，还有炊烟和蜗居的人们。有一种说法是南方人向往雪，北方人向往海，这么一看，我应该是个不那么彻底的南方人。我两个都很向往。\n今天是元宵节，我本科的时候是灯谜社社长。元宵节，英语叫Lantern festival，也就是灯节，是灯谜社最重要的节日。在传统文化节里，我会张贴灯谜，擂起鼓，在鼓声滚滚中，同学需要猜出我出的谜题——有些是我们社团自己写的，所以很难猜，需要脑子有点儿回路，特别是与英雄联盟或者是与本科学校相关的那种谜题。\n虽然现在天气还很冷，但是感觉春意渐浓，即便树梢仍然枯枝，阳光灿烂的时候也能把寒意给驱散。\n我已经开始期待春天了。玉兰、银柳、山茶、樱花、樱桃花、木绣球、垂丝海棠。我要抄起我的相机，出门拍花拍鸟。\n不管怎么说，虽然已经上了几年（学校一年，外界一天）班了，现在还是春节期间，我一会儿说不定可以出门，去颐和园旁边拍花灯。\n","date":"2025-02-12T17:51:19+08:00","image":"https://ionfeather.github.io/p/lanternfestival/cover_hu10283662063726621985.jpg","permalink":"https://ionfeather.github.io/p/lanternfestival/","title":"元宵节 | 东风夜放花千树，更吹落、星如雨"},{"content":"之前一直在用Typora来写文章，发现有的时候也太难用了，不仅插件少，还要付费。这个时候看到很多的博客都用Obsidian来写，不得不心动了。\nObsidian的插件\rLinter\rObsidian Linter插件：打造统一、美观的笔记环境 - 知乎\n我不得不赞赏一下这个Linter，真的很好用，格式化目前的Markdown内容一直是我的心头痒，对于我这个强迫症来说，现在只需要按一下Ctrl+S就可以让我的敲击的内容都非常规范化，这实在是伟大的发明。\nExcalidraw\r这个插件还挺好看的，可以绘制手绘风格的图像，我绘制一些想法会更加方便。\n其他\rAdvanced Tables：对写表格比较有帮助。 Customizable Menu：自定义右键快捷键。 ","date":"2025-02-12T15:59:30+08:00","permalink":"https://ionfeather.github.io/p/obsidian/","title":"使用Obsidian来写博客"},{"content":"全书结构\r预备知识\r张量\r张量表示一个由数值组成的数组，这个数组可能有多个维度。\n具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。\n张量的创建\rimport torch x = torch.arange(12) x.shape x.numel() X = x.reshape(3, 4) torch.zeros((2, 3, 4)) torch.ones((2, 3, 4)) torch.randn(3, 4) torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) 运算符\r按元素运算\r常见的运算符这里用作按元素运算。\nx = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y # **运算符是求幂运算 可以得到\n(tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) 还有很多的一元运算符都可以用在按元素运算。\n线性代数运算\r求和/平均值\r直接调用sum函数，会将其变成一个标量，也可以指定axis = 1维度来指定轴来进行降维。\nA.sum() A.sum(axis = 1) A.sum(axis = [0, 1])# 对于矩阵来说，相当于A.sum() 同理，A.mean()也是一样的。\n如果希望能够在求和或者平均值的时候保持轴数不变，可以使用keepdims = True。\n如果希望能够沿着某个轴计算A元素的累计总和，可以使用cumsum函数。\nsum_A = A.sum(axis=1, keepdims=True) A.cumsum(axis=0) 点积\rx = torch.arange(4) y = torch.ones(4, dtype = torch.float32) torch.dot(x, y) 矩阵-向量积\r当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。\n矩阵-矩阵乘法\r我们可以将矩阵-矩阵乘法AB看作简单地执行m次矩阵-向量积，并将结果拼接在一起，形成一个n×m矩阵。\n在下面的代码中，我们在A和B上执行矩阵乘法。 这里的A是一个5行4列的矩阵，B是一个4行3列的矩阵。 两者相乘后，我们得到了一个5行3列的矩阵。\nB = torch.ones(4, 3) torch.mm(A, B) 张量连结\r在这里，dim=0说明是第一个维度进行拼接；dim=1说明是第二个维度进行拼接。\nX = torch.arange(12, dtype=torch.float32).reshape((3,4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) 广播机制\r特别需要注意这个，可能会导致错误发生。\na = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a, b 由于a和b分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的3×2矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。\n索引和切片\r与Dataframe中相似。\n节省内存\r如果直接使用X = X + Y就是重新创建一个元素。但是，有些时候希望执行原地操作。\n如果希望执行原地操作的话，可以使用两种方式，此时不会占用新的空间：\nX[:] = X + Y X += Y 转换为其他对象\r转换为Numpy非常容易：A = X.numpy()\n转换为Python标量：a.item()或者使用内置函数float(a)等。\n自动求导\r自动求导是计算一个函数在指定值上的导数。\n如何实现？ 计算图：将代码分解成操作子，将计算表示成一个无环图。 关于计算图，有显式构造 vs 隐式构造两种构造方式。\n特性 显式构造 隐式构造 计算图构建方式 显式定义 隐式定义 计算图类型 静态图 动态图 典型框架 TensorFlow 1.x, Theano PyTorch, TensorFlow 2.x (Eager) 有两种求导的方式，对于一个链式法则，我们可以采取正向累积和反向累积（也称反向传递）。\n**使用反向传递的时候，在我们计算y关于x的梯度之前，需要一个地方来存储梯度。**重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。\n","date":"2025-01-08T16:00:34+08:00","image":"https://ionfeather.github.io/p/d2l-01/pics/cover_hu3492737127834985036.png","permalink":"https://ionfeather.github.io/p/d2l-01/","title":"《动手学深度学习》"},{"content":"虚拟环境配置经历\r我之前配置好了一个虚拟环境名为vllm，专门用于vllm的启动，我还特意将其中的虚拟环境中的所有包的版本保存到vllm_requirements.txt文件中。\n但是我一顿操作之后，原本配置好的环境现在也没办法使用了。此时我庆幸自己想到用vllm_requirements.txt文件保存。但是在进行pip install -r vllm_requirements.txt的时候，出现了报错的情况，竟然说里面有一个包的版本是yanked version（撤回版本），无法下载，给我气晕了。\n吃一堑，长一智。配置好的环境就不要变了，应该另外复制一个环境，在复制的环境上进行修改。\n此外，我每次进行配置环境我都会忘记怎么配置和删除。是我最近记性变得太差了吗？总之我写一个文档，记不住就查一下。\n配置环境\r使用conda配置虚拟环境\r创建新的环境\r使用Terminal创建新的环境。\nconda create -n \u0026lt;new_env_name\u0026gt; python=3.10.0 激活虚拟环境\nconda activate \u0026lt;new_env_name\u0026gt; 安装包\nconda install \u0026lt;package\u0026gt; pip install \u0026lt;package\u0026gt; 从已有的文件中安装包/虚拟环境\r如果想要安装requirements.txt文件，就可以直接\npip install -r requirements.txt 如果想要安装的是environment.yml文件，应该改用conda来创建虚拟环境\nconda env create -f environment.yml 查看虚拟环境列表\nconda env list 复制原来已有的虚拟环境\r如果有一个环境已经配置好，我不希望破坏它，可以复制一个一模一样的环境，再在上面进行修改，这样就不会导致原来那个环境产生问题。\nconda create --name \u0026lt;new_env_name\u0026gt; --clone \u0026lt;old_env_name\u0026gt; 删除虚拟环境\r删除指定的虚拟环境\nconda activate base conda remove -n \u0026lt;env_name\u0026gt; --all 在conda中配置Jupyter内核\r安装Jupyter内核\r总是忘记Jupyter内核如何配置。记录一下：\n安装ipykernel。\nconda install ipykernel 将虚拟内核添加到jupyter内核中。\npython -m ipykernel install --user --name \u0026lt;your_env_name\u0026gt; 删除jupyter内核\r查看目前有的jupyter内核\njupyter kernelspec list 删除指定的jupyter内核\njupyter kernelspec remove \u0026lt;your_kernel_name\u0026gt; 照片\r照片是2024/12/7的时候同门团建的时候我拿大疆Pocket3拍的。拍的建筑是东郊民巷的圣弥厄尔大教堂。非常开心的一天。\n","date":"2024-12-15T20:24:19+08:00","image":"https://ionfeather.github.io/p/virtual-environment-config/cover_hu15097618714508979196.jpg","permalink":"https://ionfeather.github.io/p/virtual-environment-config/","title":"虚拟环境配置操作记录"},{"content":"为什么要学习LangChain\r我希望能够构建一个能阅读PDF论文的Agent，并且能够输出对论文优缺点的评价。\n导师\u0026nbsp;\u0026nbsp;\u0026nbsp;2024-10-12 14:30\r做一个论文阅读的大模型。 2024-10-12 14:45\u0026nbsp;\u0026nbsp;\u0026nbsp;我\r好的老师。 使用LangChain听说比较方便。\nLangChain是用来做什么的？\rLangChain是一个用于开发由LLM驱动的应用程序的框架。也就是说我们可以把LLM作为内核，LangChain作为外壳，搭建一个程序出来。\nLangChain提供了\n组件：处理LLM的组件的抽象； 定制链：把组件拼起来，实现一个特定用例。 对于阅读PDF，目前有两个想法：\n将PDF转为JSON，然后输入到LLM中； 构建RAG。使用LangChain能够比较方便地实现这个功能，听ZLB说这个也不是很难。我之前的畏难情绪可能太重了，现在写一个文档，激励和记录一下自己学习。 RAG是什么？\r虽然LLM非常强大，但它们对于它们未经训练的信息一无所知。如果您想使用LLM来回答它未经训练的文档相关问题，您需要向其提供这些文档的信息。最常用的方法是通过“检索增强生成”（ retrieval augmented generation，RAG ）。\n检索增强生成的思想是，在给定一个问题时，首先进行检索步骤以获取任何相关文档。然后将这些文档与原始问题一起传递给语言模型，并让它生成一个回答。然而，为了做到这一点，首先需要将文档以适合进行此类查询的格式呈现。\n构造一个语义搜索引擎\rBuild a semantic search engine | 🦜️🔗 LangChain\n读取PDF\rHow to load PDFs | 🦜️🔗 LangChain\n这里，文档中推荐使用了pypdf库。这里\n在实际应用中可以使用其他提取效果更好的库。LangChain支持的PDF格式很多，可以选择一下。\nDocument Loader Description Package/API PyPDF Uses pypdf to load and parse PDFs Package Unstructured Uses Unstructured\u0026rsquo;s open source library to load PDFs Package Amazon Textract Uses AWS API to load PDFs API MathPix Uses MathPix to load PDFs Package PDFPlumber Load PDF files using PDFPlumber Package PyPDFDirectry Load a directory with PDF files Package PyPDFium2 Load PDF files using PyPDFium2 Package PyMuPDF Load PDF files using PyMuPDF Package PDFMiner Load PDF files using PDFMiner Package 此外，导师之前还给我推荐了titipata/scipdf_parser库，能够更好地处理图像和扫描文本，并且运行在docker上，便于部署。\npypdf的介绍\rWelcome to pypdf — pypdf 5.1.0 documentation\nPyPDF 是一个用于处理 PDF 文件的 Python库。它提供了一组工具和功能，用于读取、解析和操作 PDF 文件的内容。\nSplitting\r原文\rFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \u0026ldquo;washed out\u0026rdquo; by surrounding text.\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nWe set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”.\nSee this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\n对于问题提问的文本来说，直接回答一整页肯定是太粗略了。我们最终的目标是检索回答输入查询的文档对象，进一步拆分 PDF 将有助于确保文档相关部分的含义不会被周围的文本“冲淡”。\n所以接下来应该用文本分割器来进行分割（Splitting）处理。这里用一个RecursiveCharacterTextSplitter进行分割。这里使用常见分隔符来对文档进行分割，适用于一般的文本。\n使用RecursiveCharacterTextSplitter无法读取图像或特定区域的文本。\nEmbeddings\r接下来将文本嵌入到向量中去，便于进行相似度指标来识别相关文本。\n这里LangChain支持数十种Embeddings方法。这里我选择了使用Hugging Face，可以选择将模型下载至本地或者使用Hugging Face Inference API来调用接口。这里可以直接使用HuggingFaceEmbeddings来进行处理。非常方便。\nfrom langchain_huggingface import HuggingFaceEmbeddings embeddings_model = HuggingFaceEmbeddings(model_name=\u0026#34;sentence-transformers/all-mpnet-base-v2\u0026#34;) embeddings = embeddings_model vector_1 = embeddings.embed_query(all_splits[0].page_content) vector_2 = embeddings.embed_query(all_splits[1].page_content) assert len(vector_1) == len(vector_2) print(f\u0026#34;Generated vectors of length {len(vector_1)}\\n\u0026#34;) print(vector_1[:10]) Vector Stores\rLangChain的Vector Stores对象包括了一些把文本和Document对象加入到Stores中的方法，然后通过相似性进行一个排列。\nfrom langchain_core.vectorstores import InMemoryVectorStore vector_store = InMemoryVectorStore(embeddings) ids = vector_store.add_documents(documents=all_splits) 此时就完成了存储和排列。\n这里向量存储一般来说是可以连接到现有的Vector Stores中的。\nUsage\r查询和这句话相似的句子 results = vector_store.similarity_search( \u0026#34;Diffusion is a image generation method.\u0026#34; ) ) print(results[0]) 异步查询（用于流程控制） results = await vector_store.asimilarity_search(\u0026#34;What is diffusion?\u0026#34;) print(results[0]) 返回分数 # Note that providers implement different scores; # the score here is a distance metric that varies inversely with similarity. results = vector_store.similarity_search_with_score(\u0026#34;What is Diffusion?\u0026#34;) doc, score = results[0] print(f\u0026#34;Score: {score}\\n\u0026#34;) print(doc) 通过和embedded query的相似度进行查询 embedding = embeddings.embed_query(\u0026#34;What is diffusion\u0026#34;) results = vector_store.similarity_search_by_vector(embedding) print(results[0]) Retrievers\r检索器（Retriever）可以从向量存储中进行构建，但是也可以和非向量形式进行交互。如果我们要构建一个能够检索文档的方法的话，我们可以创建一个runnable的检索器。\nfrom typing import List from langchain_core.documents import Document from langchain_core.runnables import chain @chain def retriever(query: str) -\u0026gt; List[Document]: return vector_store.similarity_search(query, k=1) retriever.batch( [ \u0026#34;What is diffusion?\u0026#34;, \u0026#34;What is forward process?\u0026#34;, ], ) 至此，我们构建了一个能够读多篇PDF文章的、能够对PDF文章进行查询的语义搜索引擎。\nChat Models和Prompt模板\r这里通过Vllm启动LLM，以Qwen2.5-7B-Instruct模型为例。\nfrom langchain_community.llms import VLLM llm = VLLM(model=\u0026#34;/home/ubuntu/jjq/Qwen/Qwen2.5-7B-Instruct/\u0026#34;, trust_remote_code=True, max_new_tokens=512, top_k=10, top_p=0.95, temperature=0.8, max_model_len = 30000, ) print(llm(\u0026#34;What is the capital of France ?\u0026#34;)) 接下来设计Prompt模板。\nfrom langchain import LLMChain from langchain.prompts import PromptTemplate from langchain.memory import ConversationBufferMemory from langchain.chains import ConversationalRetrievalChain from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate template = \u0026#39;\u0026#39;\u0026#39; 【任务描述】 请仔细阅读论文，回答用户给出的问题，尽量具有批判性。 【论文】 {{context}} ----------- {question} \u0026#39;\u0026#39;\u0026#39; # 检索器 retriever = db.as_retriever() # 记忆 memory = ConversationBufferMemory(memory_key=\u0026#34;chat_history\u0026#34;, return_messages=True) # 构建Agent qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory) qa({\u0026#34;question\u0026#34;: \u0026#34;能不能用中文给出论文的优势或者前景？\u0026#34;}) ","date":"2024-11-26T13:45:58+08:00","image":"https://ionfeather.github.io/p/langchain-learning/cover_hu13968487782357828171.png","permalink":"https://ionfeather.github.io/p/langchain-learning/","title":"LangChain学习笔记"},{"content":"梦\r这是一个英雄辈出的时代。所谓阴阳师，就是使用牌组与其他人对战来决定胜负的职业。在这里，国之阴阳师是一国中最强的阴阳师，中日韩三国每年都会选拔国之阴阳师，并且对战，决出最后的冠军。\n我和梦梦是青梅竹马，从小便展露了阴阳师的天赋。所谓“绕床弄青梅，郎骑竹马来”，我和梦梦那就是“绕床打牌组，郎打牌组来”。从小与其他人对战，胜利了之后可以选择是否获取一张新卡\n牌放进自己的卡组，最终打一个最强者之间的对战。这个对战从来都是我和梦梦之间的私人聊天与沟通时间。\n随着我们渐渐长大，我和梦梦之间也互生情愫。但是，认真打牌，赢得中日韩三国之间的对战，获得至高无上的荣耀是我们的最大目标。儿女情长，英雄气短，阴阳师需要克制。\n……\n梦梦要去日本打探消息了。她去那里，是为了我们中国能够更了解日本的特殊卡牌。可是，一个人在异国他乡，离开最亲近的人，是那么容易的事情吗？\n终于到了中日韩会赛的时间。\n我期待着到达了梦梦的住所，敲门，迎接我的果然是笑靥如花的梦梦。我们见到对方，思念已久的澎湃难抑制，但我们都克制住了自己，只是眼睛里互相诉说着彼此。\n但梦梦的房间里有股不详的气息。她好像被监视了。“你的房间里曾经有过一个男人躲在里面”，我说。\n梦梦害怕极了，但为了国家能够去刺探信息的人必然非常坚强，她脸色发白，不住地颤抖，但声音很小：“还在吗……这怎么办…”\n我安慰她：“这没什么，这是日本人监视你的手段，但梦梦你肯定也有没被看破的地方。”随后我离开了梦梦的家，脸色发青。\n……\n接下来就是我和八重岛神子最终对战了。近几年韩国式微，只剩中日交战。日本去年赢过了我国。去年对战使用的是30张左右的小牌组，打到后面基本就是6张左右一个循环，对方的强度比我们高。\n八重岛神子太强了，所有人都不相信我能打败他，包括我自己。我用尽心血，准备了一套120张卡牌的超大牌组进行对战。奇妙的是，八重岛神子也拿出来120张的卡组。这极其少见。我们来了一场古典的交锋。\n……\n突然，我想起之前在什么地方，在很久很久的从前，有个人和我说我会赢的，于是我便充满了信心，我的阴霾一扫而空。\n我尽量诚实地描述了我的梦境，可惜忘记了太多。我感觉里面有蛮多意象的。围棋中日韩会战、恋爱、谍战监视、杀戮尖塔烧牌循环、炉石传说对战\u0026hellip;很有意思的一个梦，醒来之后回味了很久。但是看起来可能没有那么有趣，或许我应该加一点戏剧性要素？\n照片\r照片是我2024/11/3的时候在地坛拍的。我想那一天是北京秋天最美的一天。北京的秋天是短暂的，11/3之前雾霾太重，看什么都朦胧；11/3那天的风很大，无数灿烂的叶子不断地落下来，在这一天之后，树梢上就稍微有一些秃了。\n","date":"2024-10-20T13:45:58+08:00","image":"https://ionfeather.github.io/p/dream/cover_hu13768060218741915990.jpg","permalink":"https://ionfeather.github.io/p/dream/","title":"一个有趣的梦 | 三国阴阳师"}]