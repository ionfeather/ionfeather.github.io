[{"content":"阅读综述\rAi-Driven Drug Repurposing: Uncovering Hidden Potentials OfEstablished Medications For Rare Disease Treatment Structure-based drug repurposing: Traditional and advanced AI/ML-aided methods Artificial intelligence for drug repurposing against infectious diseases | Artificial Intelligence Chemistry 2024 Artificial intelligence in COVID-19 drug repurposing | The Lancet Digital Health 可用的模型\r目前的再利用方法的实例主要集中在网络医学和大规模的患者数据分析上。这里有CoV-KGE、BenevolentAI、异质有向生物医学图等。\n例子\rArtificial intelligence in drug development | nature medicine\r这是一篇综述，讲解了人工智能在药物发现中的应用取得了显著进展，特别是在靶点识别、虚拟筛选、新药设计（de novo design）、ADMET预测和合成规划等方面。\n这里主要关注他讲的AI在药物重定位中的作用。\nA foundation model for clinician-centered drug repurposing | nature medicine\r摘要\r文章介绍了药物再利用人工智能模型在临床应用中的局限性，并提出了本研究的核心——TxGNN模型。\n解决的问题\r目前的模型过于依赖已有疗法和关于此疾病详细的分子机制，对于多疾病、零样本情境下的药物再利用没有好用的模型。\n挑战是什么\r现有计算方法假设待预测疾病已有可用药物，然而大量疾病（研究中 92% 的 17,080 种疾病）缺乏已知适应症，罕见病中约 95% 无 FDA 批准药物，85% 甚至无有潜力的治疗药物， 药物重利用的适应症可能与最初研究的适应症无关，现有机器学习模型对数据不完整、稀疏且无已知疗法的疾病，识别治疗候选药物的能力大幅下降。 核心方法\rTxGNN。图基础模型，在医学知识图谱上训练，利用图神经网络和度量学习模块，将药物再利用问题转化为零样本预测问题。\nTxGNN模型主要由两个模块组成：TxGNN Predictor（预测模块）和TxGNN Explainer（解释模块）。\nTxGNN Predictor（预测模块）： 功能：预测药物的适应症（indications）和禁忌症（contraindications）。 架构：该模块基于图神经网络（GNN）构建，优化了医学知识图谱（KG）中的关系。通过大规模、自监督的预训练，GNN为KG中的所有概念生成有意义的表示。然后，通过微调，该预训练模型被适应于处理治疗任务，并预测跨多种疾病的药物候选适应症和禁忌症。 度量学习模块：用于零样本预测，利用疾病可以共享疾病相关的遗传和基因组网络这一见解，从可治疗的疾病向无治疗方法的疾病转移知识。 TxGNN Explainer（解释模块）： 功能：解析KG，提取并简洁地表示相关医学知识，以生成多跳可解释的路径，这些路径解释了模型预测背后的逻辑。 架构：使用名为GraphMask的自我解释性方法，生成一个稀疏但充分的医学概念子图，这些概念被认为是TxGNN预测的关键。它为KG中的每条边生成一个0到1之间的重要性分数，并通过组合药物-疾病子图和边重要性分数，产生多跳可解释的路径，将疾病与预测药物联系起来。 优势\r在零样本条件下，TxGNN预测效果相比别的模型体现出了广泛的泛化性和准确性。\n数据集\r医学知识图谱公开，包含 10 种类型的节点和 29 种类型的无向边，共有 123,527 个节点和 8,063,026 条边，涵盖 17,080 种疾病（92% 缺乏 FDA 批准药物）和 7,957 种潜在药物重利用候选药物，还包含生物过程、分子功能、蛋白质、表型等多种生物医学概念信息。\n知识图谱下载地址：PrimeKG 演示界面：TxGNN Explorer Github仓库：mims-harvard/TxGNN In Silico Drug Repurposing for Anti-Inflammatory Therapy: Virtual Search for Dual Inhibitors of Caspase-1 and TNF-Alpha\rIn Silico Drug Repurposing for Anti-Inflammatory Therapy: Virtual Search for Dual Inhibitors of Caspase-1 and TNF-Alpha - PubMed\n摘要\r炎症与多种疾病相关，caspase-1 和 TNF-alpha 在炎症发展中起关键作用，针对二者的双重抑制剂有望治疗多种炎症性疾病。本文构建了基于定量构效关系和多层感知器神经网络的多条件模型（mtc-QSAR-MLP）用于虚拟筛选抗炎药物。经数据集处理、模型开发与评估，该模型准确性超 88%。利用此模型筛选出如 linagliptin、icariin 和 rolipram 等潜在的 caspase-1 和 TNF-alpha 双重抑制剂，为抗炎治疗提供新方向，有望用于自身免疫疾病、癌症及 COVID-19 等疾病的治疗。\n解决的问题\rcaspase-1 和 TNF-alpha是炎症发展中的关键蛋白，针对它们的双重抑制剂有望治疗多种炎症性疾病。\n挑战是什么\r通过药物再利用的方式来加速抗炎药物的发现。\n核心方法\r构建基于定量结构 - 活性关系（QSAR）和多层感知器神经网络（MLP）的多条件模型（mtc-QSAR-MLP），用于虚拟筛选 caspase-1 和 TNF-alpha 的双重抑制剂。\n数据处理与分子描述符计算：从 ChEMBL数据库获取 caspase-1 和 TNF-alpha 的化学及抑制数据，经筛选得到 1476 个分子案例。用MODESLAB v1.5软件计算多种分子描述符，包括拓扑指数等。还通过 Box-Jenkins 方法融合化学和生物信息，得到能反映分子结构与实验条件关系的\\(D[GTI]cj\\)描述符，为模型构建提供丰富数据支持。 mtc-QSAR-MLP 模型构建 数据集划分：将数据集按 3:1 随机分为训练集（1111 个分子案例，占 75.27%）和测试集（365 个分子案例，占 24.73%） 。 描述符筛选：在训练集中，用信息增益比（IGR）对\\(D[GTI]cj\\)描述符排序，通过两两 Pearson 相关系数（PCC）筛选出相关性在\\(-0.6 \u003c PCC \u003c 0.6\\)的描述符，减少信息冗余。 模型训练与选择：以 MLP 神经网络为基础构建模型，利用 STATISTICA v13.5.0.17 软件分析不同 MLP 网络，依据敏感性（Sn (%)）、特异性（Sp (%)）、准确性（Acc (%)）和马修斯相关系数（MCC）等指标，选择性能最佳的 MLP 网络作为 mtc-QSAR-MLP 模型。 模型评估：采用 Bounding Box 方法确定模型适用性域，计算描述符的局部适用性域分数（\\(LSAD_D [GTI] cj\\)）和总分数（TSAD），判断分子是否在适用性域内。通过分析模型在训练集和测试集上的性能指标，如准确率、敏感性、特异性和 MCC 等，验证模型对不同实验条件下化学物质抑制活性的分类和预测能力。 虚拟筛选与结果验证：使用构建好的 mtc-QSAR-MLP 模型对 8922 种机构监管化学品数据库进行虚拟筛选，考虑 8 种实验条件，计算每个分子的 FA (%)（分子在 8 种实验条件下被预测为活性的频率）和 S (TSAD)（考虑 8 种实验条件下分子的总适用性域分数）指标，筛选潜在双重抑制剂。对筛选结果，通过在科学文献中搜索相关信息，结合实验证据验证其抑制 caspase-1 和 TNF-alpha 的可能性。 优势\r整合多源数据：基于扰动理论和机器学习构建模型，能够整合不同类型的化学和生物数据。通过 Box-Jenkins 方法对传统分子描述符进行处理，得到融合化学结构与实验条件信息的描述符，使模型能综合考虑多种因素。 多靶点多终点预测：可同时预测多个生物终点（如活性、毒性、药代动力学性质），并针对多个不同靶点（如 caspase-1 和 TNF-alpha）进行预测。 高预测准确性：模型在训练集和测试集上都表现出较高的准确性。训练集准确率达 92.44%，测试集准确率为 88.49%，马修斯相关系数（MCC）在训练集和测试集分别为 0.844 和 0.764。 提供理化和结构解释：通过对分子描述符的分析，能够从物理化学和结构层面解释模型的预测结果。 数据集\rChEMBL开放数据库。\nMachine learning–enabled virtual screening indicates the anti-tuberculosis activity of aldoxorubicin and quarfloxin with verification by molecular docking, molecular dynamics simulations, and biological evaluations | Briefings Bioinf\r核心方法\r研究者首先从ChEMBL数据库中收集了大量已知的抗结核药物的生物活性数据，用于训练和验证多种机器学习和深度学习模型。这些模型包括XGBoost和图神经网络（GNNs），用于预测化合物的最小抑制浓度（MIC）。接着，研究者使用这些模型对DrugBank数据库中的11,576种化合物进行虚拟筛选，筛选出具有潜在抗结核活性的化合物。通过多步过滤和实验验证，最终确定了两种具有显著抗结核活性的化合物：aldoxorubicin和quarfloxi，为了进一步验证作用机制，研究者通过分子对接、分子动力学模拟和表面等离子共振实验，确认了它们与结核分枝杆菌DNA gyrase的直接结合。\n","date":"2025-04-04T01:42:58+08:00","permalink":"https://ionfeather.github.io/2025/moleculerepurposing-2/","title":"Molecule Repurposing-2"},{"content":"数据库\rTTD\rTTD 治疗靶点数据库。它是一个专门收集和整理与治疗靶点相关信息的数据库。 下载地址：Full Data Download | Therapeutic Target Database 数据库比较大，这里筛选了药物-靶点-疾病关联数据： Target to drug mapping with mode of action TargetID：靶点的唯一标识符。可以在数据库中找到靶点的详细信息。 DrugID：药物的唯一标识符。可以获取该药物的相关信息。 Highest_status：表示药物在研发或临床应用中的最高阶段或状态，Approved说明该药物已经通过了相关监管机构的审批，被批准用于临床治疗。 MOA：即Mode of Action，作用模式的缩写，Modulator表明该药物对靶点的作用方式是作为调节剂。 Drug to disease mapping with ICD identifiers TTDDRUID：表示 TTD 药物 ID，即该药物的唯一标识。 DRUGNAME：药物名称。 INDICATI：适应症，即药物被用于治疗的病症。 Disease entry：疾病条目，这里使用了 ICD-11 编码来表示具体的疾病。 Clinical status：临床状态，指药物在临床试验或临床应用中的阶段或情况，如已批准、处于某一阶段的临床试验等。 Target to disease mapping with ICD identifiers TARGETID：代表 TTD 靶点 ID，为靶点在数据库内的唯一识别编号。 TARGNAME：靶点的具体名称。 INDICATI：适应症，即药物被用于治疗的病症。 Disease entry：疾病条目，这里使用了 ICD-11 编码来表示具体的疾病。 Clinical status：临床状态。\n表一：Target to drug mapping with mode of action\rTargetID DrugID Highest_status MOA T87024 D00RRU Approved Modulator 这个表格表示：ID为D00RRU的药物能够以调节剂的方式作用ID为T87024的疾病，并且已经被批准。\n表二：Drug to disease mapping with ICD identifiers\rTTDDRUID DRUGNAME INDICATI ICD-11 Clinical status DZB84T Maralixibat Pruritus EC90 Approved DZB84T Maralixibat Progressive familial intrahepatic cholestasis 5C58.03 Phase 3 DZB84T Maralixibat Alagille syndrome LB20.0Y Phase 2 这个表格表示ID为DZB84T的药物，名为Maralixibat，有对应几种适应症：\n适应症为Pruritus（瘙痒症），ICD-11 编码是EC90，临床状态为Approved（已批准）。 适应症为Progressive familial intrahepatic cholestasis（进行性家族性肝内胆汁淤积症），ICD-11 编码是5C58.03，临床状态为Phase 3（三期临床试验）。 适应症为Alagille syndrome（阿拉基综合征），ICD-11 编码是LB20.0Y，临床状态为Phase 2（二期临床试验）。 表三：Target to disease mapping with ICD identifiers\rTARGETID TARGNAME INDICATI INDICATI INDICATI T00033 Transforming growth factor alpha (TGFA) Phase 1/2 Chronic kidney disease [ICD-11: GB61] 这个表格表示 ID 为 T00033 的靶点，名为 Transforming growth factor alpha (TGFA)，有对应的临床关联信息：关联的疾病适应症为Chronic kidney disease（慢性肾脏病），ICD-11 编码是GB61，临床状态为Phase 1/2（一期 / 二期临床试验 ）。\nPubChem\rPubChem PubChem是美国国立卫生研究院的一个开放化学数据库。化合物信息数据库，收录大量化合物的结构、生物活性等信息，为科研、药物研发等提供数据支持，也用于化学知识普及和教学。这个数据库非常大。 用途参考：Nucleic Acids Research | 疗效药物靶标的比较性研究与数据平台构建。 FTP下载：Index of /pubchem 药物-疾病关联数据 ./Bioassay：生物测定数据，包含大量药物对不同生物靶点或细胞系的活性测试结果 ./Target：靶标数据，涵盖蛋白质、基因、通路和分类学等信息。 ./Commpound和./Compound_3D：化合物的数据信息包含结构信息。 ./Other：GooglePatents和IBM中包含专利信息。寻找专利即将过期/已过期的药物，并对这些药物进行再利用评估。 下面以./Bioassay数据为例。\n表 1：PubChem的./Bioassay的CSV文件的标题行的分类和标签说明\r分类 标签名称 说明 数据行 PUBCHEM_RESULT_TAG 行ID 数据行 PUBCHEM_SID PubChem SID 数据行 PUBCHEM_CID PubChem CID 数据行 PUBCHEM_ACTIVITY_OUTCOME PubChem活性结果（即，Inactive, Active, Inconclusive, Unspecified, or Probe） 数据行 PUBCHEM_ACTIVITY_SCORE PubChem活性得分，值越高表示活性越强 数据行 PUBCHEM_ACTIVITY_URL 测试结果特定的url 数据行 PUBCHEM_ASSAYDATA_COMMENT 测试结果特定的注释 数据行（可选） 测试结果名称1（如：name of test result 1） 测试结果1的数据 数据行（可选） 测试结果名称2（如：name of test result 2） 测试结果2的数据 可选标题行（测试结果） RESULT_UNIT 单位（e.g. MICROMOLAR, NANOMOLAR和其他PubChem上传系统使用的标签） 可选标题行（测试结果） RESULT_IS_ACTIVE_CONCENTRATION 如果测试结果表示有效浓度则为TRUE 可选标题行（测试结果） RESULT_IS_ACTIVE_CONCENTRATION_QUALIFIER 如果测试结果表示与有效浓度相关的终点限定词（e.g. \u0026lt;, \u0026lt;=, =, \u0026gt;, \u0026gt;=）则为TRUE 可选标题行（测试结果） RESULT_ATTR_CONC_MICROMOL 以微摩尔为单位的测试浓度 表二：PubChem CSV/Data/0000001_0001000/1.csv文件\rPUBCHEM_RESULT_TAG PUBCHEM_SID PUBCHEM_CID PUBCHEM_EXT_DATASOURCE_SMILES PUBCHEM_ACTIVITY_OUTCOME PUBCHEM_ACTIVITY_SCORE PUBCHEM_ACTIVITY_URL PUBCHEM_ASSAYDATA_COMMENT LogGI50_M LogGI50_u LogGI50_V IndnGI50 StddevGI50 LogTGI_M LogTGI_u LogTGI_V IndnTGI StddevTGI 1 66954 11122 CC1=CC(=O)C=CC1=O Inactive 10 http://dtp.nci.nih.gov/dtpstandard/servlet/doseresponse?searchtype=NSC\u0026searchlist=1\u0026systemname=NCI+Cancer\u0026idn1=1\u0026idn2=1 -4.5753 1 0 -4 1 0 这个表格表示数据行 ID 为 1 的记录，其中涉及的 PubChem SID 为 66954，PubChem CID 为 11122，化合物的 SMILES 表示形式为CC1=CC(=O)C=CC1=O，有对应的生物活性关联信息：\nPubChem 活性结果为Inactive（无活性），PubChem 活性得分为 10，表明在此次生物测定中该化合物活性较低。 测试结果特定的 url 为dtp.cancer.gov/services/nci60data/colordoseresponse/pdf/1，可通过该链接获取更多相关测试结果信息。 测试结果特定的注释为空。 关于 GI50（半数生长抑制浓度）的相关数据： 以摩尔为单位的 GI50 结果的对数LogGI50_M为 - 4.5753。 测试次数平均数量IndnGI50为 1。 对所有测试的 GI50 结果的对数（Log10）的标准偏差StddevGI50为 0，说明该测量值较为稳定。 关于 TGI（总生长抑制）的相关数据： 以摩尔为单位的 TGI 结果的对数LogTGI_M为 - 4。 测试次数平均数量IndnTGI为 1。 对所有测试的 TGI 结果的对数（Log10）的标准偏差StddevTGI为 0，表明该测量值稳定性较好 。 其他的如./Target、./Commpound和./Compound_3D等数据也呈现如此结构。\nDGIdb\rDGIdb 药物基因相互作用数据库，主要收集和整理药物与基因之间相互作用的相关信息。 这个数据库一共就四个表格，分别存储药物-基因相互作用、基因、药物和分类相关信息，使用tsv格式进行存储。数据大小约为24.9MB。 interactions.tsv：存储所有药物-基因相互作用声明数据，包含不同药物与基因之间相互作用的信息。 genes.tsv：记录了基因声明相关数据，可能涵盖基因的基本信息，如基因名称、基因 ID、基因功能注释等内容。 drugs.tsv：存放药物声明数据，包括药物的名称、药物 ID、药物的基本属性、药理作用等信息。 categories.tsv：用于存储与数据分类相关的信息。\n表一：interactions.tsv\r其中的各列的意思分别是\ngene_claim_name：基因的声明名称。 gene_concept_id：基因的唯一标识代码。 gene_name：基因的标准正式名。 interaction_source_db_name：基因与药物相互作用数据的来源数据库名。 interaction_source_db_version：来源数据库的版本。 interaction_type：基因和药物间相互作用的类别。 interaction_score：体现基因与药物相互作用强度的数值。 drug_claim_name：药物的特定称谓。 drug_concept_id：药物的唯一标识代码。 drug_name：药物的标准正式名。 approved：药物是否已获批上市的标识。 immunotherapy：药物是否属于免疫治疗药物的标识。 anti_neoplastic：药物是否具有抗肿瘤作用的标识。 gene_claim_name gene_concept_id gene_name interaction_source_db_name interaction_source_db_version interaction_type interaction_score drug_claim_name drug_concept_id drug_name approved immunotherapy anti_neoplastic CYP2D6 hgnc:2625 CYP2D6 DTC 9/2/20 NULL 0.017709164 RACLOPRIDE ncit:C152139 RACLOPRIDE FALSE FALSE FALSE PPARG hgnc:9236 PPARG DTC 9/2/20 NULL 0.84012274 KALOPANAX-SAPONIN F chembl:CHEMBL1833984 CHEMBL:CHEMBL1833984 FALSE FALSE FALSE 这个表格有两列，这里只对第一列进行解释：基因声明名称为CYP2D6，基因概念 ID 是hgnc:2625，基因名称同样为CYP2D6。相互作用来源数据库名称是DTC，数据库版本为9/2/20，相互作用类型为空（NULL），相互作用得分为0.017709164。药物声明名称是RACLOPRIDE，药物概念 ID 为ncit:C152139，药物名称是RACLOPRIDE。该药物未被批准（approved为FALSE），不是免疫疗法（immunotherapy为FALSE），也不是抗肿瘤药物（anti_neoplastic为FALSE） 。\n表二：drugs.tsv\r其中的各列的意思分别是\ndrug_claim_name：药物的特定称谓。 nomenclature：药物命名法类型。 concept_id：药物的唯一标识代码。 drug_name：药物的标准正式名。 approved：药物是否已获批上市的标识。 immunotherapy：药物是否属于免疫治疗药物的标识。 anti_neoplastic：药物是否具有抗肿瘤作用的标识。 source_db_name：药物数据的来源数据库名。 source_db_version：来源数据库的版本。 drug_claim_name nomenclature concept_id drug_name approved immunotherapy anti_neoplastic source_db_name source_db_version BRAF(V600E) Kinase Inhibitor RO5212054 Primary Drug Name ncit:C92591 BRAF(V600E) KINASE INHIBITOR RO5212054 FALSE FALSE TRUE NCIt 24.02d 药物声明名称为BRAF(V600E) Kinase Inhibitor RO5212054，药物命名法类型是Primary Drug Name，药物概念 ID 是ncit:C92591，药物名称为BRAF(V600E) KINASE INHIBITOR RO5212054。该药物未被批准（approved为FALSE），不是免疫疗法（immunotherapy为FALSE），是抗肿瘤药物（anti_neoplastic为TRUE） 。药物数据的来源数据库名称是NCIt，数据库版本为24.02d。\n表三：gene.tsv\r其中的各列的意思分别是\ngene_claim_name：基因的声明名称。 nomenclature：基因命名法类型。 concept_id：基因的唯一标识代码。 gene_name：基因的标准正式名。 source_db_name：基因数据的来源数据库名。 source_db_version：来源数据库的版本。 gene_claim_name nomenclature concept_id gene_name source_db_name source_db_version NGFIBA NCBI Gene Name NULL NULL BaderLab Feb-14 基因声明名称为NGFIBA，基因命名法类型是NCBI Gene Name，基因概念 ID 是NULL，基因名称同样为NULL。基因数据的来源数据库名称是BaderLab，数据库版本为Feb-14。\n表四：categories.tsv\r其中的各列的意思分别是\nname：某实体的名称（这里可能是基因或其他生物相关实体名称）。 name-2：该实体的另一个相关名称或描述。 source_db_name：数据的来源数据库名。 source_db_version：来源数据库的版本。 name name-2 source_db_name source_db_version PXR NUCLEAR HORMONE RECEPTOR BaderLab Feb-14 名称为PXR，另一个相关名称或描述是NUCLEAR HORMONE RECEPTOR。数据的来源数据库名称是BaderLab，数据库版本为Feb-14。\nDrugBank\rDrugBank 综合药物信息数据库，整合了药物的化学、药理、毒理等多方面信息。 一个巨大的数据库。这个数据库免费开放给学术用户，但商业使用收费。可以在网页上进行学生/老师认证注册。药物再利用在其中是很小的一个板块，可以在这里下载。 学生身份目前正在审核状态，无法下载。 ChEMBL\rChEMBL ChEMBL 是一个开源的生物活性分子数据库，专注于小分子化合物与生物靶点之间的相互作用信息。 数据大小大约2GB。有不同的版本，提供FTP方式下载，可以在这里下载最新的版本ChEMBL_35。 目录中的内容可以参考README文件。主要的部分可以见下图，组织时FASTA文件、SDF文件、HTML文件和TXT文件都有使用。数据可以加载到MySQL、PostgreSQL等数据库中。 有官方的数据库结构图如下，主要分成 化合物信息：主要以蓝色区域呈现，记录了化合物的结构、理化性质等，像分子式、分子量和二维结构这些 实验数据：用紫色区域表示，包含化合物与靶点相互作用的活性数据，还有实验条件，比如不同化合物对特定靶点的结合亲和力数值，以及实验采用的检测方法等 靶点和结合位点信息：红色区域代表这部分内容。靶点信息涉及蛋白质靶点的氨基酸序列、三维结构和功能分类；结合位点信息聚焦靶点与化合物结合的区域，包括氨基酸组成、空间结构和相互作用模式 药物代谢数据：浅绿色区域涵盖这部分，记录了药物在体内的代谢途径、产物和酶——我们可以通过这部分预测药物疗效、毒性以及药物之间的相互作用。 作用机制 / 药物注释：浅蓝色区域负责注释药物作用机制，关联化合物、靶点和生物效应。 来源和已批准药物数据：灰色区域中，来源信息保证数据可追溯，已批准药物 数据包含批准文号、适应症和生产厂家等，这个应该是用来避免专利壁垒的，可以查看专利即将到期的药物。 常规信息：浅黄色区域存放数据库的版本号、更新时间等基础信息。\nBindingDB\rBindingDB 主要收集药物靶点蛋白质和类药小分子之间的相互作用亲和力数据。数据库大小为484.07 MB（TSV版本）。 该数据文件只有一个表格，该表总共包含117列。 配体标识与结构（7 列）：如 “BindingDB MonomerID” 等，用于确定和描述配体，包括多种结构表示方式和自定义名称。 靶点基础信息（2 列）：“Target Name” 和 “Target Source Organism According to Curator or DataSource”，明确靶点名称及所属生物。 结合活性数据（6 列）：“Ki (nM)” 等，反映配体与靶点结合的强度和速率。 实验环境参数（2 列）：“pH” 和 “Temp (°C)”，体现结合数据测量时的环境条件。 数据与文献来源（7 列）：“Curation/DataSource” 及各类文献标识符，用于追溯数据出处和原始研究。 数据库交叉引用（14 列）：含 “PubChem CID” 等多个数据库的配体标识，方便数据整合查询。 蛋白质链详情（80 列）：先以 “Number of Protein Chains in Target” 记录链数量，后续多列针对每条链，涵盖序列、结构 ID、UniProt 相关名称和 ID 等信息。 表格中的信息以第一条记录（数据行 ID 为 1）为例，大概描述了：\n配体相关信息：BindingDB MonomerID 为 608734，Ligand SMILES 为 “O [C@@H] 1C@@HC@@HN (CCCCCC (O)=O) C (=O) N (CCCCCC (O)=O)[C@@H] 1Cc1ccccc1”，其对应的 BindingDB Ligand Name 是 “6-[(4R,5S,6S,7R)-4,7 - 二苄基 - 3-(5 - 羧基戊基)-5,6 - 二羟基 - 2 - 氧代 - 1,3 - 二氮杂环庚烷 - 1 - 基] 己酸::DMPC 环脲 1”。 靶点相关信息：Target Name 为 “Dimer of Gag-Pol polyprotein [501 - 599]”，表明靶点是 Gag-Pol 多聚蛋白二聚体的 501 - 599 区域；Target Source Organism 为 “Human immunodeficiency virus 1”，即靶点来源于人类免疫缺陷病毒 1。 结合活性数据：Ki (nM) 为 0.24 ，显示配体与靶点的结合亲和力较强。 实验条件：测量时 pH 为 5.5，温度为 37°C。 数据来源：由 BindingDB 从文献整理而来，相关文献的 DOI 是 10.1021/jm9602571，PMID 为 8784449，可据此追溯原始研究。 数据库交叉引用：PubChem CID 为 3009304，PubChem SID 为 483500124，方便在 PubChem 数据库中查找该配体更多信息。 蛋白质链信息：靶点含 1 条蛋白质链，其序列为 “PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF”，相关 PDB ID 有 “1W5Y”“1W5X” 等多个，UniProt（SwissProt）相关信息显示，其推荐名称是 “Gag-Pol polyprotein”，Entry Name 为 “POL_HV1BR”，Primary ID 为 “P03367”。 ","date":"2025-03-23T17:15:44+08:00","permalink":"https://ionfeather.github.io/2025/moleculerepurposing/","title":"论文阅读 | Molecule Repurposing"},{"content":"周四晴朗，趁着早上刚开完会的懈怠，去北海公园探春。春色正好，看到了索尼、尼康、富士、佳能相机不下十余只对着阐福寺的牌匾和桃花「咔咔咔」地按下快门。\n北海公园的桃花和樱花不多，但河柳正是时候，被气温诈骗的柳树抽出了新芽，此时正是杨柳最美的时候，嫩嫩茸茸，纤巧细美，切切可人。柳丝被春风吹起，在红墙白塔的照映下，格外嫩绿。暖风熏得我沉醉其中。\n公园里的女孩格外得多。大清还没亡，格格、皇后的数量可以再住满紫禁城，手机相机Pocket3可以堆满武库。我揣着相机漫步其间，遇见了在北京工作的朋友二人、从山东来旅游的女生和姐妹俩、反扣着帽子的酷女孩、爱好摄影的Tony老师——形形色色，但都友好温柔。\n我端起相机，拍得或好或差，「咔」——留下了她们生命的一个瞬间，也同时在相机上留下了我的剪影。\n絮絮叨叨\r真的有点沉迷于认识新的朋友了——虽然里面有些人可能不会再相见，可能不会再联系，但是在认识、沟通的那个瞬间，真的很愉快。\n2025-03-23 14:00\n去汉光百货买了三件衣服，竟然如此昂贵！花了我一千多大洋，顿时一阵肉痛。不过妈妈说如果觉得好看，如果能穿比较久的话，也不亏。这么一想确实还可以。\n2025-03-29 23:46\n玉渊潭的春天真的好美。天空蓝得清澈，白云懒洋洋地漂浮在上面——冬天是绝对见不到这样的景色的，受到西伯利亚高压控制的北京在冬天干燥而又晴朗。\n柳条刚刚抽出，还透着稚嫩，樱花却已经开得烂漫了。玉渊潭里的鸟儿不怕人，在我们面前随意踱步、游泳、起飞。远处的白桥和中央电视塔在这样清澈湛蓝的湖水的衬托下熠熠生光。公园里的coser很多，看到了芙莉莲和勇者，看到了好多粉色的头发的女生——有些我已经不认识了，我感觉我也没多大呀，但好像已经是个上古时代的二次元了。我逐渐迈入现充了（）\n我也拿起我的相机，我们同门四人的一张张照片也在一声声快门声中被记录。在今天的玉渊潭的景色的衬托下，拍什么都明媚可人、青春洋溢。世界仿佛被蒙上了一层名为鲜艳青春的蒙版或者滤镜，湖蓝、天清、云白、树绿。\n2025-03-30 22:00\n","date":"2025-03-22T14:39:56+08:00","image":"https://ionfeather.github.io/2025/the-spring-equinox/cover_hu7395602889184453154.jpg","permalink":"https://ionfeather.github.io/2025/the-spring-equinox/","title":"春分 | 拍堤春水四垂天，红墙白塔，小花杨柳"},{"content":"盼望着，盼望着，春天的脚步近了。飞快，羽绒服已经太热，食堂门口的桃花准备抽枝，路过图书馆时望见乌鸦「哇——哇——」地向北飞。\n已经摸上我的相机，快乐地订好了出游计划。\n万物复苏，只期待一场春日游。春日游。杏花吹满头。陌上谁家年少，足风流——说不定是我。\n北京的天气也是神奇起来了。今天终于补上了一个冬天的遗憾，在春暖花开之前，冬天好像也知道自己的谢幕。在临别之际，给出了自己最后的一个吻，撒下了雪花。\n2025-03-15 15:34\n听到了今年第一声雷响！记录一下时间点。\n惊蛰到了。\n2025-03-15 22:33\n参与了黄老师的季会。我在里面是下午发言。\n季会人太多，每个人发言20分钟，竟然需要一天的时间，从早上8:50开始，到下午16:00结束。\n导师真的好厉害，英语水平好高，演讲的时候神采飞扬，而且英语句子地道流利。在台下给我看傻了。 我的演讲好像也不错，嘿嘿。同门给我录了一小段，我发现我的状态特别好——发型、手势和声音都不错。最近看自己是原来越顺眼了。 2025-03-16 17:00\n春天肆意地驱散冬天的肃杀氛围。首先被春天派遣的先行部队是山桃花、迎春花和玉兰花三位大将。请看：\n花儿不知道这是早春，用尽全身力气绽放。\n2025-03-20\n","date":"2025-03-05T14:37:00+08:00","image":"https://ionfeather.github.io/2025/the-waking-of-insects/cover_hu12324456251382613032.jpg","permalink":"https://ionfeather.github.io/2025/the-waking-of-insects/","title":"惊蛰 | 漫长的冬天终于到达了尾声"},{"content":"开展一些AI-assist Drug Design 的方向探索，需要进行一些论文阅读和调研，形成调研报告。\n参考：\n[2402.08703] A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation Hao Zhou（周浩) 目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 思维导图：\nA Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation\r[2402.08703] A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation\n摘要\r将从头药物设计（de novo drug design）归纳为两大主题：小分子生成和蛋白质生成。在每个主题下，我们识别出多种子任务和应用，重点介绍重要的数据集、基准测试、模型架构，并比较顶尖模型的性能。\n引言\r并非是虚拟筛选/定向进化。而是从头开始的自然界中并未存在的新的生物实体的生成。 文章中分成两个部分来讲解。小分子和蛋白质。 文章将介绍 生成式模型的种类：Diffusion/VAE/Flow-Based/GAN 将文章分成小分子和蛋白质两个领域，分别介绍 一般背景/任务定义 用于训练和测试的常见数据集 常用的评估指标 对过去和当前的机器学习方法的概述 对SOTA方法的性能的对比分析 总结 相关研究\r其他的方法都太专业，而这篇文章对小分子和蛋白质生成进行宏观层面的分析，有助于那些想要对化学创新领域中新兴的生成式AI模型有一个高屋建瓴的了解的人。\n前言：生成式AI模型\r介绍了VAE、GAN、Flow-Based Models和Diffusion，还介绍了一些其他模型，比如GNN、EGNN等。\n应用\r小分子\r任务背景\r分子生成聚焦于为药物设计创造新的分子化合物。这些生成的分子旨在具有 （1）有效性、（2）稳定性和（3）独特性，总体目标是具有药物适用性。“药物适用性” 是一个宽泛的术语，用于描述分子对各种生物靶标的结合亲和力。\n虽然前三个任务可能看起来微不足道，但仅仅生成有效和稳定的分子就存在各种挑战。因此，无靶向分子生成领域专注于生成有效的分子集合，而不考虑任何生物靶标。靶向分子生成（或配体生成）侧重于针对特定蛋白质结构生成分子，因此更关注药物成分。最后，3D 构象生成涉及在给定 2D 连接的情况下生成各种 3D 构象。\n无靶向分子设计\r必须满足前两个特点，也就是有效性和稳定性。需要满足很多的复杂的条件，所以其实还是挺难的。深度学习可以帮助人们更有效率地生成有更高可能性满足有效性的分子。\n任务：无输入，输出为生成一组新的、有效的、稳定的分子。\n数据集：QM9和GEOM-Drug\n指标：\n分子生成任务指标：原子稳定性、分子稳定性、有效性、独特性、（新颖性）、药物相似性度量估计值QED。 模型的评估方式：通过在QM9数据集上的一部分训练属性分类网络，然后对模型生成的分子进行评估，计算目标和评估属性值之间的平均绝对误差。 分子在特定化学性质方面的指标：极化率$α$、最高占据分子轨道能量 $\\varepsilon_{HOMO}$、最低未占据分子轨道能量 $\\varepsilon_{LUMO}$、$\\varepsilon_{HOMO}$ 和 $\\varepsilon_{LUMO}$ 的差值$\\Delta_\\varepsilon$、偶极矩 $\\mu$、298.15K 下的摩尔热容 $C_v$。 模型：\n过去几年间，分子生成任务的方法从一维的SMILES转变成二维的连接图，然后是三维的几何结构，最后到融合二维和三维信息的方法。\n1D SMILES字符串模型 早期方法：如CVAE、GVAE直接处理，但是SMILES因为是一维的，存在问题：两个化学结构相似的分子图可能会得到非常不同的 SMILES 字符串，这使得模型更难学习到这些相似性和模式。 2D 图生成模型 JTVAE：首个直接生成 2D 分子图的模型，通过树状骨架迭代扩展并验证结构有效性。 3D 结构模型 早期方法：Flow-Based方法ENF和自回归方法G-SchNet。 EDM：基于Diffusion的 3D 点云模型，利用 E (3) 等变性提升性能，避免原子排序依赖。 GCDM：结合几何深度学习与Diffusion，引入注意力机制优化消息传递。 联合2D和3D JODO：联合2D和3D的扩散模型，使用几何图形表示来捕获 3D 空间信息和连接信息，对这种联合表示应用分数随机微分方程，同时提出扩散图变换器来参数化数据预测模型，避免在每个独立通道独立添加噪声后相关性的丢失。 MiDi：应用了DDPM，提出了「松弛」的图神经网络（EGNN）。 这里给出了三个表格。\n表格1：生成模型在QM9数据集上的条件无关的分子设计任务上的性能表现。Diffusion方法比之前的方法好很多，但是在GEOM-Drugs上可能表现不佳。 表格2：EDM、MDM、MiDi 等模型在GEOM-Drugs数据集上的条件无关的分子设计任务上性能表现。MiDi能生成更稳定的复杂分子，但是有效性较低。 表格3：生成模型在条件分子生成任务上的性能表现。MDM、GCDM生成表现不错，MDM前四项较好，GCDM后两项较好。 靶向分子设计\r有两种，一种是基于配体的药物设计（LBDD），另一种是基于结构的药物设计（SBDD）。LBDD利用目标蛋白质的氨基酸序列，借助已知的配体特征来构建；SBDD利用目标蛋白质的三维结构来设计。\n任务：给定氨基酸序列/蛋白质的三维结构，生成对应的有高结合亲和力以及潜在相互作用的分子。\n数据集：CrossDocked2020、ZINC20和Binding MOAD。\n指标：Vina Score、Vina Energy、高亲和力百分比High Affinity Percentage、合成可及性分数SAscore和多样性Diversity。\n模型：\nLBDD 结合了Transformer结构，例如DrugGPT，训练的时候输入为SMILES和蛋白质氨基酸序列，从而训练输出可行的SMILES配体。 SBDD LiGAN：三维目标感知分子输出的概念，将分子适配到网格格式，以便利用卷积神经网络（CNN）进行学习，并在变分自编码器（VAE）框架下训练模型 TargetDiff模型：基于EGNN进行Diffusion，在结构上和EDM相似，目标是学习条件分布。特别地，研究人员通过原子嵌入的熵来将原子类型的灵活性降低，从而提高结合亲和力。 DiffSBDD：DiffSBDD-cond是一种DDPM，而在基准测试中，DiffSBDD-inpaint则进行了图像增强，使用掩蔽和替换等方法对配体-蛋白质的部分区域进行处理。 这里给出了一个表格。展示了不同的模型的结果。\n蛋白质\r任务背景\r蛋白质可以通过其3D结构或者氨基酸序列来表示，氨基酸序列类似人类的语言，可以应用于自然语言模型。可以定义几个子任务：1）表示学习。2）结构预测。3）序列生成。4）主干设计。此外还讨论了抗体生成和肽生成。\n蛋白质表示学习\r使用氨基酸序列/原子坐标学习一个嵌入从而为其他生成模型创建更丰富的数据空间以供训练。类似于自然语言处理中的word2vec。\n结构预测\r从氨基酸序列来预测结构是极具挑战性和重要的工作。\n任务：从氨基酸序列来预测蛋白质结构。\n数据集：主要来自蛋白质结构预测的关键评估（CASP）。有PDB、CASP14和CAMEO。\n指标：均方根误差RMSD、全局距离测试总得分GDT-TS、模板建模得分TM-score和局部距离差异测试LDDT。\n模型：\nAlphaFold2：里程碑模型。采用端到端的方式集成了多层Transformer，融合多序列比对和成对表示的信息。基于氨基酸之间的成对距离探索折叠空间、氨基酸的潜在取向和整体结构。 trRosetta：transform-restrained Rosetta。输入MSA之后，预测残基对之间的距离和取向，然后利用Rosetta协议构建3D结构。 RoseTTAFlod：在CASP14上表现效果比肩AlphaFold2，特别是生成速度很快，仅需10分钟，相较AlphaFold2快了100倍。 ESMFold：利用ESM-2的输出嵌入到自注意力「折叠块」中，并通过以哦个具有SE（3）的transformer架构的结构模块生成最中国的结构预测。 EigenFold：应用Diffusion生成蛋白质结构的模型。它将蛋白质表示为一个谐振子系统，在正向过程中可以将结构投影到该系统的本征模式上，在反向过程中先采样粗糙的全局结构再细化局部细节。作为一种基于分数的模型，EigenFold 计算强度不高，但在准确性和范围方面仍不如其他模型。 抗体结构预测：\n这里的MSA结构不能作为抗体的输入。因此通用的模型如AlphaFold2效率非常低。\nIgFlod：使用来自AniBERTy的序列嵌入和不变点注意力机制来预测。 tFlodAb：减少了对Rosetta能量函数等外部工具的依赖。 序列生成\r序列生成，也被称为反向折叠或固定骨架设计，是结构预测的逆任务。生成能折叠成目标结构的氨基酸序列，对于设计具有期望结构和功能特性的蛋白质至关重要。由于有效序列的空间巨大，且蛋白质折叠过程复杂难以预测，因此需要多种深度学习方法来解决这些挑战\n任务：给定固定的蛋白质骨架结构，生成能折叠成该结构的相应氨基酸序列。\n数据集：模型主要使用 CATH 进行训练，部分会利用 UniRef 和 UniParc 进行数据增强，评估时常用 CATH 和 TS500。此外，Yu 等人创建了一组 14 个已知的从头蛋白质结构，用于避免数据污染。\n指标：\nAAR（氨基酸恢复率）：生成序列与天然序列中匹配氨基酸的比例。 多样性（Diversity）：通过 Clustalw2 测量生成序列对之间的平均差异。 RMSD（均方根偏差）：将生成序列折叠成结构后，与天然骨架结构进行比较的结构差异指标。 非极性损失（Nonpolar Loss）：衡量折叠结构中极性氨基酸类型合理性的指标，表面非极性氨基酸含量越高，损失越大。 PPL（困惑度）：交叉熵损失的指数化，代表天然序列出现在预测序列分布中的逆可能性。 模型：\n初步的一类模型在不考虑固定骨架目标的情况下生成蛋白质序列。但这些模型无法考虑关键的结构信息。 ProteinVAE 利用 ProtBERT 将原始输入序列转化为潜表示； ProT-VAE 使用不同的预训练语言模型 ProtT5NV； ProteinGAN 则采用 GAN 架构。 主要的模型接收固定骨架目标作为输入来生成氨基酸序列。 ProteinSolver 将生成骨架结构与解决数独问题联系起来，使用 GNN 架构； PiFold 引入更全面的特征表示； Anand 等人设计 3D CNN 直接学习条件分布； ABACUS-R 结合预训练的 transformer 来推断残基的氨基酸类型； ProRefiner 通过引入熵分数改进预测。 GPD 使用 Graphormer 架构， GVP-GNN 采用新颖的几何表示， ESM-IF1 扩展表示并在扩展数据集上训练， ProteinMPNN 实现了顺序无关的自回归方法。 在这些模型中，ProteinMPNN 在序列恢复、RMSD 和非极性损失方面表现最佳，GPD 则是最省时的方法。 主干设计\r生成全新的蛋白质可以直接扩充蛋白质库，实现高度复杂和多样的功能，是从头设计的核心。蛋白质设计在结构和序列上存在差异，有的模型生成 1D 氨基酸序列，有的直接生成 3D 结构，还有的同时设计两者。\n任务：从无输入或基于现有背景设计蛋白质骨干结构，即生成每个氨基酸的骨干原子（氮、$\\alpha$ - 碳、羰基和氧原子）坐标，外部工具可用于侧链填充。包含上下文无关生成（生成多样的蛋白质结构）和上下文给定生成（根据天然蛋白质的基序填充缺失残基）两个子任务。\n数据集：常用的数据集有 PDB、AlphaFoldDB、SCOP（及其扩展 SCOPe）和 CATH。\n指标：\nscTM（自洽 TM 分数）：通过将提议的结构输入序列预测模型（通常是 ProteinMPNN）生成相应氨基酸序列，再将其输入结构预测模型（通常是 AlphaFold2）生成样本结构，计算生成结构与样本结构之间的 TM 分数。分数大于 0.5 的结构通常被认为是可设计的。 scRMSD（自洽 RMSD）：与 scTM 类似，但使用 RMSD 进行评估，分数小于 2 通常作为截止值。 AAR（氨基酸恢复率）：比较生成的氨基酸序列与真实序列的相似程度。 RMSD（均方根偏差）：衡量生成的残基坐标与真实值之间的距离。 模型：\n较短蛋白质 ProtDiff 使用 3D 笛卡尔坐标表示每个残基和粒子滤波扩散方法，但 3D 笛卡尔点云不能反映蛋白质折叠过程； FoldingDiff 则使用角度表示，更接近蛋白质折叠过程中的旋转能量优化，通过 DDPM 和 BERT 架构从随机未折叠状态去噪到折叠结构； LatentDiff 先使用带 GNN 的等变蛋白质自动编码器将蛋白质嵌入潜在空间，再用等变扩散模型学习潜在分布，在潜在空间采样比在原始蛋白质空间快十倍。 长蛋白结构：基于框架的构建方法 Genie 使用由平移和旋转元素确定的框架云进行离散时间扩散来生成骨干结构； FrameDiff 基于框架流形参数化骨干结构，使用基于分数的生成模型； RFDiffusion 结合 RoseTTAFold 的强大结构预测方法和扩散模型，通过微调 RoseTTAFold 权重并输入掩码输入序列和随机噪声坐标来迭代生成骨干结构，还进行自我条件约束，性能优异； GPDL 使用 ESMFold 代替 RoseTTAFold 作为基础结构预测模型，并结合 ESM2 语言模型提取进化信息，生成骨干结构速度比 RFDiffusion 快 10 - 20 倍。 同时设计蛋白质序列和结构 GeoPro 使用 EGNN 编码和预测 3D 蛋白质结构，并设计单独的解码器解码蛋白质序列； Protpardelle 在反向扩散过程中对可能的侧链状态进行 “叠加” 并在每次迭代更新时进行塌缩； ProtSeed 使用三角函数感知编码器计算约束和相互作用，并通过等变解码器更新序列和结构； Anand 等人使用 IPA 在框架空间中进行扩散，高效生成蛋白质序列和结构 。 抗体CDR-H3生成：\n特别地，抗体生成聚焦于一个被称为 CDR-H3 区域的生成。最开始使用的是LSTM方法，后来转变为RefineGNN方法。此外，一些模型超越了CDR-H3生成任务，而是一次性处理抗体生成的多个环节。dyMEAN是一种端到端的方法将结构预测、对接和CDR-H3生成整合到一个模型中。\n多肽设计\r虽然已经有在蛋白质生成方面的重要、强大的模型，但是由于多肽结构的复杂和依赖于上下文已经下游应用的多样性，因此有必要为多肽的需求来定制模型。\n多肽生成：从头生成新型多肽\nMMCD：基于Diffusion的治疗性多肽生成模型，它联合设计多肽序列和结构（骨干坐标），采用Transformer编码器处理序列，EGNN 处理结构，并运用对比学习策略对齐序列和结构嵌入，区分治疗性和非治疗性多肽嵌入。 多肽-蛋白质互相作用：预测提议的多肽 - 蛋白质对的物理结合位点\nPepGB：基于GNN的模型。它利用图注意力神经网络学习多肽和蛋白质之间的相互作用。 多肽表示学习：将原始多肽序列转换为能捕获有价值信息的潜在表示\nPepHarmony：使用序列编码器（ESM）和结构编码器（GearNet），多视图对比学习模型，集成序列和结构信息以增强多肽表示学习。 多肽测序：解决质谱分析中从含噪数据提取氨基酸序列的挑战\nAdaNovo：从头多肽测序模型，由质谱编码器和两个受Transformer架构启发的多肽解码器组成。它利用条件互信息和自适应训练策略，在多种物种的多肽水平和氨基酸水平精度上显著优于之前的模型。 最近趋势\r生成式AI正在深刻地改变药物设计。\n生成式AI领域：GNN和基于图的方法的出现，推动了从基于序列的方法向基于结构的方法的转变，最终促使在生成任务中实现了序列和结构的整合。 分子生成领域：基于图的Diffusion模型作为主导。利用E（3）等变形来实现最先进的性能。 GeoLDM、MiDi——无靶点分子设计 TargetDiff、Pocket2Mol、DiffSBDD——有靶点分子设计 Torsional Diffusion——分子构象生成 此外，有靶点分子设计中也出现了从基于序列的方法到基于结构的方法的出现。 蛋白质生成领域：也出现了从序列到结构的转变。 GearNET：基于结构的表示学习模型 ESM-1B、UniRep：3D结构的重要性 AlphaFold2：结构预测的最先进模型 一些Diffusion方法也致力于蛋白质骨架构建。 挑战\r分子生成领域：\n复杂性 适用性 可解释性 蛋白质生成领域：\n基准测试 性能 结论\r介绍了生成式AI在从头开始的药物设计上的全貌，特别关注分子和蛋白质生成。\nRegularized Molecular Conformation Fields | NeurIPS 2022\rRegularized Molecular Conformation Fields | OpenReview\n属于上面综述文章里里面的分子生成里面的3D构象生成的部分。\n解决的问题\r给定2D分子图，预测有机分子能量最有利的3D构象。\n挑战\r分子在三维欧氏空间的 SE (3) 变换下具有不变性，同一分子的构象在刚性运动下有无限可能，增加了建模难度 分子在环境条件下存在多种动力学，导致高维且复杂的势能面，使得机器学习模型难以识别局部最小值来生成能量有利的构象 现有方法使用的不变特征可能冗余、相互依赖，导致数值不稳定和不合理的构象预测，且专门的等变层可能降低神经网络的表达能力，部分模型处理环状图存在困难 核心方法和优势\r核心方法\r构建正则化构象场。依据最少内部自由度（DoF）原则将分子分割。蓝色圆圈代表片段构型，一般来说是低内部柔性的。 利用MRF建模。红色圆圈是二面角构型，黑色方块是相邻构型之间的相互作用。利用马尔可夫随机场（MRF）对片段构型和二面角构型的联合概率分布进行建模。 推理和采样。推理时无环RMCF采用多年动态规划进行最大后验解码，有环使用LBP算法。采样使用Gibbs采样，每次采样后固定其他节点。采样后用特定距离度量样本差异，通过 K-means 聚类，从每个聚类中随机抽取样本，提升生成构象的多样性。 构象组装。将片段和二面角进行组装。 优势\r通过分子切割减少了构象空间维度，避免生产许多无关变量对模型的影响 MRF更好地捕捉相邻片段间的关系并对构象不确定性进行建模 数据集\rGEOM-QM9和GEOM-Drugs数据集。\nGEOM-QM9 评测方式\r分子构象的质量和多样性：覆盖分数（COV-R）和匹配分数（MAT-R） 预测精度：COV-P和MAT-P。 Zero-Shot 3D Drug Design by Sketching and Generating | NeurIPS 2022\r[2209.13865] Zero-Shot 3D Drug Design by Sketching and Generating\n文章属于综述里面的分子生成任务下的靶向分子设计的内容。提出零样本3D药物设计方法DESERT（Drug dEsign by SkEtching and geneRaTing）。\n目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 解决的问题\r目前的药物设计中传统方法和深度学习方法都有很多局限性。\n挑战是什么\r目前的方法都有一些局限性。\n传统方法遍历大规模药物库，耗时且难以产生新的候选药物 现有的深度学习方法依赖稀缺的实验数据，但是蛋白质口袋的生物活性数据大多缺乏，另一些依赖对接模拟，但是这个非常耗时，且准确性不够会影响模型的泛化能力 核心方法\r这里提出的方法是DESERT。把药物设计分成草图绘制和生成两个阶段。\n草图绘制阶段：获取与目标口袋互补的合理的分子形状。 有参考配体时，直接使用配体形状 在无参考配体时，基于生物学观察从蛋白质口袋中采样合理形状\n生成阶段：通过预训练的SHAPE2MOL模型将形状转换为具体的3D分子。 这里SHAPE2MOL将问题形状到分子的生成问题转换为了图像到序列的生成问题。也就是输入3D图像，给出一个序列，表示3D分子。\n内部结构是3D拓展后的ViT结构。\n优势\r减少数据和模拟依赖：DESERT 不严重依赖对接模拟，仅在后期可选使用对接进行后处理，同时抛弃了昂贵的实验数据，通过在大规模分子数据库（如 ZINC 数据库）上训练模型，降低了对实验数据的需求，避免了过拟合问题。 高效快速：相比基于 MCMC 的 GEKO 模型，DESERT 利用生物知识修剪搜索空间，能更快速地找到较好的解决方案，生成速度比 GEKO 快约 20 倍。 生成高质量分子：基于形状的设计方式使 DESERT 能够生成质量更高的分子。 数据集\r训练：使用了ZINC数据中的数据对SHAPE2MOL模型进行训练，包含了10亿对分子及其相应形状的数据。\n评估模型性能：12 种蛋白质（PDB IDs: 1FKG, 2RD6, 3H7W, 3VRJ, 4CG9, 4OQ3, 4PS7, 5E19, 5MKU, 3FI2, 4J71）相关的数据\n评测方式\r设计结果覆盖的分子空间：唯一性（Uniqueness）、新颖性（Novelty）、多样性（Diversity）、成功率（Success rate）和乘积（Product） 高活性分子的能力：通过比较Vina评分的分布，使用 Median Vina Score（Median）来量化分布。 On Pre-trained Language Models for Antibody | ICLR 2023\r[2301.12112] On Pre-trained Language Models for Antibody\n文章属于综述里面的蛋白质生成里面的抗体生成部分。\n目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 解决的问题\r目前的难以探究目前不同的预训练语言模型在抗体任务中的表现。 没有引入生物机制在模型之中。 挑战\r缺乏可靠的抗体特异性基准用于性能评估； 对当前蛋白质预训练语言模型（PPLMs）和抗体预训练语言模型（PALMs）的综合研究不足； 难以判断引入生物机制是否能真正有益于抗体表示学习； 确定预训练表示在实际应用（如药物发现和免疫过程理解）中的作用存在困难 预训练蛋白质语言模型PPLMs：\n利用蛋白质序列探索大语言模型。\n如ProtTrans和ESM-1b将单个蛋白质序列作为输入，使用Transformer架构进行预训练。 MSA-Transformer/MSA-1b模型通过多序列比对（MSA）作为输入。在结构预测方面，该模型优于 ESM-1b，这表明进化信息有助于蛋白质表征学习。 预训练抗体语言模型PALMs：\nAntiBERTy：提出首个抗体特异性语言模型，对在OAS数据库中的5.58亿条天然抗体序列使用Transformer架构进行预训练。 Abalang-H/L：恢复抗体序列中缺失的残基上的迁移学习。 AntiBERTa：在OAS数据库上预训练，并进行微调以用于抗原结合位点位置预测。 核心方法\r提出了抗体理解评估（AnTibody Understanding Evaluation，ATUE）基准和包含特定进化信息的EATLM模型。\n创建了一个全面的抗体基准测试工具ATUE。 抗原结合预测：二分类序列分类任务，确定抗体的CDR区域能否与特定抗原结合。 原因：通过对抗体 CDR 区域的分析，预测其与特定抗原的结合情况，有助于筛选出具有潜在治疗效果的抗体。 互补决定区预测：确定抗体序列上的结合位置的序列标注任务为 CDR 片段的每个残基预测 0/1 标签。 原因：确定抗体与抗原的结合位置，有助于深入理解抗体与抗原的相互作用机制。 B 细胞成熟分析：是一个 6 分类任务，区分 B 细胞抗体序列的成熟阶段，每个序列属于 {未成熟、过渡、成熟、浆细胞、记忆 IgD+、记忆 IgD-} 中的一种。 原因：有助于理解免疫进化过程中的机制。 抗体发现：是一个二分类序列分类任务，区分哪个抗体直接对 SARS-CoV-2 结合负责。 原因：从大量抗体中找出能与 SARS-CoV-2 结合的抗体，对于开发针对该病毒的治疗方法意义重大。 得出了关键观察结果，提供了如何更好地表示抗体的一些指导方针。 PPLMs 在与结构高度相关的抗体任务中表现良好，但在具有高抗体特异性的任务中表现不佳 在大多数情况下，PALMs 在预训练数据较少时表现得与 PPLMs 一样好甚至更好 通过结合进化过程可以改进 PALMs，但来自 MSA 的进化信息并不总是对抗体任务有益 探究引入生物机制对模型的影响，提出EATLM模型。 在传统掩码语言建模（MLM）的基础上，引入两个新的预训练目标以模拟抗体进化的生物机制。 祖先种系预测AGP 突变位置预测MPP 优势\rATUE基准涵盖了多个具有不同特异性的真实任务，能更加全面地评估模型。\n对于EATLM模型来说，引入了AGP和MPP两个预训练目标之后，有以下优势：\n抗原结合预测：对AUC和F1指标有改进 表达预测：在F1和MCC指标上优于其他模型 B细胞成熟分析任务：显著优于其他PALM模型 抗体发现任务：是识别所有钱再结合物最有效的方法，确定了11种潜在的SARS-CoV-2结合抗体，展示了在实际应用中的潜力。 数据集\r抗原结合预测 Mason等人（2021）的数据集 互补决定区预测 使用从 Liberis 等人（2018）收集的包含 1662 个 CDR 片段的数据进行研究。由于只有部分抗体来自进化，所以该任务具有中等特异性。 B细胞成熟分析 数据集来自 Mroczek 等人（2014），有 6 个成熟阶段的 88094 个序列。特异性高，抗体进化与 B 细胞成熟高度耦合。 抗体发现 研究人员收集了 133 名 SARS-CoV-2 患者和 87 名健康人的抗体序列，按照特定流程处理数据，并与 CoV-AbDab 数据库中的序列匹配，以确定潜在的结合抗体。由于来自同一疾病的抗体具有强烈的趋同种系信号，所以该任务特异性高。 评测方式\r对于不同任务，分别进行评估：准确度ACC、马修斯相关系数MCC、F1值、AUC。\nLearning Harmonic Molecular Representations on Riemannian Manifold | ICLR 2023\r[2303.15520] Learning Harmonic Molecular Representations on Riemannian Manifold\n这篇文章所属的领域属于分子生成领域中的表示学习。\n解决的问题\r现在的基于欧几里得空间的分子表示方法需要借助等变网络保证分子在表示旋转和平移的时候的一致性，这里的等距变换群就是E(3)/SE(3)。 目前的分子表示学习多采用自下而上，难以提供不同分辨率的特征。 挑战\r设计一种绕过等变要求，并且能够准确编码3D分子结构的表示。 开发在不同分辨率下为不同任务提供合适特征的多分辨率消息传递机制，特别是复杂的大分子。 核心方法\r用分子表面的拉普拉斯 - 贝尔特拉米（Laplace - Beltrami）特征函数来表示分子，在 2D 黎曼流形上实现多分辨率的分子几何和化学特征表示，并引入谐波消息传递方法进行高效的谱消息传递。\n大模型的解释：\r改变表示空间：传统方法在 3D 欧几里得空间中编码分子结构，为保证分子表示在旋转和平移时的正确性，需要借助等变网络。而该方法将分子表示在 2D 黎曼流形上。可以把黎曼流形想象成一个可以弯曲、变形，但局部性质类似欧几里得空间的特殊空间。在这个空间上，分子的表示天生就具有旋转和平移不变性。就好比把分子放在一个有弹性但又有自身规律的 “网” 上，无论分子怎么旋转、平移，这个 “网” 对分子的描述都不会改变，不需要额外的等变网络来调整，从而绕过了等变要求。 利用拉普拉斯 - 贝尔特拉米特征函数：分子表面可看作黎曼流形，拉普拉斯 - 贝尔特拉米（LB）特征函数是这个流形的固有属性。不同的分子表面有不同的 LB 特征函数，它们就像分子的 “指纹”，能反映分子的形状和结构特点。这些特征函数在刚性变换下保持不变，所以可以用来准确编码分子结构。例如，我们可以把分子表面想象成一个有很多不同纹理的曲面，LB 特征函数就像是描述这些纹理分布规律的工具，不管分子怎么转动、移动，这些纹理的分布规律是不变的，通过分析这些规律就能准确编码分子结构。 多分辨率表示与信息传递：通过对 LB 特征函数的线性组合，可以实现分子表面的多分辨率表示。不同频率的 LB 特征函数可以捕捉分子不同尺度的特征，低频部分反映分子的整体、大致的形状，高频部分则能体现分子的细节特征。在进行信息传递（类似消息在分子表面不同区域传播）时，利用基于 LB 特征函数构建的谐波消息传递机制，能在不同尺度上传播信息。这就好像在一个城市中，有不同规模的道路来传递信息，主干道（低频特征）传递整体的、大致的信息，小巷（高频特征）传递详细的、局部的信息，从而全面、准确地编码 3D 分子结构 优势\r在2D黎曼流形上的分子天然具有旋转和平移不变性，无需依赖等变网络 采用自上而下的方式，能提供多分辨率特征 分子形状定义了黎曼流形，原子构型决定流形上的相关函数，更全面反映分子性质 数据集\rQM9：小分子性质回归任务 配体结合口袋数据集，数据划分方式参考这里 刚性蛋白质对接数据集作为训练集，Docking Benchmark 5.5作为测试集。 评测方式\rQM9 小分子性质回归：通过计算预测结果与真实值的平均绝对误差（MAE）来评估模型性能，对比其他不变性和等变网络模型，如 SchNet、NMP 等。 配体结合口袋分类：使用平衡准确率评估模型预测蛋白质口袋结合配体类型的能力，与 MaSIF-ligand 模型对比。 刚性蛋白质对接：采用 Complex RMSD、Interface RMSD、DockQ 和成功率等指标评估对接性能。Complex RMSD 和 Interface RMSD 衡量预测结构与真实结构的偏差；DockQ 是基于多个标准化标准的综合评分；成功率表示预测结果达到 “可接受” 或更高水平的比例 。 Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D | ICML 2023\r[2305.13266] Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D\n文章属于综述里面的分子生成中的3D分子生成。\n解决的问题\r现有的3D分子生成方法在生成大尺寸分子的时候存在结构质量差的问题。\n挑战\r自回归模型：按照人工设定的顺序逐个生成原子，如同语言生成过程。但分子在 3D 空间具有自然的几何结构，这种方法引入的人为顺序与分子的自然结构不匹配，并且会产生规模和误差累积问题。 非自回归模型：原先的原子级生成方法虽然灵活性较高，但是在片段级生成时，由于化学价态限制，片段冲突常见，且避免片段冲突的复杂度高，随着结构尺寸增加，复杂度呈指数上升，难以获得可靠的分子结构。 核心方法\r提出基于分层Diffusion的分子生成方法Hierarchical Diffusion-based 模型（HierDiff）。\n将3D分子生成问题看作约束生成问题。\n先通过等变Diffusion过程生成粗粒度分子几何结构，其中每个粗粒度节点反映分子中的一个片段 通过消息传递过程和一个新设计的迭代细化采样模块，将粗粒度节点解码为细粒度片段 将细粒度片段组装起来，得到完整的原子分子结构 详细解释\r粗粒度片段扩散：HierDiff 将 3D 分子生成视为约束生成问题，首先定义粗粒度节点的表示，包括不变化学特征和等变位置特征。通过精心设计化学特征（如基于属性和元素的特征）和位置特征（使用中心坐标），利用扩散模型生成粗粒度片段表示及其笛卡尔坐标。在这个过程中，通过特殊设计的初始分布和转移核，保证模型的 SE (3) 不变性，从而有效生成合理的粗粒度分子几何结构。 细粒度片段生成：基于生成的粗粒度节点，通过一系列步骤生成细粒度片段类型和边。具体包括选择焦点节点、预测新边、确定细粒度片段类型以及迭代细化。利用消息传递神经网络和迭代细化模块，不断纠正细粒度节点中的偏差，提高生成分子的真实性。 组装成原子构象：根据细粒度生成过程确定的节点和链接关系，选择合适的原子合并方式构建原子级构象。利用 RDkit 生成局部构象，并通过 Kabsch 算法计算旋转矩阵和平移向量，将局部构象对齐到采样的中心位置，逐步生成完整的原子构象。 优势\r与同类方法相比，HierDiff 能保持非自回归方法的全局建模特性，显著降低寻找可连接片段的复杂度，有效避免片段冲突，生成的分子更具现实性和药物样属性，在多个评估指标上优于现有方法。\n数据集\r使用GEOM-Drugs和CrossDocked2020数据集。\n评测方式\r药物相似性 QED 逆向合成可及性RA 药物化学过滤器MCF 合成可及性分数SAS LogP $\\Delta$LogP 分子重量MW 构象质量 计算覆盖度Cov 匹配度Mat Accelerating Antimicrobial Peptide Discovery with Latent Structure | SIGKDD 2023\r[2212.09450] Accelerating Antimicrobial Peptide Discovery with Latent Structure\n这篇文章属于综述下面的蛋白质生成下的肽设计部分。\n解决的问题\r加速抗菌肽（AMP）的发现。现有的深度学习模型只考虑序列属性，但是忽略了结构对于活性的关系。\n挑战\r目前深度学习模型只考虑序列特征，忽略结构-活性关系。结构对于肽的活性具有重要影响，但是却没有应用。\n核心方法\r提出 Latent Sequence-Structure 模型（LSSAMP）。\n将序列特征和二级结构映射到潜空间中 采用VQ-VAE为每个位置分配一个潜在变量 通过在潜空间采样生成理想序列组成和结构的肽 优势\r考虑了序列和结构信息，肽的抗菌性更强\n数据集\rUniversal Protein Resource（UniProt）中的蛋白质序列和通过ProSPr预测的二级结构 Antimicrobial Peptide Database中的抗菌肽数据集 评测方式\r自动评估指标 使用开源的AMP预测工具估计生成序列的AMP概率。 依据电荷、疏水性、疏水矩这三个对AMP机制至关重要的序列属性评估生成性能。 通过唯一性、多样性和相似性来衡量生成的肽的新颖性。 湿实验室实验 通过上面的自动评估指标从5000个肽中筛选出21个肽进行合成实验 合成了之后在培养皿中采用肉汤微量稀释法测定最小抑菌浓度（MIC）,从而验证。 Equivariant Flow Matching with Hybrid Probability Transport for 3D Molecule Generation | NeurIPS 2023\r这篇文章属于分子生成领域中的3D分子生成。\n解决的问题\r3D 分子生成需要同时确定原子类型和坐标。\n目前的模型，特别是Diffusion方法存在采样速度低和概率动力学不稳定的问题。\n挑战\r现在的扩散模型存在概率动力学不稳定和采样速度低的问题。\n核心方法\r提出等变流匹配（EquiFM）框架。\n引入等变最优传输（Equivariant Optimal-Transport）引导原子坐标的生成概率路径。这方法蕴含最小化坐标变化的先验，能稳定训练并提升生成性能。 基于信息量构建混合生成路径解决模态不一致问题：根据不同组件的信息量差异，设计不同的生成概率路径，形成混合生成路径。 原子特征空间包含多种数据模态，如电荷、原子类型和坐标分别属于离散、整数和连续变量。 利用 ODE 参数化模型提升推理效率：模型基于连续归一化流，由 ODE 参数化。相比Diffusion使用的SDE提高了推理效率。 优势\r稳定 效率高 生成分子质量更好 数据集\rQM9、GEOM-DRUG数据集\n评测方式\r分子建模与生成 通过预测键类型评估生成分子的化学可行性，计算原子稳定性、分子稳定性、有效性、唯一性等指标衡量生成质量 条件分子生成 在 QM9 数据集上测试模型根据给定属性生成分子的能力，通过计算生成分子属性值与目标属性值的平均绝对误差（MAE）来评估性能。 Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks | ICLR 2024\r这篇文章属于分子生成领域的3D分子生成部分。\n解决的问题\r生成模型在应用于3D分子几何生成时面临的挑战，尤其是多模态和噪声敏感性问题。\n挑战\r多模态 分子几何的原子级描述依赖多种数据形式，不同模态数据的统一处理较为困难。 噪声敏感性 微小的坐标噪声就可能导致分子层面的信号急剧下降，影响模型性能 核心方法\r提出GeoBFN。\n贝叶斯流网络BFNs\n假设：数据样本的信息应沿着潜变量的马尔科夫链逐步增加，且信息变化尽可能平滑。 基于引入噪声变量的潜变量模型，通过优化变分下界学习概率分布，在参数空间操作以保证信息变化平滑 统一概率建模 使用统一的概率建模公式处理分子几何中的不同模态。 将3D分子表示为$g=\u0026lt;x,h\u0026gt;$，其中$x$为原子坐标矩阵，$h$包含原子类型和原子电荷等节点信息。 通过EGNN对输出分布进行参数化 保持SE(3)不变性 零质心空间约束下，通过设计满足特定条件的概率模型，使似然函数 \\(p_{\\phi}\\) 具有平移和旋转不变性， 克服噪声敏感性 GeoBFN 在参数空间中通过贝叶斯更新过程来降低方差。在更新中，噪声程度较低的样本会被赋予更小的权重。 优化离散变量采样 使用NEAREST_CENTER函数将输入和中心桶进行比较，并为每个输入值返回值返回最近的中心。 优势\r生成质量上表现卓越。 可以在任意采样步数下达到效率和质量的最优平衡 采样效率较高 数据集\rQM9和GEOM-Drugs\n评测方式\r无条件分子生成 条件分子生成 Multimodal Molecular Pretraining via Modality Blending | ICLR 2024\r这篇文章属于分子生成领域中的表示学习方向。\n解决的问题\r现在的多模态分子预训练方法在对齐2D和3D模态时，仅进行粗粒度分子级对齐，未充分挖掘内在关系。\n挑战\r准确捕捉2D和3D分子中原子关系的内在联系 充分整合多模态信息的模型架构和训练方法 核心方法\r提出blend-then-predict自监督学习方法。\n将不同模态的原子关系混合成统一矩阵进行联合编码，注入到Transformer的自注意力模块中 再恢复特定模态信息 优势\r能在细粒度原子级别对齐和整合2D和3D模态，全面描绘分子\n数据集\r预训练使用 PCQM4Mv2 数据集，来自 OGB Large - Scale Challenge.\n评估在多个公开数据集上进行\nMoleculeNet（用于 2D 分子性质预测，涵盖多种分子性质相关数据集） QM9 量子性质数据集（包含 13.4 万个小有机分子，用于 3D 任务评估） 评测方式\r2D任务：使用支架分割（scaffold split）方式划分数据集 分类任务以ROC-AUC分数为指标 回归任务以RMSE为指标 3D任务：随机划分验证集和测试集 以MAE为评估指标 Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge | SIGKDD 2024\r这篇文章属于分子生成领域的表示学习方向。\n解决的问题\r现有分子表示学习（MRL）模型在学习多视图分子表示时存在不足。难以从化学结构、生物医学文本和知识图谱等异构源中有效捕捉分子知识，且无法充分利用不同视图间的共识和互补信息，不能很好地适应不同应用场景。\n多视图分子表示指的是从不同角度捕捉分子信息，如从微观角度，分子有原子、化学键等；从宏观角度，分子的晶体结构、物理状态等。\n挑战\r现有分子表示学习（MRL） 模型需将视图信息显式融入表示中，以适应广泛应用，但此前模型多通过 “包装文本” 或微调隐式整合，影响对不同视图知识关系的理解 要处理分子结构、生物医学文本和知识图谱等质量和数量各异的信息源的异质性，以往将知识图谱转换为文本的方法可能因预训练数据分布不均衡引入偏差。 核心方法\r提出 MV-Mol 模型\nMV-Mol 模型架构 (a) 基于视图的分子编码器：此编码器是获取视图相关分子信息的关键组件，输入为分子结构和文本提示。 （b）分子分支 结构由\\(M=(V, E, C)\\)表示，利用预训练的 Uni-Mol 进行编码，将其转化为原子的特征表示\\(z(a)\\)。 Q-Former 的分子分支以 K 个可训练的查询向量作为输入嵌入，借助跨注意力机制，在每隔一个 Transformer 层时，从原子表示\\(z(a)\\)中提取关键信息。 （c）文本分支 提示\\(T=[x_{1}, x_{2}, \\cdots, x_{L}]\\)代表不同的分子视图。 Q-Former 的文本分支对文本提示T进行处理。 Q-Former 两个分支的自注意力层共享，使分子表示能够融合不同视图的信息，最终输出基于视图的分子表示。 （d）模态对齐 通过跨模态对比和跨模态匹配进行模态对齐。 跨模态对比损失用于最大化分子结构与文本表示间的互信息， 跨模态匹配损失则通过预测分子结构和文本是否对应同一分子，来培养模型对两者的细粒度理解 （e）多视图知识融合 将关系建模为一种文本提示，从特定视图约束分子知识。 设计知识图谱嵌入和知识图谱不全目标来实现多视图知识融合。 多模态解码器：采用 BioT5 的解码器作为多模态解码器。 输入是基于视图的分子编码器输出。 通过因果生成，将输入转化为自然语言文本，实现对分子表示的自然语言解释 。 优势\rMV-Mol 能更好地捕捉不同视图的分子知识。\n在分子属性预测任务上，比最先进的 Uni-Mol 模型平均绝对增益 1.24% 在跨模态检索任务中，相比最佳基线模型，top-1 检索准确率平均提高 12.9% 在跨模态生成任务中预测也更准确。 数据集\r预训练采用大规模分子-文本对和知识图谱。\n分子 - 文本对通过对 350 万篇科学出版物进行命名实体识别和实体链接获得 知识图谱由多个公共数据库合并构建。 下游实验使用多个数据集\n用于分子属性预测的 MoleculeNet 中的 8 个分类数据集 用于跨模态检索的 PCdes 和 MVST 数据集 用于跨模态生成的 ChEBI-20 数据集等。 代码和数据可在https://github.com/PharMolix/OpenBioMed获取。\n评测方式\r在分子属性预测中 采用 Scaffold split 划分数据集 在多个数据集上微调模型并报告 AUROC 分数 跨模态检索包含结构到文本和文本到结构检索两个子任务， 使用 PCdes 和 MVST 数据集 按 Scaffold split 划分 报告 MRR（平均倒数排名）和 Recall at 1/5/10 跨模态生成包括结构到文本生成和文本到结构生成 在 ChEBI-20 数据集上按原始划分进行实验 采用 BLEU、ROUGE、METEOR 等指标评估分子字幕任务 用精确率、有效率等指标评估文本到分子生成任务 MolCRAFT: Structure-Based Drug Design in Continuous Parameter Space | ICML 2024\r这篇文章属于分子生成领域的靶向药物分子设计方向中的基于结构的药物设计（SBDD）。\n解决的问题\r靶向分子设计有两种，一种是基于配体的药物设计（LBDD），另一种是基于结构的药物设计（SBDD）。LBDD利用目标蛋白质的氨基酸序列，借助已知的配体特征来构建；SBDD利用目标蛋白质的三维结构来设计。\n当前基于SBDD生成模型在生成分子时，常出现不符合要求的情况。\n生成的分子不能同时满足高亲和力、良好的类药性质和合理的 3D 构象这几个关键标准，产生假阳性结果，阻碍了 SBDD 模型在实际中的应用 。\n挑战\r分子模式坍塌：自回归模型在生成分子时倾向于产生有限数量的特定（子）结构，从化学和几何角度来看，其生成的独特分子比例较低，对某些环结构存在偏好，且在模拟不同键类型的键长时表现不佳，无法有效捕捉参考分布的多模态特征。 混合连续 - 离散空间：Diffusion模型虽然通过非自回归生成在一定程度上缓解了模式坍塌问题，但混合连续 - 离散空间使得模型难以准确捕获分子的复杂数据流形。在这个空间中进行去噪时，不同模态之间的不一致性会导致生成的分子存在高应变和不可行的情况，中间噪声潜在值容易超出流形范围。 核心方法\r提出MolCRAFT模型，是首个在连续空间运行的SBDD模型。\n特点 统一的 SE-(3) 等变生成模型 在连续参数空间中进行分子生成 实现 统一参数化：将连续原子坐标和离散原子类型分别进行参数化。 不同模态噪声处理：由于参数的连续性，即使对于离散原子类型也能应用连续噪声。 SE(3)等变网络：使用 SE-(3) 等变网络对蛋白质 - 分子复合物的相互作用进行建模，确保模型在平移和旋转下的不变性。 噪声减少采样策略： 传统的采样方式在每个时间步都对连续原子坐标和离散原子类型进行采样，易引入过多噪声。 MolCRAFT 设计了在参数空间内的噪声减少采样策略，用估计的\\(\\hat{m}=[\\hat{x},\\hat{v}]\\)（\\(\\hat{v}\\)直接采用连续输出的类别值而不采样）直接更新下一步的参数。 优势\r结合亲和力。 能达到参考水平的 Vina 评分（-6.59 kcal/mol），远超其他强基线模型 构象稳定性 在模拟局部模式时表现出色，在键长和角度分布上排名第一，且生成的配体 - 蛋白质复合物中的冲突更少，重新对接后的 RMSD 表现最佳，46% 的生成分子在无需力场优化或重新对接的情况下就接近准确的对接姿势 采样效率更高 速度更快 数据集\r使用 CrossDocked 数据集，经过基于 RMSD 的过滤和 30% 序列同一性拆分后，得到 100,000 个训练对和 100 个测试蛋白质。\n评测方式\r结合亲和力 构象稳定性 类药性质 QED 合成可及性SA 多样性Div 整体评估 结合可行性（合理亲和力+构象稳定的分子比例） 成功率（满足Vina Dock、QED和SA阈值） 生成效率 ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling | ICML 2024\r这篇文章属于蛋白质生成领域中的表示学习领域。\n解决的问题\r当前蛋白质语言模型主要在残基尺度运行，无法提供原子尺度信息。\n挑战\r统一分子建模难题：残基和原子尺度使用的词汇表不兼容，直接在原子尺度对蛋白质进行表示和预训练效率低下，难以实现有效的统一分子建模。 位置编码设计困难：设计合适的位置编码来准确描述同一蛋白质中残基和原子之间的关系颇具挑战，涉及残基与残基、残基与原子、原子与原子之间的多种关系，而现有蛋白质语言模型的编码方法无法满足需求。 核心方法\r提出ESMAA（ESM All-Atom），实现原子尺度和残基尺度统一分子建模的新方法。\n在多尺度代码转换蛋白质序列上进行预训练 利用多尺度位置编码来捕捉残基和原子之间的关系 优势\rESM-AA 能同时处理残基和原子尺度信息\n在蛋白质 - 分子任务上表现更优。 如在酶 - 底物亲和力回归、药物 - 靶点亲和力回归等任务中超越了其他模型，实现了最先进的结果。 在蛋白质任务和分子基准测试中也有良好表现 数据集\r预训练数据集\n蛋白质 AlphaFold DB 数据集： 800 万个由 AlphaFold2 预测的高置信度（PLDDT \u0026gt; 90）蛋白质序列和结构 小分子 使用 Zhou 等人提供的数据集：含有 1900 万个分子和 2.09 亿个由 ETKGD 和默克分子力场生成的构象 评测方式\r蛋白质 - 分子任务：在酶 - 底物亲和力回归、药物 - 靶点亲和力回归和酶 - 底物对分类任务上进行微调评估，将模型预测结果与实验数据对比，使用均方误差（MSE）、决定系数（R²）、皮尔逊相关系数、准确率（ACC）、马修斯相关系数（MCC）、受试者工作特征曲线下面积（ROC-AUC）等指标衡量性能。 蛋白质任务：通过二级结构预测和无监督接触预测任务测试模型对蛋白质结构的理解能力，使用准确率等指标评估，且模型输入为纯残基序列。 分子任务：利用标准分子基准测试 MoleculeNet 中的任务，如分子性质分类和回归任务，使用平均绝对误差（MAE）、AUC 等指标评估模型性能。 Mol-AE: Auto-Encoder Based Molecular Representation Learning With 3D Cloze Test Objective | ICML 2024\r这篇文章属于分子生成领域中的3D分子表示学习方向。\n解决的问题\rencoder-only model在预训练和下游任务目标之间存在不一致性，导致预训练学到的特征在下游任务中迁移性差 坐标去噪目标会引发训练不稳定以及引入不真实噪声，同时原子坐标在该过程中作为内容和标识符的双重角色产生冲突，影响模型性能。 挑战\r克服预训练和下游任务目标不一致带来的难题，使模型在不同阶段学到的特征能有效应用于实际任务 解决坐标去噪导致的训练不稳定和原子标识符混乱问题，让模型能够稳定训练并准确学习分子结构信息。 核心方法\r提出 MOL-AE 模型，核心包含基于 Transformer 的 3D 信息感知自编码器结构以及 3D Cloze Test 目标。\n整体架构： MOL-AE 模型处理 3D 分子时，主要聚焦 3D 结构和原子类型信息。 原子类型建模：可借助原子 MLM 目标轻松实现 3D 结构的建模：模型由编码器\\(q_{\\phi}\\)、解码器\\(p_{\\theta}\\)构成，且二者均以 Transformer 架构为基础。 3D 信息感知自编码器 Transformer Block：Transformer 由多个 Transformer 块组成，每个块包含多头自注意力层和前馈层。此过程能有效捕捉分子信息。 3D 感知成对特征：因普通 Transformer 难以处理 3D 信息，MOL-AE 采用将原子对之间的欧几里得距离编码为额外成对特征的方法。帮助模型更好地理解分子的3D结构。 编码器和解码器：编码器\\(q_{\\phi}\\)由\\(L^{enc}\\)层 Transformer 块构成，3D 坐标信息C经其处理后编码为\\(X^{L^{enc}}\\) ，并将其作为潜在表示Z 。解码器\\(p_{\\theta}\\)由\\(L^{dec}\\)层 Transformer 块组成，由于 3D 结构已编码在Z中，其输入成对特征初始化为零。 3D Cloze Test 目标 添加位置编码（PE）到解码器：为解决坐标去噪中原子标识符混乱问题，MOL-AE 在解码器中添加 PE。 在打乱坐标时，PE 作为稳定的标识符，帮助模型区分不同原子。同时，仅在解码器添加 PE 可避免引入的顺序信息对编码器学习分子高质量表示产生影响。 这里可以类比GPT会添加位置编码。 原子丢弃：传统去噪目标可能使模型学习不可靠的噪声分布，MOL-AE 通过随机丢弃部分原子及其坐标（如从输入坐标C中随机移除k行得到\\(D(C)\\) ）来干扰数据，使模型专注于剩余无噪声的子结构，从而更好地学习原子空间关系。 优势\r缓解了预训练和下游任务目标不一致的问题，提升了特征的迁移能力 解决了坐标去噪带来的不稳定训练和原子标识符混乱问题，使模型训练更稳定 在多个分子理解任务中性能显著优于当前最先进的 3D 分子建模方法，在分子分类和回归任务的基准测试中取得了优异成绩 数据集\r预训练数据集：使用 Zhou 等人提供的大规模分子数据集，包含 1900 万个分子和 2.09 亿个构象，由 ETKGD 和 Merck 分子力场生成，每个分子有 11 个随机生成的构象，为提高计算效率，预训练时去除了氢原子，未明确该数据集是否公开。 微调数据集：采用广泛使用的 MoleculeNet 基准数据集，包括 9 个分类数据集和 6 个回归数据集，该数据集公开。 评测方式\r分子分类任务：使用 ROC-AUC 作为评估指标，在 9 个分类数据集上进行实验，数据集为 MoleculeNet 中的相关分类数据集。 分子回归任务：采用平均绝对误差（MAE）和均方根误差（RMSE）作为评估指标，在 6 个回归数据集上进行实验，数据集为 MoleculeNet 中的相关回归数据集。通过与多个监督和预训练方法的对比，评估 MOL-AE 的性能。 ","date":"2025-02-26T22:17:58+08:00","permalink":"https://ionfeather.github.io/2025/ai-assist-drug-design/","title":"论文阅读 | AI Assist Drug Design"},{"content":"一直想用一些shortcodes来优化一下页面的呈现方式。\n特别感谢：\n在 Stack 主题上可行的短代码们 一些Hugo短代码 Hugo |另一篇Stack主题装修记录 | 小球飞鱼 Hugo | 在文章中插入轮播图片 | 小球飞鱼 文字\r重点标记\r好喜欢蓝色！\r文本折叠\r点击展开\r这是第一个段落的内容。\n这是第二个段落的内容，位于折叠部分，实际使用别忘了双括号！\n文字黑幕\r数据删除！数据删除！\n但总之换行的话就加个空标签。\n高斯模糊\r一些手动打码效果！\n但总之换行的话就加个空标签。\n文本位置\r文字居左\n文字居中\n文字居右\n摘录引用\r羊皮卷上所载一切自永远至永远不会再重复，因为注定经受百年孤独的家族不会有第二次机会出现在大地上出现。\n加西亚·马尔克斯\r《百年孤独》 居中引用\r有些人说\n换行敲多了\n就是\n诗\n键盘样式\rCtrl+Alt+Del\n卡片\r可以在这里插入链接假装是卡片式链接。\n好像不能插入图片？\n换行需要空标签。实际使用需要双括号。\n标签块\rWarning：需要双括号。\ninfo：这是一条信息。\nnote：可以标注一下，但是没必要。\ntip：在示例里胡说八道会使观看者会心一笑。\n对话框\r导师\u0026nbsp;\u0026nbsp;\u0026nbsp;2024-10-12 14:30\r做一个论文阅读的大模型。 2024-10-12 14:45\u0026nbsp;\u0026nbsp;\u0026nbsp;我\r好的老师。 时间轴\r2024-10-20\r博客\r创建ionfeather\u0026#39;Log\r使用Hugo的Stack主题\r2024-11-04\r博客\r增加评论区\r部署Waline，但还有一些问题\r2025-02-13\r博客\rBug修复\r修复了一些bug，如邮箱显示错误、搜索功能失效等\r图片滚动\rBilibili\r网易云音乐\r标签墙\r其实这个不算是shortcodes，但是我不知道把这个放在哪里。所以这个犄角旮旯就不错！\n特别感谢：\nHugo | 月球基地开发历程 Blog | 主题重新施工，和书影游展示墙 | 小球飞鱼 标签墙最后放在了我的「关于」页面里。\n欢迎大家去看！\n","date":"2025-02-18T13:40:27+08:00","permalink":"https://ionfeather.github.io/2025/shortcodes/","title":"搬运一些Shortcodes"},{"content":"论文阅读 | 多智能体协作机制：大语言模型综述\r[2306.03314] Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents\n摘要\r随着大语言模型（LLMs）的最新进展，代理式人工智能（Agentic AI）在现实应用中取得了显著进展，朝着基于多个大语言模型的智能体迈进，实现感知、学习、推理和协同行动。这些基于大语言模型的多智能体系统（MASs）使得一组智能体能够协作解决复杂任务，并以大规模方式实现集体行动，从孤立的模型转向以协作为核心的方法。\n本文提供了关于多智能体系统协作方面的广泛综述，并提出了一个可扩展的框架来指导未来的研究。我们的框架根据关键维度对协作机制进行表征：参与者（涉及的智能体）、类型（例如，合作、竞争或合作竞争）、结构（例如，点对点、集中式或分布式）、策略（例如，基于角色或基于模型）以及协调协议。通过对现有方法的回顾，我们的研究成果为揭示和推动基于大语言模型的多智能体系统向更加智能和协作的解决方案发展，特别是在复杂的现实应用中，提供了基础。\n此外，本文还探讨了多智能体系统在不同领域的各种应用，包括5G/6G网络、工业5.0、问答系统、以及社会文化环境，展示了它们的广泛应用和更深远的影响。最后，我们总结了关键经验教训，分析了多智能体系统面临的开放挑战，并指出了朝着人工集体智能发展的潜在研究方向。\n文章大纲\r应用\r方法 领域 主要贡献 优点 缺点 参考文献 LLM-SC 物联网 作为知识生成器增强语义解码器 利用大语言模型，实现显著的编码增益 由于使用大语言模型，计算资源需求高 [130] LaMoSC 物联网 提出一种大语言模型驱动的多模态融合语义通信 在低信噪比条件下表现稳健 由于使用大语言模型和视觉 Transformer，计算资源需求高 [157] LAM-MSC 物联网 为多模态数据设计联合编码器；大语言模型作为知识生成器 一个编码器和解码器可处理多种类型的数据；实现更好的编码率和重建误差 由于使用大语言模型，计算资源需求高 [65] GMAC 物联网 利用大语言模型实现观察状态与自然语言之间的语义对齐，并压缩语义信息 提高收敛速度；实现无通信的多智能体协作 由于使用大语言模型，计算资源需求高 [160] LLM-Blender 自然语言生成 采用多种大语言模型代理的集成方法进行候选排序 能够生成比现有候选更好的输出 为实现最优解，需要进行 O (n) 次推理，导致计算开销大 [64] SOT 自然语言生成 并行生成每个答案框架；完成答案内容（需要规划结构） 通过并行加速推理速度；适用于需要长结构答案的问题 答案质量评估远非完美，由于提示集有限；不同代理的并行请求可能会影响服务吞吐量 [95] Meta-Prompting 自然语言生成 构建高级元提示来指导大语言模型 保持连贯的推理思路；挖掘各种专家角色 多次模型调用成本较高；需要大量的规模和相当大的上下文窗口 [119] MAD 自然语言生成 两个代理表达各自的论点；一个评判者监控和管理辩论 减少偏差和扭曲的认知；鼓励无限的外部反馈 由于辩论时间长，计算成本高；大语言模型在长场景中难以保持连贯性和相关性 [77] FORD 自然语言生成 包括三个阶段的辩论：公平辩论、不匹配辩论、圆桌辩论 通过辩论让大语言模型探索自身理解与他人概念化之间的差异 除常识推理外，无法涵盖各种任务；严重依赖多项选择任务，限制了其泛化能力 [140] ChatDev 自然语言生成 采用聊天链将每个阶段分解为更小的子任务，实现代理之间的多轮通信，以协作开发解决方案 最大限度减少代码幻觉（提供的源代码缺失的情况） 没有清晰、详细的要求时，代理难以理解任务想法；通用软件的自动化评估非常复杂；多个代理需要更多的令牌和时间，导致计算需求大 [105] AgentVerse 自然语言生成 由专家招募、协作决策、行动执行、评估四个阶段组成 提高大语言模型在不确定情况下的泛化能力；提高代理的适应性 协作决策过程中代理之间的通信存在挑战 [24] AgentCoord 社会与文化领域 为协调策略提供结构化表示；采用三阶段方法将一般目标转化为可执行策略 简化协调策略的表示和探索；最小化代理的重复实例 仅支持在纯文本环境中协调代理协作；仅支持静态协调策略设计 [97] OpenAI\u0026rsquo;s Swarm 自然语言生成 用于多智能体编排的例程和交接；轻量级协调与执行框架 适用于需要可扩展性的应用；交接机制允许在专门代理之间实现无缝过渡 主要关注基于角色的协议和集中式 / 分布式结构；尚未准备好投入生产 见原文 TE 社会与文化领域 在主题研究中模拟人类参与者的代表性样本 能够模拟不同的人类行为，并揭示模拟中的一致偏差 需要研究更多的人类行为和额外的大语言模型，以确保关键发现的准确性 [36] AgentInstruct 社会与文化领域 通过迭代的跨代理细化生成多样化的自然语言数据，包括文化数据 能够通过工具使用、代理能力等从生成的数据中训练更强大的模型 需要人工构建生成流程 [88] SocialMind 社会与文化领域 整合言语、非言语和社交线索，通过增强现实眼镜生成现场建议 设计并利用多模态、多层协作代理系统 需要先进的边缘硬件来处理复杂系统 [144] CulturePark 社会与文化领域 促使基于大语言模型的代理进行跨文化交流模拟 生成的数据可用于训练具有不同文化背景的模型，减少偏差并实现民主化 仍然依赖大语言模型对每种文化的了解，因此对资源较少的文化效果有限 [73] Mango 社会与文化领域 通过对概念和文化的提示，从基于大语言模型的代理中提取高质量知识 自动化方法可生成大量资源 人类评估需要来自更多样化的背景 [94] 六个思考帽的设计\r白色思考帽\r功能：收集客观信息。 实现方式 对论文进行解析。 从论文文本中抽取结构化数据。 从网络中搜索作者之前的研究成果。 从网络中搜索同类研究的对比数据。 绿色思考帽\r功能：对论文提出创新性改进，探索论文的可能性 实现方式 未定。 黄色思考帽\r功能：积极角度评估论文，找出论文的优点和贡献。 实现方式 用优点和创新点微调后的大模型。 黑色思考帽\r功能：批判性思考，找出论文的问题和不足。 实现方式 用批判性数据集微调后的大模型。 红色思考帽\r功能：主观感受和直觉判断。 实现方式 让智能体多阅读论文，找到好的论文之间的共性和形成自己的「偏好」。 蓝色思考帽\r功能：控制评审流程。 实现方式 未定。 蓝色思考帽智能体应该如何控制？\r基于工作流管理的集中式控制方法。蓝色智能体明确规定了其他智能体的工作顺序、时间和交互方式。 基于协商机制的分布式控制方法。在评审开始时，蓝色智能体发起评审任务，各思考帽智能体根据自身能力和状态反馈可承担的工作及预计时间。比如白色告诉蓝色需要5分钟完成，绿色说在白色完成后需要10分钟\u0026hellip;通过这些反馈，蓝色智能体来制定计划。 基于事件驱动的动态控制方法。不同的智能体换成之后会触发不同的事件，如白色完成后让绿色工作，黑色和黄色在辩论后无法达成共识，就再次进行辩论等。这个事件定义较难。 ","date":"2025-02-15T21:58:31+08:00","permalink":"https://ionfeather.github.io/2025/multiagentcollaboration/","title":"论文阅读 | 多智能体协作机制：大语言模型综述"},{"content":"雨水：表示降水开始，雨量逐步增多。雨水节气天气变化不定，是全年寒潮过程出现最多的时节之一，忽冷忽热，天气乍暖还寒。\n北京最近的天气确实是这样。在北京能感受到24节气的准确，能感受到四季分明是什么感觉。\n但是！！雨水，雨水，北京来点儿雨吧。自从去年的12月以来，我没有见过一滴雨落到北京的地表，这里的晴天就像是默认背景，太阳和月亮每天都是固定角色出现在地平线和天空中。北京的「雨水」是艳阳高照。\n上了研究生学术没有做多少，兴趣爱好培养了不少——台球、博客、摄影、健身…可能还打算学个吉他和乒乓球。忙不过来，实在是忙不过来了。还是得多放点时间在学习上呀。\n今天是情人节，这么一想，晚上的健身房应该会比较空，可惜昨天跑步跑太狠，把脚掌磨出了一个水泡，走路都有点儿疼，今晚回去可以做做力量训练，就先不做有氧了。\n好羡慕甜甜的爱情。\n2025-02-14 15:02\n买了辆电瓶车，之后出行方便多了。 终于不用来回奔波那么久了。 去地铁站终于不用思考用共享单车还是公交车了。\n2025-02-14 21:58\n今天把原先的房子里的东西全部打扫干净，让它恢复成原来的样子。我才意识到，虽然住了半年，但是我们在这个房子里的印记能在一天之内被打扫得无影无踪。原来人是流动的水。\n和室友买了一些做饭的家伙，之后可能会在出租房里（我其实也愿意称它为「家」）做一些简单的菜。室友的女友可能过两天会来，到时候可以期待一下她的厨艺（好像听说也是新手，那还是期待我自己的进步吧\n上了称，感觉自己胖得不行了。但是过两天又是组会，头疼，我这周什么也没干。\n2025-02-15 19:26\n房东来看房，她（果然）嫌弃我们打扫得不干净，表示要找保洁来清理。MSY、SH和我商量了一下，最后让她扣了300元。我该怎么说——其实这个人还算好说话。反正扣完了之后也没说什么，现场就转了钱。我本来还以为她会拿门钥匙这件事说事。\n帽子到啦！我的MBTI帽子到了，我直接往上贴了一个「ENFP」，然后周游工位告诉每个见到我的人，可惜今天是周日，没见到几个人。大家都去哪儿啦？\n不过感觉自己也不用担心（什么担心，我这叫好奇）大家都去哪儿了。明天就是组会，目前还没有进展，现在就开始看论文吧！\n2025-02-16 13:35\n听说有个师姐被求婚了，被求婚了？被求婚了！看到了照片，男帅女靓啊，又是羡慕别人爱情的一天。\n不过我还是有一点儿恍惚：原来这也是我这个年纪该听到的事情吗？我还以为自己还小。问了一下同门，她说她也不介意研究生读完就结婚，我开玩笑地说：「那明年可以暗示你一下你的男朋友了。」\n对我来说，可能还是有点早了吧？我心里还没准备好。但是这种事情谁说得好呢？\n2025-02-16 22:33\n开完组会了。\n最近组会的气氛比较轻松愉快。但是在我看到同门和老师写的一篇论文之后，还是有些不淡定——我还没有将idea实现、落地的能力。在上面散发着新鲜油墨气味的论文被交到我手上的时候，我内心还是有些无奈。\n我也不想安慰自己说未来也能写出来，我对这件事情甚至没有任何认知。写一篇论文到底需要什么？就我来说，和如何把冰箱里的大象拿出来一样，是一个全新的领域。\n2025-02-17 17:07\n又是10点才起床的一天。\n室友的女友来我们这里借住几天，没想到是个精力十足、爱笑、笑起来是「嘿嘿嘿」的山东女生。她的笑声确实听起来很愉快，能够感受到她很开心，听起来穿透力很强。没想到这么响亮的笑声在之前都没听过，看来之前在出租屋的小房间里，她也是忍耐住了自己的笑（笑\n中午的时候，她和室友下厨，做了油泼面，吃着还可以，比学校的面也不遑多让——或许是学校食堂太糊弄。我拿了我的碗，刷刷刷吃了一碗半。\n晚上又碰见他们了。室友和他女友在跑步机旁边，室友跑步，女友爬坡。\n2025-02-19 23:35\n昨天去中国电影博物馆看了《哪吒2之魔童脑闹海》。电影在15:25开始，我是在12:00出发，但是博物馆的位置确实有点儿偏——也可能是我的位置太偏——我到达那里已经是14:00了。\n走马观花地参观了一下中国电影博物馆的展厅，进门是主旋律正能量主题展厅。往里走，里面有一个巨大的环形的展厅，地板、墙壁都是LED屏幕，一共有四层楼高，有一个环形的缓缓上升的参观路线，绕着墙壁，通向其他二楼三楼的展厅。\n在展厅里，对我这个电影盲来说，只能看到那些最脍炙人口的电影，比如《警察故事》《小蝌蚪找妈妈》，才会知道「啊！原来是它！」\n有没有书的展览馆！搜了一下文学没有像是中国电影博物馆一样这种大而全的，但是专题类的很多：中国现代文学馆、北京鲁迅博物馆、老舍纪念馆、巴金故居、上海文学博物馆\u0026hellip;感觉又种草了不少。\n不愧是IMAX GT屏幕，真的很壮观。我坐得特别前面，还有点儿偏，哪吒也并非最适合它的1.43:1的屏幕比例，因此上下还是有黑边，《奥本海默》和《沙丘》会更适合它。但，但是（申公豹式强调）这不妨碍我看电影的时候感慨画质和细节，云雾、锁链、粒子\u0026hellip;在我正前方130°范围里，全都是哪吒。\n电影好！屏幕好！看电影的人好！真是一次美好的体验。\n2025-02-21 21:30\n有的时候也很焦虑诶。又是一天什么也没做。昨天给妙妙讲了讲道理——学习就像是跑仓鼠的轮子，滚起来就会一直向前。\n我也该把我的轮子滚起来了。\n2025-02-22 21:15\n朋友的生日，唱歌从11点唱到凌晨4点。\n五个人算是把自己拿手的歌全唱了——我发现我以前的五音不全病似乎有好转的迹象——《思念是一种病》《一事无成的伟大》我宣布现在已经是我的拿手曲目了。\n2025-02-28 12:00\n","date":"2025-02-14T14:36:39+08:00","image":"https://ionfeather.github.io/2025/rainwater/cover_hu14924232115569819230.jpg","permalink":"https://ionfeather.github.io/2025/rainwater/","title":"雨水 | 北京的「雨水」是艳阳高照"},{"content":"春节快乐！\n工科研究生的假期有点短暂了。我已经回校了。\n到学校了感觉自己好多东西需要购置。Apple Watch的表带现在明显太松了，但是官方太贵，第三方又有点儿硬，计划看一下Bilibili上的测评，进行一波购置。还有看上了影视飓风的一款帽子，上面可以贴上你的mbti，这对我这个enfp根本无法拒绝。\n又来到了北方，又遇到了高铁上一望无际的平原，感觉是另一种大海，在这片海里，有冰封的河流，有枯黄的树枝，还有炊烟和蜗居的人们。有一种说法是南方人向往雪，北方人向往海，这么一看，我应该是个不那么彻底的南方人。我两个都很向往。\n今天是元宵节，我本科的时候是灯谜社社长。元宵节，英语叫Lantern festival，也就是灯节，是灯谜社最重要的节日。在传统文化节里，我会张贴灯谜，擂起鼓，在鼓声滚滚中，同学需要猜出我出的谜题——有些是我们社团自己写的，所以很难猜，需要脑子有点儿回路，特别是与英雄联盟或者是与本科学校相关的那种谜题。\n虽然现在天气还很冷，但是感觉春意渐浓，即便树梢仍然枯枝，阳光灿烂的时候也能把寒意给驱散。\n我已经开始期待春天了。玉兰、银柳、山茶、樱花、樱桃花、木绣球、垂丝海棠。我要抄起我的相机，出门拍花拍鸟。\n不管怎么说，虽然已经上了几年（学校一年，外界一天）班了，现在还是春节期间，我一会儿说不定可以出门，去颐和园旁边拍花灯。\n","date":"2025-02-12T17:51:19+08:00","image":"https://ionfeather.github.io/2025/lanternfestival/cover_hu10283662063726621985.jpg","permalink":"https://ionfeather.github.io/2025/lanternfestival/","title":"元宵节 | 东风夜放花千树，更吹落、星如雨"},{"content":"之前一直在用Typora来写文章，发现有的时候也太难用了，不仅插件少，还要付费。这个时候看到很多的博客都用Obsidian来写，不得不心动了。\nObsidian的插件\rLinter\rObsidian Linter插件：打造统一、美观的笔记环境 - 知乎\n我不得不赞赏一下这个Linter，真的很好用，格式化目前的Markdown内容一直是我的心头痒，对于我这个强迫症来说，现在只需要按一下Ctrl+S就可以让我的敲击的内容都非常规范化，这实在是伟大的发明。\nExcalidraw\r这个插件还挺好看的，可以绘制手绘风格的图像，我绘制一些想法会更加方便。\n其他\rAdvanced Tables：对写表格比较有帮助。 Customizable Menu：自定义右键快捷键。 ","date":"2025-02-12T15:59:30+08:00","permalink":"https://ionfeather.github.io/2025/obsidian/","title":"使用Obsidian来写博客"},{"content":"全书结构\r预备知识\r张量\r张量表示一个由数值组成的数组，这个数组可能有多个维度。\n具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。\n张量的创建\rimport torch x = torch.arange(12) x.shape x.numel() X = x.reshape(3, 4) torch.zeros((2, 3, 4)) torch.ones((2, 3, 4)) torch.randn(3, 4) torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) 运算符\r按元素运算\r常见的运算符这里用作按元素运算。\nx = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y # **运算符是求幂运算 可以得到\n(tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) 还有很多的一元运算符都可以用在按元素运算。\n线性代数运算\r求和/平均值\r直接调用sum函数，会将其变成一个标量，也可以指定axis = 1维度来指定轴来进行降维。\nA.sum() A.sum(axis = 1) A.sum(axis = [0, 1])# 对于矩阵来说，相当于A.sum() 同理，A.mean()也是一样的。\n如果希望能够在求和或者平均值的时候保持轴数不变，可以使用keepdims = True。\n如果希望能够沿着某个轴计算A元素的累计总和，可以使用cumsum函数。\nsum_A = A.sum(axis=1, keepdims=True) A.cumsum(axis=0) 点积\rx = torch.arange(4) y = torch.ones(4, dtype = torch.float32) torch.dot(x, y) 矩阵-向量积\r当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。\n矩阵-矩阵乘法\r我们可以将矩阵-矩阵乘法AB看作简单地执行m次矩阵-向量积，并将结果拼接在一起，形成一个n×m矩阵。\n在下面的代码中，我们在A和B上执行矩阵乘法。 这里的A是一个5行4列的矩阵，B是一个4行3列的矩阵。 两者相乘后，我们得到了一个5行3列的矩阵。\nB = torch.ones(4, 3) torch.mm(A, B) 张量连结\r在这里，dim=0说明是第一个维度进行拼接；dim=1说明是第二个维度进行拼接。\nX = torch.arange(12, dtype=torch.float32).reshape((3,4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) 广播机制\r特别需要注意这个，可能会导致错误发生。\na = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a, b 由于a和b分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的3×2矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。\n索引和切片\r与Dataframe中相似。\n节省内存\r如果直接使用X = X + Y就是重新创建一个元素。但是，有些时候希望执行原地操作。\n如果希望执行原地操作的话，可以使用两种方式，此时不会占用新的空间：\nX[:] = X + Y X += Y 转换为其他对象\r转换为Numpy非常容易：A = X.numpy()\n转换为Python标量：a.item()或者使用内置函数float(a)等。\n自动求导\r自动求导是计算一个函数在指定值上的导数。\n如何实现？ 计算图：将代码分解成操作子，将计算表示成一个无环图。 关于计算图，有显式构造 vs 隐式构造两种构造方式。\n特性 显式构造 隐式构造 计算图构建方式 显式定义 隐式定义 计算图类型 静态图 动态图 典型框架 TensorFlow 1.x, Theano PyTorch, TensorFlow 2.x (Eager) 有两种求导的方式，对于一个链式法则，我们可以采取正向累积和反向累积（也称反向传递）。\n示例说明\n以 \\( y = (x_1 + 2x_2)^2 \\) 为例：\n反向传递： 前向计算 \\( z=11, y=121 \\)。 反向计算 \\( \\partial y/\\partial z=22 \\rightarrow \\partial y/\\partial x_1=22, \\partial y/\\partial x_2=44 \\)。 正向传递： 前向计算 \\( z=11 \\)，同时记录 \\( \\partial z/\\partial x_1=1, \\partial z/\\partial x_2=2 \\)。 前向计算 \\( y=121 \\)，同时记录 \\( \\partial y/\\partial z=22 \\)。 直接组合导数得到 \\( \\partial y/\\partial x_1=22 \\times 1=22 \\)，\\( \\partial y/\\partial x_2=22 \\times 2=44 \\)。 反向累积\r使用反向传递的时候，在我们计算$y$关于$x$的梯度之前，需要一个地方来存储梯度。\n重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量$x$的梯度是向量，并且与$x$具有相同的形状。\n","date":"2025-01-08T16:00:34+08:00","image":"https://ionfeather.github.io/2025/d2l-01/assets/cover_hu3492737127834985036.png","permalink":"https://ionfeather.github.io/2025/d2l-01/","title":"《动手学深度学习》"},{"content":"虚拟环境配置经历\r我之前配置好了一个虚拟环境名为vllm，专门用于vllm的启动，我还特意将其中的虚拟环境中的所有包的版本保存到vllm_requirements.txt文件中。\n但是我一顿操作之后，原本配置好的环境现在也没办法使用了。此时我庆幸自己想到用vllm_requirements.txt文件保存。但是在进行pip install -r vllm_requirements.txt的时候，出现了报错的情况，竟然说里面有一个包的版本是yanked version（撤回版本），无法下载，给我气晕了。\n吃一堑，长一智。配置好的环境就不要变了，应该另外复制一个环境，在复制的环境上进行修改。\n此外，我每次进行配置环境我都会忘记怎么配置和删除。是我最近记性变得太差了吗？总之我写一个文档，记不住就查一下。\n配置环境\r使用conda配置虚拟环境\r创建新的环境\r使用Terminal创建新的环境。\nconda create -n \u0026lt;new_env_name\u0026gt; python=3.10.0 激活虚拟环境\nconda activate \u0026lt;new_env_name\u0026gt; 安装包\nconda install \u0026lt;package\u0026gt; pip install \u0026lt;package\u0026gt; 从已有的文件中安装包/虚拟环境\r如果想要安装requirements.txt文件，就可以直接\npip install -r requirements.txt 如果想要安装的是environment.yml文件，应该改用conda来创建虚拟环境\nconda env create -f environment.yml 查看虚拟环境列表\nconda env list 复制原来已有的虚拟环境\r如果有一个环境已经配置好，我不希望破坏它，可以复制一个一模一样的环境，再在上面进行修改，这样就不会导致原来那个环境产生问题。\nconda create --name \u0026lt;new_env_name\u0026gt; --clone \u0026lt;old_env_name\u0026gt; 删除虚拟环境\r删除指定的虚拟环境\nconda activate base conda remove -n \u0026lt;env_name\u0026gt; --all 在conda中配置Jupyter内核\r安装Jupyter内核\r总是忘记Jupyter内核如何配置。记录一下：\n安装ipykernel。\nconda install ipykernel 将虚拟内核添加到jupyter内核中。\npython -m ipykernel install --user --name \u0026lt;your_env_name\u0026gt; 删除jupyter内核\r查看目前有的jupyter内核\njupyter kernelspec list 删除指定的jupyter内核\njupyter kernelspec remove \u0026lt;your_kernel_name\u0026gt; 照片\r照片是2024/12/7的时候同门团建的时候我拿大疆Pocket3拍的。拍的建筑是东郊民巷的圣弥厄尔大教堂。非常开心的一天。\n","date":"2024-12-15T20:24:19+08:00","image":"https://ionfeather.github.io/2024/virtual-environment-config/cover_hu15097618714508979196.jpg","permalink":"https://ionfeather.github.io/2024/virtual-environment-config/","title":"虚拟环境配置操作记录"},{"content":"为什么要学习LangChain\r我希望能够构建一个能阅读PDF论文的Agent，并且能够输出对论文优缺点的评价。\n导师\u0026nbsp;\u0026nbsp;\u0026nbsp;2024-10-12 14:30\r做一个论文阅读的大模型。 2024-10-12 14:45\u0026nbsp;\u0026nbsp;\u0026nbsp;我\r好的老师。 使用LangChain听说比较方便。\nLangChain是用来做什么的？\rLangChain是一个用于开发由LLM驱动的应用程序的框架。也就是说我们可以把LLM作为内核，LangChain作为外壳，搭建一个程序出来。\nLangChain提供了\n组件：处理LLM的组件的抽象； 定制链：把组件拼起来，实现一个特定用例。 对于阅读PDF，目前有两个想法：\n将PDF转为JSON，然后输入到LLM中； 构建RAG。使用LangChain能够比较方便地实现这个功能，听ZLB说这个也不是很难。我之前的畏难情绪可能太重了，现在写一个文档，激励和记录一下自己学习。 RAG是什么？\r虽然LLM非常强大，但它们对于它们未经训练的信息一无所知。如果您想使用LLM来回答它未经训练的文档相关问题，您需要向其提供这些文档的信息。最常用的方法是通过“检索增强生成”（ retrieval augmented generation，RAG ）。\n检索增强生成的思想是，在给定一个问题时，首先进行检索步骤以获取任何相关文档。然后将这些文档与原始问题一起传递给语言模型，并让它生成一个回答。然而，为了做到这一点，首先需要将文档以适合进行此类查询的格式呈现。\n构造一个语义搜索引擎\rBuild a semantic search engine | 🦜️🔗 LangChain\n读取PDF\rHow to load PDFs | 🦜️🔗 LangChain\n这里，文档中推荐使用了pypdf库。这里\n在实际应用中可以使用其他提取效果更好的库。LangChain支持的PDF格式很多，可以选择一下。\nDocument Loader Description Package/API PyPDF Uses pypdf to load and parse PDFs Package Unstructured Uses Unstructured\u0026rsquo;s open source library to load PDFs Package Amazon Textract Uses AWS API to load PDFs API MathPix Uses MathPix to load PDFs Package PDFPlumber Load PDF files using PDFPlumber Package PyPDFDirectry Load a directory with PDF files Package PyPDFium2 Load PDF files using PyPDFium2 Package PyMuPDF Load PDF files using PyMuPDF Package PDFMiner Load PDF files using PDFMiner Package 此外，导师之前还给我推荐了titipata/scipdf_parser库，能够更好地处理图像和扫描文本，并且运行在docker上，便于部署。\npypdf的介绍\rWelcome to pypdf — pypdf 5.1.0 documentation\nPyPDF 是一个用于处理 PDF 文件的 Python库。它提供了一组工具和功能，用于读取、解析和操作 PDF 文件的内容。\nSplitting\r原文\rFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \u0026ldquo;washed out\u0026rdquo; by surrounding text.\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nWe set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”.\nSee this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\n对于问题提问的文本来说，直接回答一整页肯定是太粗略了。我们最终的目标是检索回答输入查询的文档对象，进一步拆分 PDF 将有助于确保文档相关部分的含义不会被周围的文本“冲淡”。\n所以接下来应该用文本分割器来进行分割（Splitting）处理。这里用一个RecursiveCharacterTextSplitter进行分割。这里使用常见分隔符来对文档进行分割，适用于一般的文本。\n使用RecursiveCharacterTextSplitter无法读取图像或特定区域的文本。\nEmbeddings\r接下来将文本嵌入到向量中去，便于进行相似度指标来识别相关文本。\n这里LangChain支持数十种Embeddings方法。这里我选择了使用Hugging Face，可以选择将模型下载至本地或者使用Hugging Face Inference API来调用接口。这里可以直接使用HuggingFaceEmbeddings来进行处理。非常方便。\nfrom langchain_huggingface import HuggingFaceEmbeddings embeddings_model = HuggingFaceEmbeddings(model_name=\u0026#34;sentence-transformers/all-mpnet-base-v2\u0026#34;) embeddings = embeddings_model vector_1 = embeddings.embed_query(all_splits[0].page_content) vector_2 = embeddings.embed_query(all_splits[1].page_content) assert len(vector_1) == len(vector_2) print(f\u0026#34;Generated vectors of length {len(vector_1)}\\n\u0026#34;) print(vector_1[:10]) Vector Stores\rLangChain的Vector Stores对象包括了一些把文本和Document对象加入到Stores中的方法，然后通过相似性进行一个排列。\nfrom langchain_core.vectorstores import InMemoryVectorStore vector_store = InMemoryVectorStore(embeddings) ids = vector_store.add_documents(documents=all_splits) 此时就完成了存储和排列。\n这里向量存储一般来说是可以连接到现有的Vector Stores中的。\nUsage\r查询和这句话相似的句子 results = vector_store.similarity_search( \u0026#34;Diffusion is a image generation method.\u0026#34; ) ) print(results[0]) 异步查询（用于流程控制） results = await vector_store.asimilarity_search(\u0026#34;What is diffusion?\u0026#34;) print(results[0]) 返回分数 # Note that providers implement different scores; # the score here is a distance metric that varies inversely with similarity. results = vector_store.similarity_search_with_score(\u0026#34;What is Diffusion?\u0026#34;) doc, score = results[0] print(f\u0026#34;Score: {score}\\n\u0026#34;) print(doc) 通过和embedded query的相似度进行查询 embedding = embeddings.embed_query(\u0026#34;What is diffusion\u0026#34;) results = vector_store.similarity_search_by_vector(embedding) print(results[0]) Retrievers\r检索器（Retriever）可以从向量存储中进行构建，但是也可以和非向量形式进行交互。如果我们要构建一个能够检索文档的方法的话，我们可以创建一个runnable的检索器。\nfrom typing import List from langchain_core.documents import Document from langchain_core.runnables import chain @chain def retriever(query: str) -\u0026gt; List[Document]: return vector_store.similarity_search(query, k=1) retriever.batch( [ \u0026#34;What is diffusion?\u0026#34;, \u0026#34;What is forward process?\u0026#34;, ], ) 至此，我们构建了一个能够读多篇PDF文章的、能够对PDF文章进行查询的语义搜索引擎。\nChat Models和Prompt模板\r这里通过Vllm启动LLM，以Qwen2.5-7B-Instruct模型为例。\nfrom langchain_community.llms import VLLM llm = VLLM(model=\u0026#34;/home/ubuntu/jjq/Qwen/Qwen2.5-7B-Instruct/\u0026#34;, trust_remote_code=True, max_new_tokens=512, top_k=10, top_p=0.95, temperature=0.8, max_model_len = 30000, ) print(llm(\u0026#34;What is the capital of France ?\u0026#34;)) 接下来设计Prompt模板。\nfrom langchain import LLMChain from langchain.prompts import PromptTemplate from langchain.memory import ConversationBufferMemory from langchain.chains import ConversationalRetrievalChain from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate template = \u0026#39;\u0026#39;\u0026#39; 【任务描述】 请仔细阅读论文，回答用户给出的问题，尽量具有批判性。 【论文】 {{context}} ----------- {question} \u0026#39;\u0026#39;\u0026#39; # 检索器 retriever = db.as_retriever() # 记忆 memory = ConversationBufferMemory(memory_key=\u0026#34;chat_history\u0026#34;, return_messages=True) # 构建Agent qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory) qa({\u0026#34;question\u0026#34;: \u0026#34;能不能用中文给出论文的优势或者前景？\u0026#34;}) ","date":"2024-11-26T13:45:58+08:00","image":"https://ionfeather.github.io/2024/langchain-learning/cover_hu13968487782357828171.png","permalink":"https://ionfeather.github.io/2024/langchain-learning/","title":"LangChain学习笔记"},{"content":"梦\r这是一个英雄辈出的时代。所谓阴阳师，就是使用牌组与其他人对战来决定胜负的职业。在这里，国之阴阳师是一国中最强的阴阳师，中日韩三国每年都会选拔国之阴阳师，并且对战，决出最后的冠军。\n我和梦梦是青梅竹马，从小便展露了阴阳师的天赋。所谓“绕床弄青梅，郎骑竹马来”，我和梦梦那就是“绕床打牌组，郎打牌组来”。从小与其他人对战，胜利了之后可以选择是否获取一张新卡\n牌放进自己的卡组，最终打一个最强者之间的对战。这个对战从来都是我和梦梦之间的私人聊天与沟通时间。\n随着我们渐渐长大，我和梦梦之间也互生情愫。但是，认真打牌，赢得中日韩三国之间的对战，获得至高无上的荣耀是我们的最大目标。儿女情长，英雄气短，阴阳师需要克制。\n……\n梦梦要去日本打探消息了。她去那里，是为了我们中国能够更了解日本的特殊卡牌。可是，一个人在异国他乡，离开最亲近的人，是那么容易的事情吗？\n终于到了中日韩会赛的时间。\n我期待着到达了梦梦的住所，敲门，迎接我的果然是笑靥如花的梦梦。我们见到对方，思念已久的澎湃难抑制，但我们都克制住了自己，只是眼睛里互相诉说着彼此。\n但梦梦的房间里有股不详的气息。她好像被监视了。“你的房间里曾经有过一个男人躲在里面”，我说。\n梦梦害怕极了，但为了国家能够去刺探信息的人必然非常坚强，她脸色发白，不住地颤抖，但声音很小：“还在吗……这怎么办…”\n我安慰她：“这没什么，这是日本人监视你的手段，但梦梦你肯定也有没被看破的地方。”随后我离开了梦梦的家，脸色发青。\n……\n接下来就是我和八重岛神子最终对战了。近几年韩国式微，只剩中日交战。日本去年赢过了我国。去年对战使用的是30张左右的小牌组，打到后面基本就是6张左右一个循环，对方的强度比我们高。\n八重岛神子太强了，所有人都不相信我能打败他，包括我自己。我用尽心血，准备了一套120张卡牌的超大牌组进行对战。奇妙的是，八重岛神子也拿出来120张的卡组。这极其少见。我们来了一场古典的交锋。\n……\n突然，我想起之前在什么地方，在很久很久的从前，有个人和我说我会赢的，于是我便充满了信心，我的阴霾一扫而空。\n我尽量诚实地描述了我的梦境，可惜忘记了太多。我感觉里面有蛮多意象的。围棋中日韩会战、恋爱、谍战监视、杀戮尖塔烧牌循环、炉石传说对战\u0026hellip;很有意思的一个梦，醒来之后回味了很久。但是看起来可能没有那么有趣，或许我应该加一点戏剧性要素？\n照片\r照片是我2024/11/3的时候在地坛拍的。我想那一天是北京秋天最美的一天。北京的秋天是短暂的，11/3之前雾霾太重，看什么都朦胧；11/3那天的风很大，无数灿烂的叶子不断地落下来，在这一天之后，树梢上就稍微有一些秃了。\n","date":"2024-10-20T13:45:58+08:00","image":"https://ionfeather.github.io/2024/dream/cover_hu13768060218741915990.jpg","permalink":"https://ionfeather.github.io/2024/dream/","title":"一个有趣的梦 | 三国阴阳师"}]