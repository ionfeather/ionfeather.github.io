[{"content":"盼望着，盼望着，春天的脚步近了。飞快，羽绒服已经太热，食堂门口的桃花准备抽枝，路过图书馆时望见乌鸦「哇——哇——」地向北飞。\n已经摸上我的相机，快乐地订好了出游计划。\n万物复苏，只期待一场春日游。春日游。杏花吹满头。陌上谁家年少，足风流——说不定是我。\n","date":"2025-03-05T14:37:00+08:00","permalink":"https://ionfeather.github.io/2025/the-waking-of-insects/","title":"惊蛰 | 漫长的冬天终于到达了尾声"},{"content":"开展一些AI-assist Drug Design 的方向探索，需要进行一些论文阅读和调研，形成调研报告。\n参考：\n[2402.08703] A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation Hao Zhou（周浩) 目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation\r[2402.08703] A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation\n摘要\r将从头药物设计（de novo drug design）归纳为两大主题：小分子生成和蛋白质生成。在每个主题下，我们识别出多种子任务和应用，重点介绍重要的数据集、基准测试、模型架构，并比较顶尖模型的性能。\n引言\r并非是虚拟筛选/定向进化。而是从头开始的自然界中并未存在的新的生物实体的生成。 文章中分成两个部分来讲解。小分子和蛋白质。 文章将介绍 生成式模型的种类：Diffusion/VAE/Flow-Based/GAN 将文章分成小分子和蛋白质两个领域，分别介绍 一般背景/任务定义 用于训练和测试的常见数据集 常用的评估指标 对过去和当前的机器学习方法的概述 对SOTA方法的性能的对比分析 总结 相关研究\r其他的方法都太专业，而这篇文章对小分子和蛋白质生成进行宏观层面的分析，有助于那些想要对化学创新领域中新兴的生成式AI模型有一个高屋建瓴的了解的人。\n前言：生成式AI模型\r介绍了VAE、GAN、Flow-Based Models和Diffusion，还介绍了一些其他模型，比如GNN、EGNN等。\n应用\r小分子\r任务背景\r分子生成聚焦于为药物设计创造新的分子化合物。这些生成的分子旨在具有 （1）有效性、（2）稳定性和（3）独特性，总体目标是具有药物适用性。“药物适用性” 是一个宽泛的术语，用于描述分子对各种生物靶标的结合亲和力。\n虽然前三个任务可能看起来微不足道，但仅仅生成有效和稳定的分子就存在各种挑战。因此，无靶向分子生成领域专注于生成有效的分子集合，而不考虑任何生物靶标。靶向分子生成（或配体生成）侧重于针对特定蛋白质结构生成分子，因此更关注药物成分。最后，3D 构象生成涉及在给定 2D 连接的情况下生成各种 3D 构象。\n无靶向分子设计\r必须满足前两个特点，也就是有效性和稳定性。需要满足很多的复杂的条件，所以其实还是挺难的。深度学习可以帮助人们更有效率地生成有更高可能性满足有效性的分子。\n任务：无输入，输出为生成一组新的、有效的、稳定的分子。\n数据集：QM9和GEOM-Drug\n指标：\n分子生成任务指标：原子稳定性、分子稳定性、有效性、独特性、（新颖性）、药物相似性度量估计值QED。 模型的评估方式：通过在QM9数据集上的一部分训练属性分类网络，然后对模型生成的分子进行评估，计算目标和评估属性值之间的平均绝对误差。 分子在特定化学性质方面的指标：极化率$α$、最高占据分子轨道能量 $\\varepsilon_{HOMO}$、最低未占据分子轨道能量 $\\varepsilon_{LUMO}$、$\\varepsilon_{HOMO}$ 和 $\\varepsilon_{LUMO}$ 的差值$\\Delta_\\varepsilon$、偶极矩 $\\mu$、298.15K 下的摩尔热容 $C_v$。 模型：\n过去几年间，分子生成任务的方法从一维的SMILES转变成二维的连接图，然后是三维的几何结构，最后到融合二维和三维信息的方法。\n1D SMILES字符串模型 早期方法：如CVAE、GVAE直接处理，但是SMILES因为是一维的，存在问题：两个化学结构相似的分子图可能会得到非常不同的 SMILES 字符串，这使得模型更难学习到这些相似性和模式。 2D 图生成模型 JTVAE：首个直接生成 2D 分子图的模型，通过树状骨架迭代扩展并验证结构有效性。 3D 结构模型 早期方法：Flow-Based方法ENF和自回归方法G-SchNet。 EDM：基于Diffusion的 3D 点云模型，利用 E (3) 等变性提升性能，避免原子排序依赖。 GCDM：结合几何深度学习与Diffusion，引入注意力机制优化消息传递。 联合2D和3D JODO：联合2D和3D的扩散模型，使用几何图形表示来捕获 3D 空间信息和连接信息，对这种联合表示应用分数随机微分方程，同时提出扩散图变换器来参数化数据预测模型，避免在每个独立通道独立添加噪声后相关性的丢失。 MiDi：应用了DDPM，提出了「松弛」的图神经网络（EGNN）。 这里给出了三个表格。\n表格1：生成模型在QM9数据集上的条件无关的分子设计任务上的性能表现。Diffusion方法比之前的方法好很多，但是在GEOM-Drugs上可能表现不佳。 表格2：EDM、MDM、MiDi 等模型在GEOM-Drugs数据集上的条件无关的分子设计任务上性能表现。MiDi能生成更稳定的复杂分子，但是有效性较低。 表格3：生成模型在条件分子生成任务上的性能表现。MDM、GCDM生成表现不错，MDM前四项较好，GCDM后两项较好。 靶向分子设计\r有两种，一种是基于配体的药物设计（LBDD），另一种是基于结构的药物设计（SBDD）。LBDD利用目标蛋白质的氨基酸序列，借助已知的配体特征来构建；SBDD利用目标蛋白质的三维结构来设计。\n任务：给定氨基酸序列/蛋白质的三维结构，生成对应的有高结合亲和力以及潜在相互作用的分子。\n数据集：CrossDocked2020、ZINC20和Binding MOAD。\n指标：Vina Score、Vina Energy、高亲和力百分比High Affinity Percentage、合成可及性分数SAscore和多样性Diversity。\n模型：\nLBDD 结合了Transformer结构，例如DrugGPT，训练的时候输入为SMILES和蛋白质氨基酸序列，从而训练输出可行的SMILES配体。 SBDD LiGAN：三维目标感知分子输出的概念，将分子适配到网格格式，以便利用卷积神经网络（CNN）进行学习，并在变分自编码器（VAE）框架下训练模型 TargetDiff模型：基于EGNN进行Diffusion，在结构上和EDM相似，目标是学习条件分布。特别地，研究人员通过原子嵌入的熵来将原子类型的灵活性降低，从而提高结合亲和力。 DiffSBDD：DiffSBDD-cond是一种DDPM，而在基准测试中，DiffSBDD-inpaint则进行了图像增强，使用掩蔽和替换等方法对配体-蛋白质的部分区域进行处理。 这里给出了一个表格。展示了不同的模型的结果。\n蛋白质\r任务背景\r蛋白质可以通过其3D结构或者氨基酸序列来表示，氨基酸序列类似人类的语言，可以应用于自然语言模型。可以定义几个子任务：1）表示学习。2）结构预测。3）序列生成。4）主干设计。此外还讨论了抗体生成和肽生成。\n蛋白质表示学习\r使用氨基酸序列/原子坐标学习一个嵌入从而为其他生成模型创建更丰富的数据空间以供训练。类似于自然语言处理中的word2vec。\n结构预测\r从氨基酸序列来预测结构是极具挑战性和重要的工作。\n任务：从氨基酸序列来预测蛋白质结构。\n数据集：主要来自蛋白质结构预测的关键评估（CASP）。有PDB、CASP14和CAMEO。\n指标：均方根误差RMSD、全局距离测试总得分GDT-TS、模板建模得分TM-score和局部距离差异测试LDDT。\n模型：\nAlphaFold2：里程碑模型。采用端到端的方式集成了多层Transformer，融合多序列比对和成对表示的信息。基于氨基酸之间的成对距离探索折叠空间、氨基酸的潜在取向和整体结构。 trRosetta：transform-restrained Rosetta。输入MSA之后，预测残基对之间的距离和取向，然后利用Rosetta协议构建3D结构。 RoseTTAFlod：在CASP14上表现效果比肩AlphaFold2，特别是生成速度很快，仅需10分钟，相较AlphaFold2快了100倍。 ESMFold：利用ESM-2的输出嵌入到自注意力「折叠块」中，并通过以哦个具有SE（3）的transformer架构的结构模块生成最中国的结构预测。 EigenFold：应用Diffusion生成蛋白质结构的模型。它将蛋白质表示为一个谐振子系统，在正向过程中可以将结构投影到该系统的本征模式上，在反向过程中先采样粗糙的全局结构再细化局部细节。作为一种基于分数的模型，EigenFold 计算强度不高，但在准确性和范围方面仍不如其他模型。 抗体结构预测：\n这里的MSA结构不能作为抗体的输入。因此通用的模型如AlphaFold2效率非常低。\nIgFlod：使用来自AniBERTy的序列嵌入和不变点注意力机制来预测。 tFlodAb：减少了对Rosetta能量函数等外部工具的依赖。 序列生成\r序列生成，也被称为反向折叠或固定骨架设计，是结构预测的逆任务。生成能折叠成目标结构的氨基酸序列，对于设计具有期望结构和功能特性的蛋白质至关重要。由于有效序列的空间巨大，且蛋白质折叠过程复杂难以预测，因此需要多种深度学习方法来解决这些挑战\n任务：给定固定的蛋白质骨架结构，生成能折叠成该结构的相应氨基酸序列。\n数据集：模型主要使用 CATH 进行训练，部分会利用 UniRef 和 UniParc 进行数据增强，评估时常用 CATH 和 TS500。此外，Yu 等人创建了一组 14 个已知的从头蛋白质结构，用于避免数据污染。\n指标：\nAAR（氨基酸恢复率）：生成序列与天然序列中匹配氨基酸的比例。 多样性（Diversity）：通过 Clustalw2 测量生成序列对之间的平均差异。 RMSD（均方根偏差）：将生成序列折叠成结构后，与天然骨架结构进行比较的结构差异指标。 非极性损失（Nonpolar Loss）：衡量折叠结构中极性氨基酸类型合理性的指标，表面非极性氨基酸含量越高，损失越大。 PPL（困惑度）：交叉熵损失的指数化，代表天然序列出现在预测序列分布中的逆可能性。 模型：\n初步的一类模型在不考虑固定骨架目标的情况下生成蛋白质序列。但这些模型无法考虑关键的结构信息。 ProteinVAE 利用 ProtBERT 将原始输入序列转化为潜表示； ProT-VAE 使用不同的预训练语言模型 ProtT5NV； ProteinGAN 则采用 GAN 架构。 主要的模型接收固定骨架目标作为输入来生成氨基酸序列。 ProteinSolver 将生成骨架结构与解决数独问题联系起来，使用 GNN 架构； PiFold 引入更全面的特征表示； Anand 等人设计 3D CNN 直接学习条件分布； ABACUS-R 结合预训练的 transformer 来推断残基的氨基酸类型； ProRefiner 通过引入熵分数改进预测。 GPD 使用 Graphormer 架构， GVP-GNN 采用新颖的几何表示， ESM-IF1 扩展表示并在扩展数据集上训练， ProteinMPNN 实现了顺序无关的自回归方法。 在这些模型中，ProteinMPNN 在序列恢复、RMSD 和非极性损失方面表现最佳，GPD 则是最省时的方法。 主干设计\r生成全新的蛋白质可以直接扩充蛋白质库，实现高度复杂和多样的功能，是从头设计的核心。蛋白质设计在结构和序列上存在差异，有的模型生成 1D 氨基酸序列，有的直接生成 3D 结构，还有的同时设计两者。\n任务：从无输入或基于现有背景设计蛋白质骨干结构，即生成每个氨基酸的骨干原子（氮、$\\alpha$ - 碳、羰基和氧原子）坐标，外部工具可用于侧链填充。包含上下文无关生成（生成多样的蛋白质结构）和上下文给定生成（根据天然蛋白质的基序填充缺失残基）两个子任务。\n数据集：常用的数据集有 PDB、AlphaFoldDB、SCOP（及其扩展 SCOPe）和 CATH。\n指标：\nscTM（自洽 TM 分数）：通过将提议的结构输入序列预测模型（通常是 ProteinMPNN）生成相应氨基酸序列，再将其输入结构预测模型（通常是 AlphaFold2）生成样本结构，计算生成结构与样本结构之间的 TM 分数。分数大于 0.5 的结构通常被认为是可设计的。 scRMSD（自洽 RMSD）：与 scTM 类似，但使用 RMSD 进行评估，分数小于 2 通常作为截止值。 AAR（氨基酸恢复率）：比较生成的氨基酸序列与真实序列的相似程度。 RMSD（均方根偏差）：衡量生成的残基坐标与真实值之间的距离。 模型：\n较短蛋白质 ProtDiff 使用 3D 笛卡尔坐标表示每个残基和粒子滤波扩散方法，但 3D 笛卡尔点云不能反映蛋白质折叠过程； FoldingDiff 则使用角度表示，更接近蛋白质折叠过程中的旋转能量优化，通过 DDPM 和 BERT 架构从随机未折叠状态去噪到折叠结构； LatentDiff 先使用带 GNN 的等变蛋白质自动编码器将蛋白质嵌入潜在空间，再用等变扩散模型学习潜在分布，在潜在空间采样比在原始蛋白质空间快十倍。 长蛋白结构：基于框架的构建方法 Genie 使用由平移和旋转元素确定的框架云进行离散时间扩散来生成骨干结构； FrameDiff 基于框架流形参数化骨干结构，使用基于分数的生成模型； RFDiffusion 结合 RoseTTAFold 的强大结构预测方法和扩散模型，通过微调 RoseTTAFold 权重并输入掩码输入序列和随机噪声坐标来迭代生成骨干结构，还进行自我条件约束，性能优异； GPDL 使用 ESMFold 代替 RoseTTAFold 作为基础结构预测模型，并结合 ESM2 语言模型提取进化信息，生成骨干结构速度比 RFDiffusion 快 10 - 20 倍。 同时设计蛋白质序列和结构 GeoPro 使用 EGNN 编码和预测 3D 蛋白质结构，并设计单独的解码器解码蛋白质序列； Protpardelle 在反向扩散过程中对可能的侧链状态进行 “叠加” 并在每次迭代更新时进行塌缩； ProtSeed 使用三角函数感知编码器计算约束和相互作用，并通过等变解码器更新序列和结构； Anand 等人使用 IPA 在框架空间中进行扩散，高效生成蛋白质序列和结构 。 抗体CDR-H3生成：\n特别地，抗体生成聚焦于一个被称为 CDR-H3 区域的生成。最开始使用的是LSTM方法，后来转变为RefineGNN方法。此外，一些模型超越了CDR-H3生成任务，而是一次性处理抗体生成的多个环节。dyMEAN是一种端到端的方法将结构预测、对接和CDR-H3生成整合到一个模型中。\n多肽设计\r虽然已经有在蛋白质生成方面的重要、强大的模型，但是由于多肽结构的复杂和依赖于上下文已经下游应用的多样性，因此有必要为多肽的需求来定制模型。\n多肽生成：从头生成新型多肽\nMMCD：基于Diffusion的治疗性多肽生成模型，它联合设计多肽序列和结构（骨干坐标），采用Transformer编码器处理序列，EGNN 处理结构，并运用对比学习策略对齐序列和结构嵌入，区分治疗性和非治疗性多肽嵌入。 多肽-蛋白质互相作用：预测提议的多肽 - 蛋白质对的物理结合位点\nPepGB：基于GNN的模型。它利用图注意力神经网络学习多肽和蛋白质之间的相互作用。 多肽表示学习：将原始多肽序列转换为能捕获有价值信息的潜在表示\nPepHarmony：使用序列编码器（ESM）和结构编码器（GearNet），多视图对比学习模型，集成序列和结构信息以增强多肽表示学习。 多肽测序：解决质谱分析中从含噪数据提取氨基酸序列的挑战\nAdaNovo：从头多肽测序模型，由质谱编码器和两个受Transformer架构启发的多肽解码器组成。它利用条件互信息和自适应训练策略，在多种物种的多肽水平和氨基酸水平精度上显著优于之前的模型。 最近趋势\r生成式AI正在深刻地改变药物设计。\n生成式AI领域：GNN和基于图的方法的出现，推动了从基于序列的方法向基于结构的方法的转变，最终促使在生成任务中实现了序列和结构的整合。 分子生成领域：基于图的Diffusion模型作为主导。利用E（3）等变形来实现最先进的性能。 GeoLDM、MiDi——无靶点分子设计 TargetDiff、Pocket2Mol、DiffSBDD——有靶点分子设计 Torsional Diffusion——分子构象生成 此外，有靶点分子设计中也出现了从基于序列的方法到基于结构的方法的出现。 蛋白质生成领域：也出现了从序列到结构的转变。 GearNET：基于结构的表示学习模型 ESM-1B、UniRep：3D结构的重要性 AlphaFold2：结构预测的最先进模型 一些Diffusion方法也致力于蛋白质骨架构建。 挑战\r分子生成领域：\n复杂性 适用性 可解释性 蛋白质生成领域：\n基准测试 性能 结论\r介绍了生成式AI在从头开始的药物设计上的全貌，特别关注分子和蛋白质生成。\nRegularized Molecular Conformation Fields | NeurIPS 2022\rRegularized Molecular Conformation Fields | OpenReview\n从零开始阅读这个领域。先看一下文章的摘要：\n英文摘要\rPredicting energetically favorable 3-dimensional conformations of organic molecules from molecular graph plays a fundamental role in computer-aided drug discovery research. However, effectively exploring the high-dimensional conformation space to identify (meta)stable conformers is anything but trivial. In this work, we introduce RMCF, a novel framework to generate a diverse set of lowenergy molecular conformations through sampling from a regularized molecular conformation field. We develop a data-driven molecular segmentation algorithm to automatically partition each molecule into several structural building blocks to reduce the modeling degrees of freedom. Then, we employ a Markov Random Field to learn the joint probability distribution of fragment configurations and interfragment dihedral angles, which enables us to sample from different low-energy regions of a conformation space. Our model constantly outperforms state-of-the-art models for the conformation generation task on the GEOM-Drugs dataset. We attribute the success of RMCF to modeling in a regularized feature space and learning a global fragment configuration distribution for effective sampling. The proposed method could be generalized to deal with larger biomolecular systems.\r完全看不懂啊。问一下大模型好了。\n以下是对这段话的详细解释：\n模型输出\r“Predicting energetically favorable 3-dimensional conformations of organic molecules from molecular graph plays a fundamental role in computer-aided drug discovery research.”\n从分子图预测有机分子在能量上有利的三维构象，在计算机辅助药物发现研究中起着基础性的作用。这里 “energetically favorable” 指的是从能量角度来说更有利的，也就是说这种构象下分子的能量状态相对较低，更稳定；“3-dimensional conformations” 是三维构象，即分子中原子在三维空间中的排列方式；“molecular graph” 是分子图，一种用图结构来表示分子中原子和化学键的方式；“computer-aided drug discovery” 是计算机辅助药物发现，利用计算机技术来辅助药物的研发过程。\n“However, effectively exploring the high-dimensional conformation space to identify (meta) stable conformers is anything but trivial.”\n然而，有效地探索高维的构象空间以识别（亚）稳定的构象异构体绝非易事。“high-dimensional conformation space” 指的是由于分子中原子的多种可能排列方式所形成的高维度的空间；“(meta) stable conformers” 中 “stable” 是稳定的，“meta-stable” 是亚稳定的（一种相对稳定但不是绝对稳定的状态），“conformers” 就是构象异构体，即具有相同分子式但原子排列方式不同的分子；“anything but trivial” 意思是一点也不简单，绝非易事。\n“In this work, we introduce RMCF, a novel framework to generate a diverse set of low-energy molecular conformations through sampling from a regularized molecular conformation field.”\n在这项工作中，我们引入了 RMCF，这是一个新颖的框架，通过从正则化的分子构象场中采样来生成一系列多样的低能量分子构象。“RMCF” 是一个新提出的框架名称；“novel” 是新颖的；“framework” 是框架；“diverse set” 一系列多样的；“low-energy molecular conformations” 低能量分子构象；“sampling” 是采样；“regularized molecular conformation field” 正则化的分子构象场，“regularized” 在这里可以理解为经过某种规则化处理的，使得构象场更易于处理和分析。\n“We develop a data-driven molecular segmentation algorithm to automatically partition each molecule into several structural building blocks to reduce the modeling degrees of freedom.”\n我们开发了一种数据驱动的分子分割算法，以自动将每一个分子分割成（应该是分割到）几个结构构建块，从而减少建模的自由度。“data-driven” 是数据驱动的，即基于数据来驱动算法的运行；“molecular segmentation algorithm” 分子分割算法；“automatically” 自动地；“partition…into…” 把…… 分割成……；“structural building blocks” 结构构建块，也就是把分子拆解成的一些基本的结构单元；“degrees of freedom” 自由度，在建模中表示可以变化的参数或变量的数量，减少自由度可以使模型更简单、更易于处理。\n“Then, we employ a Markov Random Field to learn the joint probability distribution of fragment configurations and interfragment dihedral angles, which enables us to sample from different low-energy regions of a conformation space.”\n然后，我们使用马尔可夫随机场来学习片段构型和片段间二面角的联合概率分布，这使我们能够从构象空间的不同低能量区域进行采样。“Markov Random Field”（马尔可夫随机场）是一种概率图模型，用于描述变量之间的概率关系；“joint probability distribution” 联合概率分布，即多个变量同时发生的概率分布；“fragment configurations” 片段构型，即分子被分割成的片段的原子排列方式；“interfragment dihedral angles” 片段间二面角，是描述两个片段之间相对取向的角度；“enables sb. to do sth.” 使某人能够做某事。\n“Our model constantly outperforms state-of-the-art models for the conformation generation task on the GEOM-Drugs dataset.”\n我们的模型在 GEOM-Drugs 数据集上的构象生成任务中一直优于最先进的模型。\n“We attribute the success of RMCF to modeling in a regularized feature space and learning a global fragment configuration distribution for effective sampling.”\n我们将 RMCF 的成功归因于在正则化特征空间中的建模以及为了有效采样而学习全局片段构型分布。“attribute…to…” 把…… 归因于……；“regularized feature space” 正则化特征空间；“global fragment configuration distribution” 全局片段构型分布，即对所有片段构型的一种整体的概率分布描述。\n“The proposed method could be generalized to deal with larger biomolecular systems”\n所提出的方法可以被推广应用于处理更大的生物分子系统。“proposed” 提出的；“generalized” 推广，使一般化；“biomolecular systems” 生物分子系统，比如蛋白质、核酸等生物大分子组成的系统。\n目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 解决的问题\r属于上面综述文章里里面的分子生成里面的3D构象生成的部分。\n解决了：给定分子图，预测有机分子能量最有利的3D构象。\n挑战\r分子在三维欧氏空间的 SE (3) 变换下具有不变性，同一分子的构象在刚性运动下有无限可能，增加了建模难度 分子在环境条件下存在多种动力学，导致高维且复杂的势能面，使得机器学习模型难以识别局部最小值来生成能量有利的构象 现有方法使用的不变特征可能冗余、相互依赖，导致数值不稳定和不合理的构象预测，且专门的等变层可能降低神经网络的表达能力，部分模型处理环状图存在困难 核心方法和优势\r核心方法\r构建正则化构象场。依据最少内部自由度（DoF）原则将分子分割。蓝色圆圈代表片段构型，一般来说是低内部柔性的。 利用MRF建模。红色圆圈是二面角构型，黑色方块是相邻构型之间的相互作用。利用马尔可夫随机场（MRF）对片段构型和二面角构型的联合概率分布进行建模。 推理和采样。推理时无环RMCF采用多年动态规划进行最大后验解码，有环使用LBP算法。采样使用Gibbs采样，每次采样后固定其他节点。采样后用特定距离度量样本差异，通过 K-means 聚类，从每个聚类中随机抽取样本，提升生成构象的多样性。 构象组装。将片段和二面角进行组装。 优势\r通过分子切割减少了构象空间维度，避免生产许多无关变量对模型的影响 MRF更好地捕捉相邻片段间的关系并对构象不确定性进行建模 数据集\rGEOM-QM9和GEOM-Drugs数据集。\nGEOM-QM9 评测方式\r分子构象的质量和多样性：覆盖分数（COV-R）和匹配分数（MAT-R） 预测精度：COV-P和MAT-P。 Zero-Shot 3D Drug Design by Sketching and Generating | NeurIPS 2022\r[2209.13865] Zero-Shot 3D Drug Design by Sketching and Generating\n文章属于综述里面的分子生成任务下的靶向分子设计的内容。提出零样本3D药物设计方法DESERT（Drug dEsign by SkEtching and geneRaTing）。\n目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 解决的问题\r目前的药物设计中传统方法和深度学习方法都有很多局限性。\n挑战是什么\r目前的方法都有一些局限性。\n传统方法遍历大规模药物库，耗时且难以产生新的候选药物 现有的深度学习方法依赖稀缺的实验数据，但是蛋白质口袋的生物活性数据大多缺乏，另一些依赖对接模拟，但是这个非常耗时，且准确性不够会影响模型的泛化能力 核心方法\r这里提出的方法是DESERT。把药物设计分成草图绘制和生成两个阶段。\n草图绘制阶段：获取与目标口袋互补的合理的分子形状。 有参考配体时，直接使用配体形状 在无参考配体时，基于生物学观察从蛋白质口袋中采样合理形状\n生成阶段：通过预训练的SHAPE2MOL模型将形状转换为具体的3D分子。 这里SHAPE2MOL将问题形状到分子的生成问题转换为了图像到序列的生成问题。也就是输入3D图像，给出一个序列，表示3D分子。\n内部结构是3D拓展后的ViT结构。\n优势\r减少数据和模拟依赖：DESERT 不严重依赖对接模拟，仅在后期可选使用对接进行后处理，同时抛弃了昂贵的实验数据，通过在大规模分子数据库（如 ZINC 数据库）上训练模型，降低了对实验数据的需求，避免了过拟合问题。 高效快速：相比基于 MCMC 的 GEKO 模型，DESERT 利用生物知识修剪搜索空间，能更快速地找到较好的解决方案，生成速度比 GEKO 快约 20 倍。 生成高质量分子：基于形状的设计方式使 DESERT 能够生成质量更高的分子。 数据集\r训练：使用了ZINC数据中的数据对SHAPE2MOL模型进行训练，包含了10亿对分子及其相应形状的数据。\n评估模型性能：12 种蛋白质（PDB IDs: 1FKG, 2RD6, 3H7W, 3VRJ, 4CG9, 4OQ3, 4PS7, 5E19, 5MKU, 3FI2, 4J71）相关的数据\n评测方式\r设计结果覆盖的分子空间：唯一性（Uniqueness）、新颖性（Novelty）、多样性（Diversity）、成功率（Success rate）和乘积（Product） 高活性分子的能力：通过比较Vina评分的分布，使用 Median Vina Score（Median）来量化分布。 On Pre-trained Language Models for Antibody | ICLR 2023\r[2301.12112] On Pre-trained Language Models for Antibody\n文章属于综述里面的蛋白质生成里面的抗体生成部分。\n目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 解决的问题\r目前的难以探究目前不同的预训练语言模型在抗体任务中的表现。 没有引入生物机制在模型之中。 挑战\r缺乏可靠的抗体特异性基准用于性能评估； 对当前蛋白质预训练语言模型（PPLMs）和抗体预训练语言模型（PALMs）的综合研究不足； 难以判断引入生物机制是否能真正有益于抗体表示学习； 确定预训练表示在实际应用（如药物发现和免疫过程理解）中的作用存在困难 预训练蛋白质语言模型PPLMs：\n利用蛋白质序列探索大语言模型。\n如ProtTrans和ESM-1b将单个蛋白质序列作为输入，使用Transformer架构进行预训练。 MSA-Transformer/MSA-1b模型通过多序列比对（MSA）作为输入。在结构预测方面，该模型优于 ESM-1b，这表明进化信息有助于蛋白质表征学习。 预训练抗体语言模型PALMs：\nAntiBERTy：提出首个抗体特异性语言模型，对在OAS数据库中的5.58亿条天然抗体序列使用Transformer架构进行预训练。 Abalang-H/L：恢复抗体序列中缺失的残基上的迁移学习。 AntiBERTa：在OAS数据库上预训练，并进行微调以用于抗原结合位点位置预测。 核心方法\r提出了抗体理解评估（AnTibody Understanding Evaluation，ATUE）基准和包含特定进化信息的EATLM模型。\n创建了一个全面的抗体基准测试工具ATUE。 抗原结合预测：二分类序列分类任务，确定抗体的CDR区域能否与特定抗原结合。 原因：通过对抗体 CDR 区域的分析，预测其与特定抗原的结合情况，有助于筛选出具有潜在治疗效果的抗体。 互补决定区预测：确定抗体序列上的结合位置的序列标注任务为 CDR 片段的每个残基预测 0/1 标签。 原因：确定抗体与抗原的结合位置，有助于深入理解抗体与抗原的相互作用机制。 B 细胞成熟分析：是一个 6 分类任务，区分 B 细胞抗体序列的成熟阶段，每个序列属于 {未成熟、过渡、成熟、浆细胞、记忆 IgD+、记忆 IgD-} 中的一种。 原因：有助于理解免疫进化过程中的机制。 抗体发现：是一个二分类序列分类任务，区分哪个抗体直接对 SARS-CoV-2 结合负责。 原因：从大量抗体中找出能与 SARS-CoV-2 结合的抗体，对于开发针对该病毒的治疗方法意义重大。 得出了关键观察结果，提供了如何更好地表示抗体的一些指导方针。 PPLMs 在与结构高度相关的抗体任务中表现良好，但在具有高抗体特异性的任务中表现不佳 在大多数情况下，PALMs 在预训练数据较少时表现得与 PPLMs 一样好甚至更好 通过结合进化过程可以改进 PALMs，但来自 MSA 的进化信息并不总是对抗体任务有益 探究引入生物机制对模型的影响，提出EATLM模型。 在传统掩码语言建模（MLM）的基础上，引入两个新的预训练目标以模拟抗体进化的生物机制。 祖先种系预测AGP 突变位置预测MPP 优势\rATUE基准涵盖了多个具有不同特异性的真实任务，能更加全面地评估模型。\n对于EATLM模型来说，引入了AGP和MPP两个预训练目标之后，有以下优势：\n抗原结合预测：对AUC和F1指标有改进 表达预测：在F1和MCC指标上优于其他模型 B细胞成熟分析任务：显著优于其他PALM模型 抗体发现任务：是识别所有钱再结合物最有效的方法，确定了11种潜在的SARS-CoV-2结合抗体，展示了在实际应用中的潜力。 数据集\r抗原结合预测 Mason等人（2021）的数据集 互补决定区预测 使用从 Liberis 等人（2018）收集的包含 1662 个 CDR 片段的数据进行研究。由于只有部分抗体来自进化，所以该任务具有中等特异性。 B细胞成熟分析 数据集来自 Mroczek 等人（2014），有 6 个成熟阶段的 88094 个序列。特异性高，抗体进化与 B 细胞成熟高度耦合。 抗体发现 研究人员收集了 133 名 SARS-CoV-2 患者和 87 名健康人的抗体序列，按照特定流程处理数据，并与 CoV-AbDab 数据库中的序列匹配，以确定潜在的结合抗体。由于来自同一疾病的抗体具有强烈的趋同种系信号，所以该任务特异性高。 评测方式\r对于不同任务，分别进行评估：准确度ACC、马修斯相关系数MCC、F1值、AUC。\nLearning Harmonic Molecular Representations on Riemannian Manifold | ICLR 2023\r[2303.15520] Learning Harmonic Molecular Representations on Riemannian Manifold\n这篇文章所属的领域属于分子生成领域中的表示学习。\n解决的问题\r现在的基于欧几里得空间的分子表示方法需要借助等变网络保证分子在表示旋转和平移的时候的一致性，这里的等距变换群就是E(3)/SE(3)。 目前的分子表示学习多采用自下而上，难以提供不同分辨率的特征。 挑战\r设计一种绕过等变要求，并且能够准确编码3D分子结构的表示。 开发在不同分辨率下为不同任务提供合适特征的多分辨率消息传递机制，特别是复杂的大分子。 核心方法\r用分子表面的拉普拉斯 - 贝尔特拉米（Laplace - Beltrami）特征函数来表示分子，在 2D 黎曼流形上实现多分辨率的分子几何和化学特征表示，并引入谐波消息传递方法进行高效的谱消息传递。\n大模型的解释：\r改变表示空间：传统方法在 3D 欧几里得空间中编码分子结构，为保证分子表示在旋转和平移时的正确性，需要借助等变网络。而该方法将分子表示在 2D 黎曼流形上。可以把黎曼流形想象成一个可以弯曲、变形，但局部性质类似欧几里得空间的特殊空间。在这个空间上，分子的表示天生就具有旋转和平移不变性。就好比把分子放在一个有弹性但又有自身规律的 “网” 上，无论分子怎么旋转、平移，这个 “网” 对分子的描述都不会改变，不需要额外的等变网络来调整，从而绕过了等变要求。 利用拉普拉斯 - 贝尔特拉米特征函数：分子表面可看作黎曼流形，拉普拉斯 - 贝尔特拉米（LB）特征函数是这个流形的固有属性。不同的分子表面有不同的 LB 特征函数，它们就像分子的 “指纹”，能反映分子的形状和结构特点。这些特征函数在刚性变换下保持不变，所以可以用来准确编码分子结构。例如，我们可以把分子表面想象成一个有很多不同纹理的曲面，LB 特征函数就像是描述这些纹理分布规律的工具，不管分子怎么转动、移动，这些纹理的分布规律是不变的，通过分析这些规律就能准确编码分子结构。 多分辨率表示与信息传递：通过对 LB 特征函数的线性组合，可以实现分子表面的多分辨率表示。不同频率的 LB 特征函数可以捕捉分子不同尺度的特征，低频部分反映分子的整体、大致的形状，高频部分则能体现分子的细节特征。在进行信息传递（类似消息在分子表面不同区域传播）时，利用基于 LB 特征函数构建的谐波消息传递机制，能在不同尺度上传播信息。这就好像在一个城市中，有不同规模的道路来传递信息，主干道（低频特征）传递整体的、大致的信息，小巷（高频特征）传递详细的、局部的信息，从而全面、准确地编码 3D 分子结构 优势\r在2D黎曼流形上的分子天然具有旋转和平移不变性，无需依赖等变网络 采用自上而下的方式，能提供多分辨率特征 分子形状定义了黎曼流形，原子构型决定流形上的相关函数，更全面反映分子性质 数据集\rQM9：小分子性质回归任务 配体结合口袋数据集，数据划分方式参考这里 刚性蛋白质对接数据集作为训练集，Docking Benchmark 5.5作为测试集。 评测方式\rQM9 小分子性质回归：通过计算预测结果与真实值的平均绝对误差（MAE）来评估模型性能，对比其他不变性和等变网络模型，如 SchNet、NMP 等。 配体结合口袋分类：使用平衡准确率评估模型预测蛋白质口袋结合配体类型的能力，与 MaSIF-ligand 模型对比。 刚性蛋白质对接：采用 Complex RMSD、Interface RMSD、DockQ 和成功率等指标评估对接性能。Complex RMSD 和 Interface RMSD 衡量预测结构与真实结构的偏差；DockQ 是基于多个标准化标准的综合评分；成功率表示预测结果达到 “可接受” 或更高水平的比例 。 Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D | ICML 2023\r[2305.13266] Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D\n文章属于综述里面的分子生成中的3D构象生成。\n解决的问题\r现有的3D分子生成方法在生成大尺寸分子的时候存在结构质量差的问题。\n挑战\r自回归模型：按照人工设定的顺序逐个生成原子，如同语言生成过程。但分子在 3D 空间具有自然的几何结构，这种方法引入的人为顺序与分子的自然结构不匹配，并且会产生规模和误差累积问题。 非自回归模型：原先的原子级生成方法虽然灵活性较高，但是在片段级生成时，由于化学价态限制，片段冲突常见，且避免片段冲突的复杂度高，随着结构尺寸增加，复杂度呈指数上升，难以获得可靠的分子结构。 核心方法\r提出基于分层Diffusion的分子生成方法Hierarchical Diffusion-based 模型（HierDiff）。\n将3D分子生成问题看作约束生成问题。\n先通过等变Diffusion过程生成粗粒度分子几何结构，其中每个粗粒度节点反映分子中的一个片段 通过消息传递过程和一个新设计的迭代细化采样模块，将粗粒度节点解码为细粒度片段 将细粒度片段组装起来，得到完整的原子分子结构 详细解释\r粗粒度片段扩散：HierDiff 将 3D 分子生成视为约束生成问题，首先定义粗粒度节点的表示，包括不变化学特征和等变位置特征。通过精心设计化学特征（如基于属性和元素的特征）和位置特征（使用中心坐标），利用扩散模型生成粗粒度片段表示及其笛卡尔坐标。在这个过程中，通过特殊设计的初始分布和转移核，保证模型的 SE (3) 不变性，从而有效生成合理的粗粒度分子几何结构。 细粒度片段生成：基于生成的粗粒度节点，通过一系列步骤生成细粒度片段类型和边。具体包括选择焦点节点、预测新边、确定细粒度片段类型以及迭代细化。利用消息传递神经网络和迭代细化模块，不断纠正细粒度节点中的偏差，提高生成分子的真实性。 组装成原子构象：根据细粒度生成过程确定的节点和链接关系，选择合适的原子合并方式构建原子级构象。利用 RDkit 生成局部构象，并通过 Kabsch 算法计算旋转矩阵和平移向量，将局部构象对齐到采样的中心位置，逐步生成完整的原子构象。 优势\r与同类方法相比，HierDiff 能保持非自回归方法的全局建模特性，显著降低寻找可连接片段的复杂度，有效避免片段冲突，生成的分子更具现实性和药物样属性，在多个评估指标上优于现有方法。\n数据集\r使用GEOM-Drugs和CrossDocked2020数据集。\n评测方式\r药物相似性 QED 逆向合成可及性RA 药物化学过滤器MCF 合成可及性分数SAS LogP $\\Delta$LogP 分子重量MW 构象质量 计算覆盖度Cov 匹配度Mat Accelerating Antimicrobial Peptide Discovery with Latent Structure | SIGKDD 2023\r[2212.09450] Accelerating Antimicrobial Peptide Discovery with Latent Structure\n这篇文章属于综述下面的蛋白质生成下的肽设计部分。\n解决的问题\r加速抗菌肽（AMP）的发现。现有的深度学习模型只考虑序列属性，但是忽略了结构对于活性的关系。\n挑战\r目前深度学习模型只考虑序列特征，忽略结构-活性关系。结构对于肽的活性具有重要影响，但是却没有应用。\n核心方法\r提出 Latent Sequence-Structure 模型（LSSAMP）。\n将序列特征和二级结构映射到潜空间中。 采用VQ-VAE为每个位置分配一个潜在变量 通过在潜空间采样生成理想序列组成和结构的肽 优势\r考虑了序列和结构信息，肽的抗菌性更强\n数据集\rUniversal Protein Resource（UniProt）中的蛋白质序列和通过ProSPr预测的二级结构 Antimicrobial Peptide Database中的抗菌肽数据集 评测方式\r自动评估指标 使用开源的AMP预测工具估计生成序列的AMP概率。 依据电荷、疏水性、疏水矩这三个对AMP机制至关重要的序列属性评估生成性能。 通过唯一性、多样性和相似性来衡量生成的肽的新颖性。 湿实验室实验 通过上面的自动评估指标从5000个肽中筛选出21个肽进行合成实验 合成了之后在培养皿中采用肉汤微量稀释法测定最小抑菌浓度（MIC）,从而验证。 ","date":"2025-02-26T22:17:58+08:00","permalink":"https://ionfeather.github.io/2025/ai-assist-drug-design/","title":"AI Assist Drug Design"},{"content":"一直想用一些shortcodes来优化一下页面的呈现方式。\n特别感谢：\n在 Stack 主题上可行的短代码们 一些Hugo短代码 Hugo |另一篇Stack主题装修记录 | 小球飞鱼 Hugo | 在文章中插入轮播图片 | 小球飞鱼 文字\r重点标记\r好喜欢蓝色！\r文本折叠\r点击展开\r这是第一个段落的内容。\n这是第二个段落的内容，位于折叠部分，实际使用别忘了双括号！\n文字黑幕\r数据删除！数据删除！\n但总之换行的话就加个空标签。\n高斯模糊\r一些手动打码效果！\n但总之换行的话就加个空标签。\n文本位置\r文字居左\n文字居中\n文字居右\n摘录引用\r羊皮卷上所载一切自永远至永远不会再重复，因为注定经受百年孤独的家族不会有第二次机会出现在大地上出现。\n加西亚·马尔克斯\r《百年孤独》 居中引用\r有些人说\n换行敲多了\n就是\n诗\n键盘样式\rCtrl+Alt+Del\n卡片\r可以在这里插入链接假装是卡片式链接。\n好像不能插入图片？\n换行需要空标签。实际使用需要双括号。\n标签块\rWarning：需要双括号。\ninfo：这是一条信息。\nnote：可以标注一下，但是没必要。\ntip：在示例里胡说八道会使观看者会心一笑。\n对话框\r导师\u0026nbsp;\u0026nbsp;\u0026nbsp;2024-10-12 14:30\r做一个论文阅读的大模型。 2024-10-12 14:45\u0026nbsp;\u0026nbsp;\u0026nbsp;我\r好的老师。 时间轴\r2024-10-20\r博客\r创建ionfeather\u0026#39;Log\r使用Hugo的Stack主题\r2024-11-04\r博客\r增加评论区\r部署Waline，但还有一些问题\r2025-02-13\r博客\rBug修复\r修复了一些bug，如邮箱显示错误、搜索功能失效等\r图片滚动\rBilibili\r网易云音乐\r标签墙\r其实这个不算是shortcodes，但是我不知道把这个放在哪里。所以这个犄角旮旯就不错！\n特别感谢：\nHugo | 月球基地开发历程 Blog | 主题重新施工，和书影游展示墙 | 小球飞鱼 标签墙最后放在了我的「关于」页面里。\n欢迎大家去看！\n","date":"2025-02-18T13:40:27+08:00","permalink":"https://ionfeather.github.io/2025/shortcodes/","title":"搬运一些Shortcodes"},{"content":"论文阅读 | 多智能体协作机制：大语言模型综述\r[2306.03314] Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents\n摘要\r随着大语言模型（LLMs）的最新进展，代理式人工智能（Agentic AI）在现实应用中取得了显著进展，朝着基于多个大语言模型的智能体迈进，实现感知、学习、推理和协同行动。这些基于大语言模型的多智能体系统（MASs）使得一组智能体能够协作解决复杂任务，并以大规模方式实现集体行动，从孤立的模型转向以协作为核心的方法。\n本文提供了关于多智能体系统协作方面的广泛综述，并提出了一个可扩展的框架来指导未来的研究。我们的框架根据关键维度对协作机制进行表征：参与者（涉及的智能体）、类型（例如，合作、竞争或合作竞争）、结构（例如，点对点、集中式或分布式）、策略（例如，基于角色或基于模型）以及协调协议。通过对现有方法的回顾，我们的研究成果为揭示和推动基于大语言模型的多智能体系统向更加智能和协作的解决方案发展，特别是在复杂的现实应用中，提供了基础。\n此外，本文还探讨了多智能体系统在不同领域的各种应用，包括5G/6G网络、工业5.0、问答系统、以及社会文化环境，展示了它们的广泛应用和更深远的影响。最后，我们总结了关键经验教训，分析了多智能体系统面临的开放挑战，并指出了朝着人工集体智能发展的潜在研究方向。\n文章大纲\r应用\r方法 领域 主要贡献 优点 缺点 参考文献 LLM-SC 物联网 作为知识生成器增强语义解码器 利用大语言模型，实现显著的编码增益 由于使用大语言模型，计算资源需求高 [130] LaMoSC 物联网 提出一种大语言模型驱动的多模态融合语义通信 在低信噪比条件下表现稳健 由于使用大语言模型和视觉 Transformer，计算资源需求高 [157] LAM-MSC 物联网 为多模态数据设计联合编码器；大语言模型作为知识生成器 一个编码器和解码器可处理多种类型的数据；实现更好的编码率和重建误差 由于使用大语言模型，计算资源需求高 [65] GMAC 物联网 利用大语言模型实现观察状态与自然语言之间的语义对齐，并压缩语义信息 提高收敛速度；实现无通信的多智能体协作 由于使用大语言模型，计算资源需求高 [160] LLM-Blender 自然语言生成 采用多种大语言模型代理的集成方法进行候选排序 能够生成比现有候选更好的输出 为实现最优解，需要进行 O (n) 次推理，导致计算开销大 [64] SOT 自然语言生成 并行生成每个答案框架；完成答案内容（需要规划结构） 通过并行加速推理速度；适用于需要长结构答案的问题 答案质量评估远非完美，由于提示集有限；不同代理的并行请求可能会影响服务吞吐量 [95] Meta-Prompting 自然语言生成 构建高级元提示来指导大语言模型 保持连贯的推理思路；挖掘各种专家角色 多次模型调用成本较高；需要大量的规模和相当大的上下文窗口 [119] MAD 自然语言生成 两个代理表达各自的论点；一个评判者监控和管理辩论 减少偏差和扭曲的认知；鼓励无限的外部反馈 由于辩论时间长，计算成本高；大语言模型在长场景中难以保持连贯性和相关性 [77] FORD 自然语言生成 包括三个阶段的辩论：公平辩论、不匹配辩论、圆桌辩论 通过辩论让大语言模型探索自身理解与他人概念化之间的差异 除常识推理外，无法涵盖各种任务；严重依赖多项选择任务，限制了其泛化能力 [140] ChatDev 自然语言生成 采用聊天链将每个阶段分解为更小的子任务，实现代理之间的多轮通信，以协作开发解决方案 最大限度减少代码幻觉（提供的源代码缺失的情况） 没有清晰、详细的要求时，代理难以理解任务想法；通用软件的自动化评估非常复杂；多个代理需要更多的令牌和时间，导致计算需求大 [105] AgentVerse 自然语言生成 由专家招募、协作决策、行动执行、评估四个阶段组成 提高大语言模型在不确定情况下的泛化能力；提高代理的适应性 协作决策过程中代理之间的通信存在挑战 [24] AgentCoord 社会与文化领域 为协调策略提供结构化表示；采用三阶段方法将一般目标转化为可执行策略 简化协调策略的表示和探索；最小化代理的重复实例 仅支持在纯文本环境中协调代理协作；仅支持静态协调策略设计 [97] OpenAI\u0026rsquo;s Swarm 自然语言生成 用于多智能体编排的例程和交接；轻量级协调与执行框架 适用于需要可扩展性的应用；交接机制允许在专门代理之间实现无缝过渡 主要关注基于角色的协议和集中式 / 分布式结构；尚未准备好投入生产 见原文 TE 社会与文化领域 在主题研究中模拟人类参与者的代表性样本 能够模拟不同的人类行为，并揭示模拟中的一致偏差 需要研究更多的人类行为和额外的大语言模型，以确保关键发现的准确性 [36] AgentInstruct 社会与文化领域 通过迭代的跨代理细化生成多样化的自然语言数据，包括文化数据 能够通过工具使用、代理能力等从生成的数据中训练更强大的模型 需要人工构建生成流程 [88] SocialMind 社会与文化领域 整合言语、非言语和社交线索，通过增强现实眼镜生成现场建议 设计并利用多模态、多层协作代理系统 需要先进的边缘硬件来处理复杂系统 [144] CulturePark 社会与文化领域 促使基于大语言模型的代理进行跨文化交流模拟 生成的数据可用于训练具有不同文化背景的模型，减少偏差并实现民主化 仍然依赖大语言模型对每种文化的了解，因此对资源较少的文化效果有限 [73] Mango 社会与文化领域 通过对概念和文化的提示，从基于大语言模型的代理中提取高质量知识 自动化方法可生成大量资源 人类评估需要来自更多样化的背景 [94] 六个思考帽的设计\r白色思考帽\r功能：收集客观信息。 实现方式 对论文进行解析。 从论文文本中抽取结构化数据。 从网络中搜索作者之前的研究成果。 从网络中搜索同类研究的对比数据。 绿色思考帽\r功能：对论文提出创新性改进，探索论文的可能性 实现方式 未定。 黄色思考帽\r功能：积极角度评估论文，找出论文的优点和贡献。 实现方式 用优点和创新点微调后的大模型。 黑色思考帽\r功能：批判性思考，找出论文的问题和不足。 实现方式 用批判性数据集微调后的大模型。 红色思考帽\r功能：主观感受和直觉判断。 实现方式 让智能体多阅读论文，找到好的论文之间的共性和形成自己的「偏好」。 蓝色思考帽\r功能：控制评审流程。 实现方式 未定。 蓝色思考帽智能体应该如何控制？\r基于工作流管理的集中式控制方法。蓝色智能体明确规定了其他智能体的工作顺序、时间和交互方式。 基于协商机制的分布式控制方法。在评审开始时，蓝色智能体发起评审任务，各思考帽智能体根据自身能力和状态反馈可承担的工作及预计时间。比如白色告诉蓝色需要5分钟完成，绿色说在白色完成后需要10分钟\u0026hellip;通过这些反馈，蓝色智能体来制定计划。 基于事件驱动的动态控制方法。不同的智能体换成之后会触发不同的事件，如白色完成后让绿色工作，黑色和黄色在辩论后无法达成共识，就再次进行辩论等。这个事件定义较难。 ","date":"2025-02-15T21:58:31+08:00","permalink":"https://ionfeather.github.io/2025/multiagentcollaboration/","title":"论文阅读 | 多智能体协作机制：大语言模型综述"},{"content":"雨水：表示降水开始，雨量逐步增多。雨水节气天气变化不定，是全年寒潮过程出现最多的时节之一，忽冷忽热，天气乍暖还寒。\n北京最近的天气确实是这样。在北京能感受到24节气的准确，能感受到四季分明是什么感觉。\n但是！！雨水，雨水，北京来点儿雨吧。自从去年的12月以来，我没有见过一滴雨落到北京的地表，这里的晴天就像是默认背景，太阳和月亮每天都是固定角色出现在地平线和天空中。北京的「雨水」是艳阳高照。\n絮絮叨叨\r上了研究生学术没有做多少，兴趣爱好培养了不少——台球、博客、摄影、健身…可能还打算学个吉他和乒乓球。忙不过来，实在是忙不过来了。还是得多放点时间在学习上呀。\n今天是情人节，这么一想，晚上的健身房应该会比较空，可惜昨天跑步跑太狠，把脚掌磨出了一个水泡，走路都有点儿疼，今晚回去可以做做力量训练，就先不做有氧了。\n好羡慕甜甜的爱情。\n2025-02-14 15:02\n买了辆电瓶车，之后出行方便多了。 终于不用来回奔波那么久了。 去地铁站终于不用思考用共享单车还是公交车了。\n2025-02-14 21:58\n今天把原先的房子里的东西全部打扫干净，让它恢复成原来的样子。我才意识到，虽然住了半年，但是我们在这个房子里的印记能在一天之内被打扫得无影无踪。原来人是流动的水。\n和室友买了一些做饭的家伙，之后可能会在出租房里（我其实也愿意称它为「家」）做一些简单的菜。室友的女友可能过两天会来，到时候可以期待一下她的厨艺（好像听说也是新手，那还是期待我自己的进步吧\n上了称，感觉自己胖得不行了。但是过两天又是组会，头疼，我这周什么也没干。\n2025-02-15 19:26\n房东来看房，她（果然）嫌弃我们打扫得不干净，表示要找保洁来清理。MSY、SH和我商量了一下，最后让她扣了300元。我该怎么说——其实这个人还算好说话。反正扣完了之后也没说什么，现场就转了钱。我本来还以为她会拿门钥匙这件事说事。\n帽子到啦！我的MBTI帽子到了，我直接往上贴了一个「ENFP」，然后周游工位告诉每个见到我的人，可惜今天是周日，没见到几个人。大家都去哪儿啦？\n不过感觉自己也不用担心（什么担心，我这叫好奇）大家都去哪儿了。明天就是组会，目前还没有进展，现在就开始看论文吧！\n2025-02-16 13:35\n听说有个师姐被求婚了，被求婚了？被求婚了！看到了照片，男帅女靓啊，又是羡慕别人爱情的一天。\n不过我还是有一点儿恍惚：原来这也是我这个年纪该听到的事情吗？我还以为自己还小。问了一下同门，她说她也不介意研究生读完就结婚，我开玩笑地说：「那明年可以暗示你一下你的男朋友了。」\n对我来说，可能还是有点早了吧？我心里还没准备好。但是这种事情谁说得好呢？\n2025-02-16 22:33\n开完组会了。\n最近组会的气氛比较轻松愉快。但是在我看到同门和老师写的一篇论文之后，还是有些不淡定——我还没有将idea实现、落地的能力。在上面散发着新鲜油墨气味的论文被交到我手上的时候，我内心还是有些无奈。\n我也不想安慰自己说未来也能写出来，我对这件事情甚至没有任何认知。写一篇论文到底需要什么？就我来说，和如何把冰箱里的大象拿出来一样，是一个全新的领域。\n2025-02-17 17:07\n又是10点才起床的一天。\n室友的女友来我们这里借住几天，没想到是个精力十足、爱笑、笑起来是「嘿嘿嘿」的山东女生。她的笑声确实听起来很愉快，能够感受到她很开心，听起来穿透力很强。没想到这么响亮的笑声在之前都没听过，看来之前在出租屋的小房间里，她也是忍耐住了自己的笑（笑\n中午的时候，她和室友下厨，做了油泼面，吃着还可以，比学校的面也不遑多让——或许是学校食堂太糊弄。我拿了我的碗，刷刷刷吃了一碗半。\n晚上又碰见他们了。室友和他女友在跑步机旁边，室友跑步，女友爬坡。\n2025-02-19 23:35\n昨天去中国电影博物馆看了《哪吒2之魔童脑闹海》。电影在15:25开始，我是在12:00出发，但是博物馆的位置确实有点儿偏——也可能是我的位置太偏——我到达那里已经是14:00了。\n走马观花地参观了一下中国电影博物馆的展厅，进门是主旋律正能量主题展厅。往里走，里面有一个巨大的环形的展厅，地板、墙壁都是LED屏幕，一共有四层楼高，有一个环形的缓缓上升的参观路线，绕着墙壁，通向其他二楼三楼的展厅。\n在展厅里，对我这个电影盲来说，只能看到那些最脍炙人口的电影，比如《警察故事》《小蝌蚪找妈妈》，才会知道「啊！原来是它！」\n有没有书的展览馆！搜了一下文学没有像是中国电影博物馆一样这种大而全的，但是专题类的很多：中国现代文学馆、北京鲁迅博物馆、老舍纪念馆、巴金故居、上海文学博物馆\u0026hellip;感觉又种草了不少。\n不愧是IMAX GT屏幕，真的很壮观。我坐得特别前面，还有点儿偏，哪吒也并非最适合它的1.43:1的屏幕比例，因此上下还是有黑边，《奥本海默》和《沙丘》会更适合它。但，但是（申公豹式强调）这不妨碍我看电影的时候感慨画质和细节，云雾、锁链、粒子\u0026hellip;在我正前方130°范围里，全都是哪吒。\n电影好！屏幕好！看电影的人好！真是一次美好的体验。\n2025-02-21 21:30\n有的时候也很焦虑诶。又是一天什么也没做。昨天给妙妙讲了讲道理——学习就像是跑仓鼠的轮子，滚起来就会一直向前。\n我也该把我的轮子滚起来了。\n2025-02-22 21:15\n朋友的生日，唱歌从11点唱到凌晨4点。\n五个人算是把自己拿手的歌全唱了——我发现我以前的五音不全病似乎有好转的迹象——《思念是一种病》《一事无成的伟大》我宣布现在已经是我的拿手曲目了。\n2025-02-28 12:00\n","date":"2025-02-14T14:36:39+08:00","image":"https://ionfeather.github.io/2025/rainwater/cover_hu14924232115569819230.jpg","permalink":"https://ionfeather.github.io/2025/rainwater/","title":"雨水 | 北京的「雨水」是艳阳高照"},{"content":"春节快乐！\n工科研究生的假期有点短暂了。我已经回校了。\n到学校了感觉自己好多东西需要购置。Apple Watch的表带现在明显太松了，但是官方太贵，第三方又有点儿硬，计划看一下Bilibili上的测评，进行一波购置。还有看上了影视飓风的一款帽子，上面可以贴上你的mbti，这对我这个enfp根本无法拒绝。\n又来到了北方，又遇到了高铁上一望无际的平原，感觉是另一种大海，在这片海里，有冰封的河流，有枯黄的树枝，还有炊烟和蜗居的人们。有一种说法是南方人向往雪，北方人向往海，这么一看，我应该是个不那么彻底的南方人。我两个都很向往。\n今天是元宵节，我本科的时候是灯谜社社长。元宵节，英语叫Lantern festival，也就是灯节，是灯谜社最重要的节日。在传统文化节里，我会张贴灯谜，擂起鼓，在鼓声滚滚中，同学需要猜出我出的谜题——有些是我们社团自己写的，所以很难猜，需要脑子有点儿回路，特别是与英雄联盟或者是与本科学校相关的那种谜题。\n虽然现在天气还很冷，但是感觉春意渐浓，即便树梢仍然枯枝，阳光灿烂的时候也能把寒意给驱散。\n我已经开始期待春天了。玉兰、银柳、山茶、樱花、樱桃花、木绣球、垂丝海棠。我要抄起我的相机，出门拍花拍鸟。\n不管怎么说，虽然已经上了几年（学校一年，外界一天）班了，现在还是春节期间，我一会儿说不定可以出门，去颐和园旁边拍花灯。\n","date":"2025-02-12T17:51:19+08:00","image":"https://ionfeather.github.io/2025/lanternfestival/cover_hu10283662063726621985.jpg","permalink":"https://ionfeather.github.io/2025/lanternfestival/","title":"元宵节 | 东风夜放花千树，更吹落、星如雨"},{"content":"之前一直在用Typora来写文章，发现有的时候也太难用了，不仅插件少，还要付费。这个时候看到很多的博客都用Obsidian来写，不得不心动了。\nObsidian的插件\rLinter\rObsidian Linter插件：打造统一、美观的笔记环境 - 知乎\n我不得不赞赏一下这个Linter，真的很好用，格式化目前的Markdown内容一直是我的心头痒，对于我这个强迫症来说，现在只需要按一下Ctrl+S就可以让我的敲击的内容都非常规范化，这实在是伟大的发明。\nExcalidraw\r这个插件还挺好看的，可以绘制手绘风格的图像，我绘制一些想法会更加方便。\n其他\rAdvanced Tables：对写表格比较有帮助。 Customizable Menu：自定义右键快捷键。 ","date":"2025-02-12T15:59:30+08:00","permalink":"https://ionfeather.github.io/2025/obsidian/","title":"使用Obsidian来写博客"},{"content":"全书结构\r预备知识\r张量\r张量表示一个由数值组成的数组，这个数组可能有多个维度。\n具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。\n张量的创建\rimport torch x = torch.arange(12) x.shape x.numel() X = x.reshape(3, 4) torch.zeros((2, 3, 4)) torch.ones((2, 3, 4)) torch.randn(3, 4) torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) 运算符\r按元素运算\r常见的运算符这里用作按元素运算。\nx = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y # **运算符是求幂运算 可以得到\n(tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) 还有很多的一元运算符都可以用在按元素运算。\n线性代数运算\r求和/平均值\r直接调用sum函数，会将其变成一个标量，也可以指定axis = 1维度来指定轴来进行降维。\nA.sum() A.sum(axis = 1) A.sum(axis = [0, 1])# 对于矩阵来说，相当于A.sum() 同理，A.mean()也是一样的。\n如果希望能够在求和或者平均值的时候保持轴数不变，可以使用keepdims = True。\n如果希望能够沿着某个轴计算A元素的累计总和，可以使用cumsum函数。\nsum_A = A.sum(axis=1, keepdims=True) A.cumsum(axis=0) 点积\rx = torch.arange(4) y = torch.ones(4, dtype = torch.float32) torch.dot(x, y) 矩阵-向量积\r当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。\n矩阵-矩阵乘法\r我们可以将矩阵-矩阵乘法AB看作简单地执行m次矩阵-向量积，并将结果拼接在一起，形成一个n×m矩阵。\n在下面的代码中，我们在A和B上执行矩阵乘法。 这里的A是一个5行4列的矩阵，B是一个4行3列的矩阵。 两者相乘后，我们得到了一个5行3列的矩阵。\nB = torch.ones(4, 3) torch.mm(A, B) 张量连结\r在这里，dim=0说明是第一个维度进行拼接；dim=1说明是第二个维度进行拼接。\nX = torch.arange(12, dtype=torch.float32).reshape((3,4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) 广播机制\r特别需要注意这个，可能会导致错误发生。\na = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a, b 由于a和b分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的3×2矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。\n索引和切片\r与Dataframe中相似。\n节省内存\r如果直接使用X = X + Y就是重新创建一个元素。但是，有些时候希望执行原地操作。\n如果希望执行原地操作的话，可以使用两种方式，此时不会占用新的空间：\nX[:] = X + Y X += Y 转换为其他对象\r转换为Numpy非常容易：A = X.numpy()\n转换为Python标量：a.item()或者使用内置函数float(a)等。\n自动求导\r自动求导是计算一个函数在指定值上的导数。\n如何实现？ 计算图：将代码分解成操作子，将计算表示成一个无环图。 关于计算图，有显式构造 vs 隐式构造两种构造方式。\n特性 显式构造 隐式构造 计算图构建方式 显式定义 隐式定义 计算图类型 静态图 动态图 典型框架 TensorFlow 1.x, Theano PyTorch, TensorFlow 2.x (Eager) 有两种求导的方式，对于一个链式法则，我们可以采取正向累积和反向累积（也称反向传递）。\n示例说明\n以 \\( y = (x_1 + 2x_2)^2 \\) 为例：\n反向传递： 前向计算 \\( z=11, y=121 \\)。 反向计算 \\( \\partial y/\\partial z=22 \\rightarrow \\partial y/\\partial x_1=22, \\partial y/\\partial x_2=44 \\)。 正向传递： 前向计算 \\( z=11 \\)，同时记录 \\( \\partial z/\\partial x_1=1, \\partial z/\\partial x_2=2 \\)。 前向计算 \\( y=121 \\)，同时记录 \\( \\partial y/\\partial z=22 \\)。 直接组合导数得到 \\( \\partial y/\\partial x_1=22 \\times 1=22 \\)，\\( \\partial y/\\partial x_2=22 \\times 2=44 \\)。 反向累积\r使用反向传递的时候，在我们计算$y$关于$x$的梯度之前，需要一个地方来存储梯度。\n重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量$x$的梯度是向量，并且与$x$具有相同的形状。\n","date":"2025-01-08T16:00:34+08:00","image":"https://ionfeather.github.io/2025/d2l-01/assets/cover_hu3492737127834985036.png","permalink":"https://ionfeather.github.io/2025/d2l-01/","title":"《动手学深度学习》"},{"content":"虚拟环境配置经历\r我之前配置好了一个虚拟环境名为vllm，专门用于vllm的启动，我还特意将其中的虚拟环境中的所有包的版本保存到vllm_requirements.txt文件中。\n但是我一顿操作之后，原本配置好的环境现在也没办法使用了。此时我庆幸自己想到用vllm_requirements.txt文件保存。但是在进行pip install -r vllm_requirements.txt的时候，出现了报错的情况，竟然说里面有一个包的版本是yanked version（撤回版本），无法下载，给我气晕了。\n吃一堑，长一智。配置好的环境就不要变了，应该另外复制一个环境，在复制的环境上进行修改。\n此外，我每次进行配置环境我都会忘记怎么配置和删除。是我最近记性变得太差了吗？总之我写一个文档，记不住就查一下。\n配置环境\r使用conda配置虚拟环境\r创建新的环境\r使用Terminal创建新的环境。\nconda create -n \u0026lt;new_env_name\u0026gt; python=3.10.0 激活虚拟环境\nconda activate \u0026lt;new_env_name\u0026gt; 安装包\nconda install \u0026lt;package\u0026gt; pip install \u0026lt;package\u0026gt; 从已有的文件中安装包/虚拟环境\r如果想要安装requirements.txt文件，就可以直接\npip install -r requirements.txt 如果想要安装的是environment.yml文件，应该改用conda来创建虚拟环境\nconda env create -f environment.yml 查看虚拟环境列表\nconda env list 复制原来已有的虚拟环境\r如果有一个环境已经配置好，我不希望破坏它，可以复制一个一模一样的环境，再在上面进行修改，这样就不会导致原来那个环境产生问题。\nconda create --name \u0026lt;new_env_name\u0026gt; --clone \u0026lt;old_env_name\u0026gt; 删除虚拟环境\r删除指定的虚拟环境\nconda activate base conda remove -n \u0026lt;env_name\u0026gt; --all 在conda中配置Jupyter内核\r安装Jupyter内核\r总是忘记Jupyter内核如何配置。记录一下：\n安装ipykernel。\nconda install ipykernel 将虚拟内核添加到jupyter内核中。\npython -m ipykernel install --user --name \u0026lt;your_env_name\u0026gt; 删除jupyter内核\r查看目前有的jupyter内核\njupyter kernelspec list 删除指定的jupyter内核\njupyter kernelspec remove \u0026lt;your_kernel_name\u0026gt; 照片\r照片是2024/12/7的时候同门团建的时候我拿大疆Pocket3拍的。拍的建筑是东郊民巷的圣弥厄尔大教堂。非常开心的一天。\n","date":"2024-12-15T20:24:19+08:00","image":"https://ionfeather.github.io/2024/virtual-environment-config/cover_hu15097618714508979196.jpg","permalink":"https://ionfeather.github.io/2024/virtual-environment-config/","title":"虚拟环境配置操作记录"},{"content":"为什么要学习LangChain\r我希望能够构建一个能阅读PDF论文的Agent，并且能够输出对论文优缺点的评价。\n导师\u0026nbsp;\u0026nbsp;\u0026nbsp;2024-10-12 14:30\r做一个论文阅读的大模型。 2024-10-12 14:45\u0026nbsp;\u0026nbsp;\u0026nbsp;我\r好的老师。 使用LangChain听说比较方便。\nLangChain是用来做什么的？\rLangChain是一个用于开发由LLM驱动的应用程序的框架。也就是说我们可以把LLM作为内核，LangChain作为外壳，搭建一个程序出来。\nLangChain提供了\n组件：处理LLM的组件的抽象； 定制链：把组件拼起来，实现一个特定用例。 对于阅读PDF，目前有两个想法：\n将PDF转为JSON，然后输入到LLM中； 构建RAG。使用LangChain能够比较方便地实现这个功能，听ZLB说这个也不是很难。我之前的畏难情绪可能太重了，现在写一个文档，激励和记录一下自己学习。 RAG是什么？\r虽然LLM非常强大，但它们对于它们未经训练的信息一无所知。如果您想使用LLM来回答它未经训练的文档相关问题，您需要向其提供这些文档的信息。最常用的方法是通过“检索增强生成”（ retrieval augmented generation，RAG ）。\n检索增强生成的思想是，在给定一个问题时，首先进行检索步骤以获取任何相关文档。然后将这些文档与原始问题一起传递给语言模型，并让它生成一个回答。然而，为了做到这一点，首先需要将文档以适合进行此类查询的格式呈现。\n构造一个语义搜索引擎\rBuild a semantic search engine | 🦜️🔗 LangChain\n读取PDF\rHow to load PDFs | 🦜️🔗 LangChain\n这里，文档中推荐使用了pypdf库。这里\n在实际应用中可以使用其他提取效果更好的库。LangChain支持的PDF格式很多，可以选择一下。\nDocument Loader Description Package/API PyPDF Uses pypdf to load and parse PDFs Package Unstructured Uses Unstructured\u0026rsquo;s open source library to load PDFs Package Amazon Textract Uses AWS API to load PDFs API MathPix Uses MathPix to load PDFs Package PDFPlumber Load PDF files using PDFPlumber Package PyPDFDirectry Load a directory with PDF files Package PyPDFium2 Load PDF files using PyPDFium2 Package PyMuPDF Load PDF files using PyMuPDF Package PDFMiner Load PDF files using PDFMiner Package 此外，导师之前还给我推荐了titipata/scipdf_parser库，能够更好地处理图像和扫描文本，并且运行在docker上，便于部署。\npypdf的介绍\rWelcome to pypdf — pypdf 5.1.0 documentation\nPyPDF 是一个用于处理 PDF 文件的 Python库。它提供了一组工具和功能，用于读取、解析和操作 PDF 文件的内容。\nSplitting\r原文\rFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \u0026ldquo;washed out\u0026rdquo; by surrounding text.\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nWe set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”.\nSee this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\n对于问题提问的文本来说，直接回答一整页肯定是太粗略了。我们最终的目标是检索回答输入查询的文档对象，进一步拆分 PDF 将有助于确保文档相关部分的含义不会被周围的文本“冲淡”。\n所以接下来应该用文本分割器来进行分割（Splitting）处理。这里用一个RecursiveCharacterTextSplitter进行分割。这里使用常见分隔符来对文档进行分割，适用于一般的文本。\n使用RecursiveCharacterTextSplitter无法读取图像或特定区域的文本。\nEmbeddings\r接下来将文本嵌入到向量中去，便于进行相似度指标来识别相关文本。\n这里LangChain支持数十种Embeddings方法。这里我选择了使用Hugging Face，可以选择将模型下载至本地或者使用Hugging Face Inference API来调用接口。这里可以直接使用HuggingFaceEmbeddings来进行处理。非常方便。\nfrom langchain_huggingface import HuggingFaceEmbeddings embeddings_model = HuggingFaceEmbeddings(model_name=\u0026#34;sentence-transformers/all-mpnet-base-v2\u0026#34;) embeddings = embeddings_model vector_1 = embeddings.embed_query(all_splits[0].page_content) vector_2 = embeddings.embed_query(all_splits[1].page_content) assert len(vector_1) == len(vector_2) print(f\u0026#34;Generated vectors of length {len(vector_1)}\\n\u0026#34;) print(vector_1[:10]) Vector Stores\rLangChain的Vector Stores对象包括了一些把文本和Document对象加入到Stores中的方法，然后通过相似性进行一个排列。\nfrom langchain_core.vectorstores import InMemoryVectorStore vector_store = InMemoryVectorStore(embeddings) ids = vector_store.add_documents(documents=all_splits) 此时就完成了存储和排列。\n这里向量存储一般来说是可以连接到现有的Vector Stores中的。\nUsage\r查询和这句话相似的句子 results = vector_store.similarity_search( \u0026#34;Diffusion is a image generation method.\u0026#34; ) ) print(results[0]) 异步查询（用于流程控制） results = await vector_store.asimilarity_search(\u0026#34;What is diffusion?\u0026#34;) print(results[0]) 返回分数 # Note that providers implement different scores; # the score here is a distance metric that varies inversely with similarity. results = vector_store.similarity_search_with_score(\u0026#34;What is Diffusion?\u0026#34;) doc, score = results[0] print(f\u0026#34;Score: {score}\\n\u0026#34;) print(doc) 通过和embedded query的相似度进行查询 embedding = embeddings.embed_query(\u0026#34;What is diffusion\u0026#34;) results = vector_store.similarity_search_by_vector(embedding) print(results[0]) Retrievers\r检索器（Retriever）可以从向量存储中进行构建，但是也可以和非向量形式进行交互。如果我们要构建一个能够检索文档的方法的话，我们可以创建一个runnable的检索器。\nfrom typing import List from langchain_core.documents import Document from langchain_core.runnables import chain @chain def retriever(query: str) -\u0026gt; List[Document]: return vector_store.similarity_search(query, k=1) retriever.batch( [ \u0026#34;What is diffusion?\u0026#34;, \u0026#34;What is forward process?\u0026#34;, ], ) 至此，我们构建了一个能够读多篇PDF文章的、能够对PDF文章进行查询的语义搜索引擎。\nChat Models和Prompt模板\r这里通过Vllm启动LLM，以Qwen2.5-7B-Instruct模型为例。\nfrom langchain_community.llms import VLLM llm = VLLM(model=\u0026#34;/home/ubuntu/jjq/Qwen/Qwen2.5-7B-Instruct/\u0026#34;, trust_remote_code=True, max_new_tokens=512, top_k=10, top_p=0.95, temperature=0.8, max_model_len = 30000, ) print(llm(\u0026#34;What is the capital of France ?\u0026#34;)) 接下来设计Prompt模板。\nfrom langchain import LLMChain from langchain.prompts import PromptTemplate from langchain.memory import ConversationBufferMemory from langchain.chains import ConversationalRetrievalChain from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate template = \u0026#39;\u0026#39;\u0026#39; 【任务描述】 请仔细阅读论文，回答用户给出的问题，尽量具有批判性。 【论文】 {{context}} ----------- {question} \u0026#39;\u0026#39;\u0026#39; # 检索器 retriever = db.as_retriever() # 记忆 memory = ConversationBufferMemory(memory_key=\u0026#34;chat_history\u0026#34;, return_messages=True) # 构建Agent qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory) qa({\u0026#34;question\u0026#34;: \u0026#34;能不能用中文给出论文的优势或者前景？\u0026#34;}) ","date":"2024-11-26T13:45:58+08:00","image":"https://ionfeather.github.io/2024/langchain-learning/cover_hu13968487782357828171.png","permalink":"https://ionfeather.github.io/2024/langchain-learning/","title":"LangChain学习笔记"},{"content":"梦\r这是一个英雄辈出的时代。所谓阴阳师，就是使用牌组与其他人对战来决定胜负的职业。在这里，国之阴阳师是一国中最强的阴阳师，中日韩三国每年都会选拔国之阴阳师，并且对战，决出最后的冠军。\n我和梦梦是青梅竹马，从小便展露了阴阳师的天赋。所谓“绕床弄青梅，郎骑竹马来”，我和梦梦那就是“绕床打牌组，郎打牌组来”。从小与其他人对战，胜利了之后可以选择是否获取一张新卡\n牌放进自己的卡组，最终打一个最强者之间的对战。这个对战从来都是我和梦梦之间的私人聊天与沟通时间。\n随着我们渐渐长大，我和梦梦之间也互生情愫。但是，认真打牌，赢得中日韩三国之间的对战，获得至高无上的荣耀是我们的最大目标。儿女情长，英雄气短，阴阳师需要克制。\n……\n梦梦要去日本打探消息了。她去那里，是为了我们中国能够更了解日本的特殊卡牌。可是，一个人在异国他乡，离开最亲近的人，是那么容易的事情吗？\n终于到了中日韩会赛的时间。\n我期待着到达了梦梦的住所，敲门，迎接我的果然是笑靥如花的梦梦。我们见到对方，思念已久的澎湃难抑制，但我们都克制住了自己，只是眼睛里互相诉说着彼此。\n但梦梦的房间里有股不详的气息。她好像被监视了。“你的房间里曾经有过一个男人躲在里面”，我说。\n梦梦害怕极了，但为了国家能够去刺探信息的人必然非常坚强，她脸色发白，不住地颤抖，但声音很小：“还在吗……这怎么办…”\n我安慰她：“这没什么，这是日本人监视你的手段，但梦梦你肯定也有没被看破的地方。”随后我离开了梦梦的家，脸色发青。\n……\n接下来就是我和八重岛神子最终对战了。近几年韩国式微，只剩中日交战。日本去年赢过了我国。去年对战使用的是30张左右的小牌组，打到后面基本就是6张左右一个循环，对方的强度比我们高。\n八重岛神子太强了，所有人都不相信我能打败他，包括我自己。我用尽心血，准备了一套120张卡牌的超大牌组进行对战。奇妙的是，八重岛神子也拿出来120张的卡组。这极其少见。我们来了一场古典的交锋。\n……\n突然，我想起之前在什么地方，在很久很久的从前，有个人和我说我会赢的，于是我便充满了信心，我的阴霾一扫而空。\n我尽量诚实地描述了我的梦境，可惜忘记了太多。我感觉里面有蛮多意象的。围棋中日韩会战、恋爱、谍战监视、杀戮尖塔烧牌循环、炉石传说对战\u0026hellip;很有意思的一个梦，醒来之后回味了很久。但是看起来可能没有那么有趣，或许我应该加一点戏剧性要素？\n照片\r照片是我2024/11/3的时候在地坛拍的。我想那一天是北京秋天最美的一天。北京的秋天是短暂的，11/3之前雾霾太重，看什么都朦胧；11/3那天的风很大，无数灿烂的叶子不断地落下来，在这一天之后，树梢上就稍微有一些秃了。\n","date":"2024-10-20T13:45:58+08:00","image":"https://ionfeather.github.io/2024/dream/cover_hu13768060218741915990.jpg","permalink":"https://ionfeather.github.io/2024/dream/","title":"一个有趣的梦 | 三国阴阳师"}]