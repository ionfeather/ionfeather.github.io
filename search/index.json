[{"content":"\r千年 那天我翻阅字典\n查什么字眼\n形容一件事很遥远\n天边是否在海角对面\n直到九岁才知道浪费时间\n这茶桌樟木的横切面\n年轮有二十三圈\n镜头的另一边跳接我成熟的脸\n周杰伦\r《爷爷泡的茶》 中秋之夜我在听《爷爷泡的茶》，看天边的月亮，我看到的月亮和浙江看到的是一样的。在357000公里之外的月亮看起来，地球上的我和家人应该是贴在一起的吧？暑假的时候，妹妹说爷爷不泡茶这个奶茶店为什么叫爷爷不泡茶呢——因为「爷爷」不泡茶，那就是「奶茶」。\n这么一说，想喝奶茶了。\n日常\r和师兄骑自行车去天津。从昌平校区出发到天津之眼总计154.7KM，骑行总用时8小时左右。但是我们中间休息了很久——因为没有穿骑行裤，屁股超级痛。\n慢慢地，也是到了天津啦。\n到了天津之后，我们直接入住九号温泉洗浴中心——非常舒服的一个地方！除了睡觉有点找不到位置，感觉其他方面已经是我遇见的最好的水准。自助餐类型多、服务也好、还有三楼室外温泉和PS5游戏打，如果下次去天津玩，推荐推荐，真的还是蛮好玩的。\n2025-10-03\n小红书上看到了尖端代码与古老智慧的碰撞： AI代码的八荣八耻\n以暗猜接口为耻，以认真查阅为荣。\n以模糊执行为耻，以寻求确认为荣。\n以盲想业务为耻，以人类确认为荣。\n以创造接口为耻，以复用现有为荣。\n以跳过验证为耻，以主动测试为荣。\n以破坏架构为耻，以遵循规范为荣。\n以假装理解为耻，以诚实无知为荣。\n以盲目修改为耻，以谨慎重构为荣。\n已经把它加入到我和Cursor对话的最前面了。非常搞笑，但是竟然意外地好用。\n2025-10-08\n","date":"2025-10-08T10:56:57+08:00","image":"https://ionfeather.github.io/2025/cold-dew/assets/cover_hu_66772b6d7342cdd0.jpg","permalink":"https://ionfeather.github.io/2025/cold-dew/","title":"寒露 | 这茶桌樟木的横切面\r，年轮有二十三圈"},{"content":"MCTS\rMC-DML\r[2504.16855] Monte Carlo Planning with Large Language Model for Text-Based Game Agents\n主要解决的问题\r解决了文字冒险游戏中 AI 规划效率低、缺乏语言理解与经验记忆能力。\n面临挑战\r游戏环境很复杂 传统的MCTS有局限性 LLM难以将规划转化成可执行动作，且无法平衡探索与利用 核心方法\rMC-DML（Monte Carlo planning with Dynamic Memory-guided Large language model）\n四阶段规划流程：沿用 MCTS 的Selection, Expansion, Simulation, Backpropagation四阶段。在扩展阶段引入 LLM 作为先验策略，让 LLM 基于场景文本为可选动作分配非均匀搜索优先级；模拟阶段通过多轮推演评估动作结果，回溯阶段更新节点价值与访问次数。 双动态记忆机制： 「 trial 内记忆（$M_i$）」：存储当前轨迹历史（如 “前一观测 - 动作 - 当前观测”），帮 LLM 结合当下语境生成动作概率分布； 「 trial 间记忆（$M_c$）」：存储过去失败轨迹的反思（如 “无光源时勿入黑暗区域”），动态调整动作价值评估，避免重复犯错。 动作选择公式优化：在 PUCT（改进型 MCTS）公式基础上，将 LLM 生成的动作概率（结合双记忆）融入计算，确保选择既符合语言逻辑又兼顾探索-利用平衡。 ","date":"2025-09-23T17:22:57+08:00","permalink":"https://ionfeather.github.io/2025/mcts/","title":"论文阅读| MCTS"},{"content":"和唐老师和Tim Ding拍月食。9月6日的晚上是中元节，9月7日则是白露，还将会有月全食。\n我开玩笑地说：「真的有点鬼影憧憧的。」唐老师告诉我，北航在中元节那天还在凌晨2点到6点进行了拉练，那天月亮西垂在树梢，被大气层折射成了红月亮。在红月亮下，一群军训学生在训练——真的很有恐怖小说的开头的味道。如果是恐怖小说，之后的视角就要切换到学生，说他从来不信这些，但是那天营地里发生了一些事，实在是无法解释\u0026hellip;\u0026hellip;\n月食直到凌晨才会开始。那天的北师大人特别少，我和唐老师约好三楼见面，路上在中央广场处遇到了Tim Ding——感觉超级像是个NPC！刷新在了那里——我们是准备去看看桥上的月亮如何。那天天朗气清，除了凌晨时分一片薄云挡在了月亮上，其他时候都是晴空万里。当机立断地，我们撺掇Tim Ding不要固守在木铎那里了，那里实在是太亮了，一圈一圈的发光带和楼房会把完美的月食给错过的。\n于是前往玉米地，于是前往桥头。我们端起相机，架起三脚架。我们三个人只有一个半三脚架——说是有三个，但一个是我临时花了40块钱买的，一个是Tim Ding多的一个超级小的三脚架，唯一称得上能用的只有Tim Ding的原来的三脚架。所以虽然看似是三个人，但是真正能一直拍的，只有一个，其他两个人都是氛围组。我抢了不少时间——还好富士的对焦不行，这里不得不说尼康的对焦虽然一直被嘲笑对焦到鸟上，但是这次它非常得棒！能够在非常暗的情况下对焦到月亮那个焦平面，而不是糊成一片，而且峰值对焦显示的蓝色可以让我能够很自信地觉得它已经做到最清晰了。\n这天刚好是白露。北方的节气准时得像是闹钟，这天的月亮从60°的高空慢慢划下，被天狗吃完又吐出，风徐徐地吹，草叶上慢慢凝结了一些露珠。或许读研究生的最终目的不是为了实习，不是为了对未来的焦虑，而是在为了一张打光完美的合照和拍完月亮之后的满足和困意。\n","date":"2025-09-10T18:52:06+08:00","image":"https://ionfeather.github.io/2025/white-dew/moon_hu_a3179da18a6884c.jpg","permalink":"https://ionfeather.github.io/2025/white-dew/","title":"白露 | 露从今夜白，月是故乡明"},{"content":"Introduction\r阅读Speech and Language Processing这本书的一些笔记。\nWords and Tokens\r我们需要一个东西来建模语言，下面是我们的选择：\nWords\r为什么不用词？\n有些语言没有orthographic words 词的数量会随着文章增长，词汇表永远都会覆盖不足 Morphemes\r语素类型\n屈折语素：inflectional morphemes 派生语素：derivational morphemes 附着语素：clitic 语言类型\nAnalytic polysynthetic fusional agglutinative 为什么不用语素？\n语素很复杂，很难定义 不同语言不同且难以统一 Unicode\rUnicode的历史\nASCII CJKV 不断更新中，越来越多，Unicode 16.0已经包含超过150000个字符 Code Points\rU+：表示接下来要用Unicode十六进制表示一个code point U+0061：0x0061一个意思，也就小写字母a。 UTF-8\r目前最常用的encoding字符的方式。中文字符 “中” 的 Unicode 码点是U+4E2D，UTF-8 编码后为 3 个字节：0xE4 0xB8 0xAD\nUTF-8是一种变长编码，兼容ASCII。\n如「世」，UTF-8 编码是0xE4 B8 96，其中E4的二进制为11100110H，开头的1110H表示这是一个3字节字符的第一个字节。 Subword Tokenization: Byte-Pair Encoding\r上面的三个候选都不行，word和morpheme难以规范定义，character可以通过unicode来定义，但又对于作为tokens来说太小了。\n为什么要tokenize输入？\n将输入转换为一组确定的、固定的单元（Token），能让不同的算法和系统在一些简单问题上达成共识。例如困惑度的计算。 对可复现很重要 为了消除unknown words的问题 为了消除unknown words问题，现代tokenizers自动引入了token包含那些比words小的token，叫subword。\n使用Tokenizer - OpenAI API中的GPT-4o \u0026amp; GPT-4o mini来分词下面这一大段话：\nFor example, if we had happened not to ever see the word lower, when it appears we could segment it successfully into low and er which we had already seen. In the worst case, a really unusual word (perhaps an acronym like GRPO) could be tokenized as a sequence of individual letters if necessary.\n最终得到的是 现在最流行的tokenization algorithm有两个：\nByte-Pair Encoding(BPE) Unigram Language modeling(ULM) BPE\r通过分析训练语料，自动学习出一套子词集合（词汇表），使得高频出现的字符 / 子词组合被合并为更大的子词单位。\n训练方法介绍。\nBPE encoder\rBPE in practice\r通常，我们会对 UTF-8 编码文本的单个字节执行 BPE 操作。BPE 处理 “中” 时，输入并非U+4E2D这个码点，而是E4、B8、AD这三个独立字节。\n仅在预先切分出的单词内部执行 BPE 操作，有助于避免潜在问题。\n一些英语里的小发现：\n大多数单词的tokens是他们自己，包含词前空格。这样可以避免独立单词和单词内部的subword。 附着语素Clitics在名字后面分开单独成token，但在常见的词语后面会是token的一部分 数字通常三位一组 一些词，如Anyhow和anyhow会有不同的分割方法 这个和预处理有关系。\nSuperBPE会合并常规的BPE子词分词，效率更高。\n特别地，低资源语言的tokens更碎，就会输出边长，最终LLM的效率变低。\nRule-based tokenization\rPenn Treebank Tokenization Standard）：事实性规范。\n分开附着语素 保留连字符连接的词 分开所有的标点符号 Sentence Segmentation\rsentence tokenization可以和word tokenization联合处理。\nCorpora\r语料库和语言数量、使用者的特征都有关。\ncode switching：在一次持续的交流）中，说话者或作者交替使用两种或多种 “语码”的现象。\ndatasheet：存储一句话的特征，如时间、说话人性格、阶级\u0026hellip;\nRegular Expressions\r正则表达式的具体实现。包含字符析取、计数、可选性、通配符、锚点和边界、替换和捕获组、前向断言等。\nSimple Unix Tools for Word Tokenization\r可以在Unix、Linux系统中使用正则表达式。如tr -sc 'A-Za-z' '\\n' \u0026lt; sh.txt表示从 sh.txt 文件中提取所有英文字母，并将非字母字符替换为换行符，同时压缩连续的非字母字符为单个换行符。\nMinimum Edit Distance\r最小编辑距离：将一个字符串通过 “插入”“删除”“替换” 三种基本操作转换为另一个字符串所需的最少操作次数\nThe Minimum Edit Distance Algorithm\r一个经典的动态规划问题。\n字符对齐：通过回溯编辑距离矩阵中的 “最优路径”，反向推导出将一个字符串转换为另一个字符串的具体操作序列。也就是路径可视化。\nExercies\r点击展开\r2.1\rWrite regular expressions for the following languages.\nThe set of all alphabetic strings. The set of all lowercase alphabetic strings ending in \u0026ldquo;b\u0026rdquo;. The set of all strings from the alphabet {a, b} such that each \u0026ldquo;a\u0026rdquo; is immediately preceded by and immediately followed by a \u0026ldquo;b\u0026rdquo;. 2.2\rWrite regular expressions for the following languages. By \u0026ldquo;word\u0026rdquo;, we mean an alphabetic string separated from other words by whitespace, relevant punctuation, line breaks, etc.\nThe set of all strings with two consecutive repeated words (e.g., \u0026ldquo;Humbert Humbert\u0026rdquo; and \u0026ldquo;the the\u0026rdquo; but not \u0026ldquo;the bug\u0026rdquo; or \u0026ldquo;the big bug\u0026rdquo;). All strings that start at the beginning of the line with an integer and end at the end of the line with a word. All strings that have both the word \u0026ldquo;grotto\u0026rdquo; and the word \u0026ldquo;raven\u0026rdquo; in them (but not, e.g., words like \u0026ldquo;grottos\u0026rdquo; that merely contain \u0026ldquo;grotto\u0026rdquo;). Write a pattern that places the first word of an English sentence in a register. Deal with punctuation. 2.3\rImplement an ELIZA-like program, using substitutions such as those described on page 27. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your program can legitimately engage in a lot of simple repetition.\n2.4\rCompute the edit distance (using insertion cost 1, deletion cost 1, substitution cost 1) of \u0026ldquo;leda\u0026rdquo; to \u0026ldquo;deal\u0026rdquo;. Show your work (using the edit distance grid).\n2.5\rFigure out whether \u0026ldquo;drive\u0026rdquo; is closer to \u0026ldquo;brief\u0026rdquo; or to \u0026ldquo;divers\u0026rdquo; and what the edit distance is to each. You may use any version of distance that you like.\n2.6\rNow implement a minimum edit distance algorithm and use your hand-computed results to check your code.\n2.7\rAugment the minimum edit distance algorithm to output an alignment; you will need to store pointers and add a stage to compute the backtrace.\nN-gram Language Models\r本章介绍最简单的语言模型：N元语法语言模型。\nN-Grams\r概率链式法则\nHow to estimate probabilities\r马尔科夫假设：假设一个单词的出现概率只和前面的一个单词有关。那么n-gram即只和前面的$n-1$个单词有关。\n最大似然估计：已知前一个词$w_{n−1}$​时，当前词$w_n$​的概率\n终止符号（end-symbol）：所有可能句子的概率总和为 1，否则是特定长度的所有句子概率之和为 1。\nDealing with scale in large n-gram models\rLog probabilities\nN元语法的计算现在甚至能达到无限元。\n对N元语法模型进行修剪也是很重要的。\nEvaluating Language Models: Training and Test Sets\r内部评估和外部评估。\n训练集、开发集和测试集。\nEvaluating Language Models: Perplexity\rPerplexity（PPL）：困惑度越低，说明模型对文本的预测越准确（即模型越 “不困惑”）。\n具体来说，是“联合概率倒数的几何平均值”。 在计算的时候常常会取对数来将求乘积变为求和，避免数值问题 Perplexity as Weighted Average Branching Factor\r困惑度也可以理解为加权平均分支系数。其中，语言的 “分支系数”指的是 “任何一个词之后可能出现的下一个词的数量”。\nSampling sentences from a language model\r“0-1 数轴 + 区间映射”来理解采样的基本原理。\nGeneralizing vs. overfitting the training set\r对于莎士比亚文本和华尔街日报的文本，两者差异过大以至于不能分别作为训练集和测试集。\n所以说要确保训练集和测试集的领域要相似。\nSmoothing, Interpolation, and Backoff\rzero probability n-grams有两个问题：\n低估了词语序可能出现的可能性，导致最终的性能变差 困惑度无法计算，因为无法除以0 因此需要Smoothing或者discounting\nLaplace Smoothing\r其实也就是add one smoothing，就是对于所有的N元语法都加一。\n对于语言模型来说，结果并不是很好。对文本分类有效。\nAdd-k Smoothing\r也就是对所有的都加K。\n对语言模型来说仍然效果一般。\nLanguage Model Interpolation\rn 元语法插值法：加权融合不同阶数 n 元语法的概率，避免高阶的n元语法零概率导致的预测失效。\n加权的$\\lambda$应该设置成多少呢？可以从预留集held-out corpus中学习。使用EM（期望最大化）算法来学习。\nStupid Backoff\r回退模型：高阶n阶的模型无法使用的时候，回退到低阶模型。\nDiscount：要让回退模型（backoff model）输出合理的概率分布，我们必须对高阶 n 元语法的概率进行 “折扣处理”（discount），从而预留出部分概率余量（probability mass），供低阶 n 元语法使用。但在实际应用中，人们常使用一种更简单的 “无折扣回退算法”—— 即名为Stupid Backoff。\nAdvanced: Perplexity\u0026rsquo;s Relation to Entropy\r熵：不确定性的度量方式。可以理解是编码某个决策或某条信息所需的最小平均比特数。越不确定，熵越大。\n熵率：平均的不确定性。自然语言的熵率定义为 “无限长序列中，每个词的平均熵”，反映语言的长期不确定性。例如，英文的熵率约 1-2 比特 / 词，意味着平均每个词需要 1-2 比特来编码。\n平稳性：序列概率不随着时间改变。自然语言不是，但是N元语法是平稳的。\n遍历性：长序列中包含了所有的短序列。\nShannon-McMillan-Breiman theorem：如果语言满足某些正则条件（准确地说，是平稳且遍历的），序列长度趋近于无穷大时，“序列的平均对数概率的负值” ，即经验熵率会以概率1收敛敛到该过程的理论熵率。\n交叉熵（Cross-Entropy）：我们虽然不知道数据的真实概率分布p，但是可以用模型m来近似p。（即我们虽然不知道自然语言的真实情况，但是可以用N元语法来近似。）交叉熵越小，模型越接近真实分布。\n困惑度：困惑度是熵的指数形式。比较直观。\nExcercies\r点击展开\r3.1\rWrite out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 40.\n3.2\rCalculate the probability of the sentence i want chinese food. Give two probabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on page 42, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(i|\u0026lt;s\u0026gt;) = 0.19$ and $P(\u0026lt;/s\u0026gt;|food) = 0.40$.\n3.3\rWhich of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why.\n3.4\rWe are given the following corpus, modified from the one in the chapter:\n\u0026lt;s\u0026gt; I am Sam \u0026lt;/s\u0026gt; \u0026lt;s\u0026gt; Sam I am \u0026lt;/s\u0026gt; \u0026lt;s\u0026gt; I am Sam \u0026lt;/s\u0026gt; \u0026lt;s\u0026gt; I do not like green eggs and Sam \u0026lt;/s\u0026gt; Using a bigram language model with add-one smoothing, what is $P(Sam | am)$? Include $\u0026lt;s\u0026gt;$ and $\u0026lt;/s\u0026gt;$ in your counts just like any other token.\n3.5\rSuppose we didn’t use the end-symbol $\u0026lt;/s\u0026gt;$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $\u0026lt;/s\u0026gt;$:\n\u0026lt;s\u0026gt; a b \u0026lt;s\u0026gt; b b \u0026lt;s\u0026gt; b a \u0026lt;s\u0026gt; a a Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet a,b is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet a,b is also 1.0.\n3.6\rSuppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating $P(w3|w1,w2)$, where $w3$ is a word which follows the bigram$ (w1,w2)$, in terms of various n-gram counts and V. Use the notation $c(w1,w2,w3)$ to denote the number of times that trigram $(w1,w2,w3)$ occurs in the corpus, and so on for bigrams and unigrams.\n3.7\rWe are given the following corpus, modified from the one in the chapter:\n\u0026lt;s\u0026gt; I am Sam \u0026lt;/s\u0026gt; \u0026lt;s\u0026gt; Sam I am \u0026lt;/s\u0026gt; \u0026lt;s\u0026gt; I am Sam \u0026lt;/s\u0026gt; \u0026lt;s\u0026gt; I do not like green eggs and Sam \u0026lt;/s\u0026gt; If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $λ₁ = 1/2$ and $λ₂ = 1/2,$ what is $P(Sam|am)$? Include $\u0026lt;s\u0026gt;$ and $\u0026lt;/s\u0026gt;$ in your counts just like any other token.\n3.8\rWrite a program to compute unsmoothed unigrams and bigrams.\n3.9\rRun your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?\n3.10\rAdd an option to your program to generate random sentences.\n3.11\rAdd an option to your program to compute the perplexity of a test set.\n3.12\rYou are given a training set of 100 numbers that consists of 91 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?\nLogistic Regression and Text Classification\r经典任务：\nsentiment analysis spam detection language id authorship attribution Machine Learning and Classification\r人工规则很脆弱，数据一变化就无法使用 LLM的弱点：幻觉、无法解释。 因此最常见的分类方法是有监督机器学习。\n概率分类器：输出样本属于每个类别的概率而不是类别标签，保证在合并的系统里不过早地输出结果。 分类器的核心组件：\nA feature representation of the input A classificaition function that computes $\\hat{y}$ An objective funcion that we want to potimize for learning loss function An algorithm for optimizing the objective function stochastic gradient descent algorithm The Sigmoid Function\r二分类逻辑回归的目标是：计算样本属于正类的概率。\n第一步：计算线性得分$z=w⋅x+b$，值域为$[-\\infty, +\\infty ]$ 第二步：通过 Sigmoid 函数转换为概率： 将线性得分 z 映射到 $[0,1] $区间 $z$常常被称作Logit（对数几率）。Logit就是Sigmoid的反函数。可以提醒我们后续要加上Sigmoid进行转换，因为$z$并不是一个真实的值。\n特别地，“正类的对数几率” 与特征呈线性关系。也就是当其他条件不变时，特征$x_1$每增加1，Logit就增加$w_1$。这非常有可解释性。\nClassification with Logistic Regression\r当概率大于0.5的时候，就把它分类到正类里。\nSentiment Classification\r举了一个例子。\nOther Classification Tasks and Features\rPeriod disambiguation：确定句号是EOS还是其他。\nDesigning v.s. Learning features：\n刚刚的例子，特征都是人工设计的。此外还有： feaure interactions：基础特征组合成的复杂特征 feature templates：抽象的特征规范来定义特征。这里的特征空间是稀疏的，此外特征一般是字符串描述的Hash值。 人工设计太复杂了。因此现代的NLP系统都是用Representation Learning来解决。 standardize和normalize。\nProcessing many examples at once\r如果有许多的值要计算，可以使用matrix arithmetic来一次计算完。\nMultinomial Logistic Regression\r多项逻辑回归也称softmax regression，老的教材上也叫maxent clasifier。\n在多项逻辑回归中，直接输出结果而不是一个概率值。\nhard classification Softmax\rSigmoid函数在多分类情况下的推广。\nApplying Softmax in Logistic Regression\r可以使用矩阵运算方式加快计算。\n$$\\hat{y}=softmax(Wx+b)$$ Doumbouya et al., 2025是这么认为的：逻辑回归将矩阵的每一行 $w_k$视为第 $k$ 类的原型（prototype），由于两个向量的相似度越高，它们的点积（dot product）值就越大，因此点积可作为衡量向量相似度的函数。模型最终将输入分配给相似度最高的类别。\nFeatures in Multinomial Logistic Regression\r特征权重同时依赖于输入文本和输出类别。\nLearning in Logistic Regression\r逻辑回归是如何实现学习的？\n使system output（classifier output）和gold output（correct output）越接近越好。两者之间的距离可以称作损失函数或者代价函数。下面介绍交叉熵。 需要一个算法来最小化损失函数。下面介绍随机梯度下降算法。 The Cross-entropy Loss Function\r条件最大似然估计：在给定$x$下，选择参数$w$和$b$使得$y$的对数概率最大。\n这里损失函数是负对数似然损失（negative log likelihood loss），通常也被称为交叉熵损失（cross-entropy loss）。\n介绍了一下为什么最小化交叉熵损失可以使得真实分布和预测分布更加接近。\nGradient Descent\r梯度下降算法的原理。介绍了梯度、学习率。\nThe Gradient for Logistic Regression\r逻辑回归的梯度就是 $$\\frac{\\partial L_{\\mathrm{CE}}(\\hat{y},y)}{\\partial w_{j}}=-(y-\\hat{y})x_{j}$$ 也就是预测值$\\hat{y}$和实际值$y$之间的差乘输入值$x_j$。\nThe Stochastic Gradient Descent Algorithm\r随机梯度下降算法是一种在线算法，可以边接收数据边学习。\nSGD每次用单个随机样本计算梯度。\nMini-batch Training\rbatch training和mini-batch training的区别。\nbatch gradient：所有的随机样本计算梯度。 mini-batch gradient：小批量梯度下降算法。每次选择一小批随机样本计算梯度。 Learning in Multinomial Logistic Regression\r多项式逻辑回归其实和二项式逻辑回归差不多。\n本质是使用独热标签+概率向量的形式进行计算。 核心是 “对正确类别的预测概率取负对数”，得到交叉熵损失，其越小则预测概率越高。 Evaluation: Precision, Recall, F-measure\rconfusion matrix accuracy precision recall F-measure F1 a weighted harmonic mean of precision and recall. microaveraging v.s. macroaveraging\n微观平均：更关注 “整体样本的预测准确性”，少数类错判代价低于多数类 宏观平均：更关注所有的类的错判代价的公平，少数类和多数类的代价相等 Test sets and Cross-validation\rCross-validation：解决测试集不足的问题。\n固定训练集和测试集。 训练集中进行分割。 Statistical Significance Testing\r统计显著性检验\n不只是简单地检查A在测试集上的结果$M(A,x)$好于B在测试集上的结果$M(B,x)$。如果差很小的话，其实不一定能证明A的结果比B小，不具有统计学上的显著性。 设计一个效应量$\\delta (x)=M(A,x)-M(B,x)$，原假设是$H_0 :\\delta (x)\\leq 0$。这样计算p值是否小于阈值可以得出是否显著性地A比B要好。 在NLP中，一般不用ANOVAs或者t检验，而是使用非参数检验：\n近似随机化检验（Approximate Randomization Test） bootstrap 检验（Bootstrap Test） 方差分析和t检验都需要有假设：方差齐性或数据服从正态分布。所以只能用非参数检验。\nThe Paired Bootstrap Test\rBootstrap 检验的核心是 “重抽样”—— 从原始测试集x中有放回地随机抽取生成新的测试集。\n在新的测试集上，P 值等于 “重抽样测试集中，$d(x^{(i)})≥2d(x)$的数量占总重抽样次数b的比例”。判断此时的p值是否低于阈值就可以判断出是否是数据集本身的偏差导致了A比B要结果好。\nLLM的解释：Bootstrap 重抽样，就是在 “不改变天平初始倾斜（测试集偏向）” 的前提下，反复放 “随机重量（重抽样的样本）”，看左边会比右边多低多少格 —— 如果只是天平本身歪了，随机放重量时，左边最多低 2 格左右（常规波动）；如果左边真的更重，就可能低 4 格以上（极端情况）。这种极端情况多了，超过了阈值，我们就更相信是天平本身的问题。\nAvoiding Harms in Classification\rrepresentational harms：由于对特定社会群体的贬低或刻板印象导致的伤害。\ntoxic detection\nmodel card\nInterpereting Models\r模型的可解释性也是很重要的。逻辑回归就是比较好的可解释的模型。\nAdvanced: Regularization\rregularization来解决过拟合的问题，提高模型泛化能力。\nL2 regulaization L1 regulaization lasso ridge Embeddings\r分布假说：相似的上下文总会表现出相似的意思。\nEmbeddings分类\nstatic embeddings contextualized embeddings 学习embeddings和它的意义的理论称为向量语义。\n自监督模型 representation learning的一种 无需通过特征工程的人工制造representations Lexical Semantics\rlemma：citation form，词元，引用形式，一个词的基本形式。 wordform：词形，一个词的具体使用形态。 word sense：词义。 synonymy：同义关系。 word similarity：词语相似度。 SimLex-999中就让人们给一个词和另一个词的相似度打分。 word relatedness/association：词汇关联性，所有能让词汇有关联感的关系。 semantic fields topic models：特别地有Latent Dirichlet Allocation，LDA 最常见的关系 hypernymy or IS-A antonymy mernoymy connotation Vector Semantics: The Intuition\r一个单词可以表示为多维语义空间中的一个点。而这个多维语义空间是从单词邻居的分布规律中推导而来的。\ntf-idf word2vec cosine Simple Count-based Embeddings\r词汇表一般在1-5万之间 稀疏向量表示：多数数值为0，目前有有效算法来有效存储和计算 权重函数 计数的时候可以用权重函数 目前最流行的方法是tf-idf 还有一些历史权重方式 Cosine for Measuring Similarity\r使用余弦计算相似度。适用于稀疏长向量。\nWord2vec\rembeddings要区别于原来的稀疏长向量，通常指短而稠密的向量。\n学习的权重变少，学习更快 有助于泛化和避免过拟合 能够更好捕捉同义性 skip-gram with negative sampling（SGNS）是word2vec两种方法的一种。word2vec是一种静态embedding方法，区别于动态embedding，如BERT表示。\n这里有一个极具创新性的想法 ——不直接计算 “词与词的关联”（共现矩阵），而是通过一个 “预测任务” 让模型自动学习这种关联，再将学习成果（权重）作为词嵌入。\n这被称为自监督方法 Skip-gram 模型的核心思路如下：\n将目标词与其相邻的语境词视为正例。 从词汇表中随机选取其他词语，作为负例。 利用逻辑回归训练一个分类器，使其能够区分上述两种情况（即区分 “目标词与语境词是相邻关系” 和 “目标词与随机词无相邻关系”）。 将训练过程中学到的权重作为embedding。 The Classifier\rSkip-gram目标是训练一个分类器，计算这个地方填这个词的概率。\n核心思路：一个词是否可能出现在目标词附近，取决于它的嵌入向量与目标词的嵌入向量是否相似。 相似度计算：点积 点积结果并非概率值，还需要经过Sigmoid函数运算‘ Skip-gram模型其实是存储了每个单词的两个embeddings，一个作为目标词，一个作为上下文。分别从target matrix W和context matrix C矩阵中学习。\nLearning Skip-gram Embeddings\rpositive examples：上下文滑动窗口\nnegative examples：词汇表中随机抽取，一般是positive examples的k倍，由目标词$w$和噪声词组成。\n在抽样的时候，会设置一个权重系数$\\alpha =0.75$来调整概率$P(w)$避免总是选择高频词。\n学习目标：\n最大化positive examples中学习的相似度 最小化negative examples中的相似度 如图，目标是使得apricot和jam的点积更大，和matrix、Tolstoy的点积更小。\n在学习完了之后，一般只使用W来表示。\nOther kinds of static embeddings\rfasttext\n解决了word2vec的未知词问题 subword models GloVe\nGlobal Vectors 基于词词共现矩阵的概率 word2vec可以看成间接优化一个 “带PPMI（Positive Pointwise Mutual Information）权重的共现矩阵” 的函数。\n我的一些想法：GNN是否也可以做类似的工作？GNN目前的方法就是直接处理图结构，进行局部图结构学习。不专门训练一个模型进行预测，而是把这个训练的模型的参数直接作为一个向量使用，反而能够更好地捕捉到隐含关系。\n这种想法是否可以将LLM和GNN更好地连接起来？\nVisualizing Embeddings\r直接列出最相似的单词 聚类算法 t-SNE：让低维空间中 “点与点的相似概率”，尽量和高维空间中 “点与点的相似概率” 一致 Semantic Properites of Embeddings\r关联和相似的区别：越短的上下文得到的向量，相似越好找，关联越难；越长正好相反。\n一阶共现：组合关系，一起组合出现，如write和poem 二阶共享：聚合关系，直接相关，如write和say 类比关系：平行四边形模型 问题是只能用在明确的关系、短的距离和频繁的单词上。\nEmbeddings and Historical Semantics\r嵌入的应用。\n很有意思。\ngay：愉悦的-\u0026gt;明亮的-\u0026gt;男同性恋 broadcast：散播-\u0026gt;报纸-\u0026gt;BBC awful：庄严的-\u0026gt;可怕的 Bias and Embeddings\rallocational harm\nembedding不仅反映输入，还放大偏见。\nEvaluating Vector Models\r相似度度量：\n不含上下文 WordSim-353 SimLex-999 TOEFL dataset 含上下文 SCWS WiC semantic textual similarity task 类比度量：\nSemEval-2012 Task 2 dataset 所有的Embedding算法都会存在固有的变异性。建议使用bootstrap采样后的文档中训练多个embeddings并平均。\nNeural Networks\rMcCulloch-Pitts neuron\nfeedforward\ndeep learning\nUnits\rbias term\nactivation：使用non-linear functions，如sigmoid、tanh、ReLU等\nsigmoid和tanh的问题：特别容易出现饱和的现象，如当特别靠近1的时候，此时的导数接近于0，输入的微小改变将无法引起输出的任何变化。这种现象称为梯度消失。 The XOR Problem\rMinsky and Papert：一层神经元无法解决异或问题。如感知机。\nThe Solution: Neural Networks\r包含隐藏层的多层感知机MLP解决了这个问题。\n下面这个图，不管对于原来的[0,1]还是[1,0]都会将他们在隐含层里转化为[1,0]。而原来的[0,0]和[1,1]则变为[2,1]，此时便线性可分。\nFeedforward Neural Networks\r与RNN对应，前馈神经网络（FNN）不带有循环。\n由于历史原因，FNN也称为多层感知机，MLPs，事实上现在的多层网络中已经不是感知机了。 感知机使用阶跃函数，现在的神经网络用的是多种非线性的单元如ReLUs或者Sigmoid等 前馈神经网络的标准结构是全连接的。 在这里，W作为输入层到隐含层的权重矩阵，U则是隐含层到输出层的权重矩阵。 $$h=\\sigma(Wx+b)$$ $$z=Uh$$ 在得到输出结果$z$之后，由于$z$是一个实数值向量（其实就是logits），而分类需要的是概率分布向量，所以要对其进行归一化（normalizing）。这里使用的是softmax函数。 $$y = \\text{softmax}(z)$$神经网络和多项逻辑回归的区别：\n有许多层 中间层的激活函数不只使用sigmoid 特征可以不只是由人工的特征模板设计，而可以由网络自身得到 逻辑回归可以理解成一层的神经网络。\nMore Details on Feedforward Networks\r为什么激活函数要非线性\n替换偏置项：使用dummy node来代替原来的偏置项。也就是下图中把b换成一个新的固定为1的$x_0$。\nFeedforward Networks for NLP: Classification\r嵌入矩阵、表示池化、表示学习先不讲，先学一下用前馈神经网络解决分类问题。\nNeural Net Classifiers with Hand-built Features\r人工设计的特征，除了把MLP换成FNN之外没变化。 Vectorizing for Parallelizing Inference\r单个样本特征维度是d，有m个样本，就可以将输入写成一个矩阵X为m×d维。\n其他的偏置项等也可以写成矩阵的形式，有助于最后的运算。此时有 $$H=\\sigma(XW^T+\\textbf{b})$$$$Z=HU^T$$$$\\hat{Y}=\\text{softmax}(Z)$$ 这里的X中行向量表示一个样本的完整特征。有的时候会写成$WX+b$，有的时候是$XW+b$。注意X的形状有所不同。\nEmbeddings as the Input to Neural Net Classifiers\rstatic embeddings代替hand-designed features。\n存储static embeddings的词典称为embedding matrix E one-hot vector：从embedding matrix选取token embedding的方法 分类器：\nconcatenation：适合对token顺序和细节敏感的任务，如语言建模 pooling：适合对整体语义敏感的任务，如情感分析 mean-pooling max-pooling 神经语言模型和N元语法模型的区别：\n可以处理更多的上下文，更加泛化，预测更准确；速度更慢，训练更麻烦 使用embeddings表示词而不是word identity Training Neural Nets\r损失函数、交叉熵\n误差反向传播/反向微分\nLoss Function\r交叉熵损失函数：负对数似然损失。用于输出为 “类别概率” 的任务。\nComputing the Gradient\r一层的时候可以用损失的导数，但是多层的时候需要用反向传播算法。\nComputation Graphs\r介绍了什么是计算图。\nBackward Differentiation on Computation Graphs\r反向传播：用链式法则一次性计算出所有参数的梯度。 先正向传播一次，然后反向传播计算。\n正向传播是为了得到损失值 反向传播是为了得到梯度，从而进行参数优化 当然在真正的神经网络中，计算会更加复杂。\n但是方法万变不离其宗。\nMore Details on Learning\r神经网络优化问题是一个非凸优化问题。目前有了很多好的正则化方法：\n初始值不设为0，而是随机的一些小的数 dropout 超参数：Adam等 GPUs等计算加速\nLarge Language Models\rELIZA\nDistributional hypothesis\nPretraining\n语言模型：依据前文预测下一个词的分布\n发展：\nN元语法 LSA/LSI，隐含语义分析，开始用向量表示词 神经网络语言模型neural language model RNN语言模型 word2vec 预训练技术 Transformer提出 掩码语言模型 自回归语言模型 如果可以预测下一个词的概率分布，那么就可以从概率分布中进行采样，从而生成下一个词。这样就从预测模型变成了生成模型。\n因果语言模型/自回归语言模型：从左到右依次生成 掩码语言模型：BERT等，可以同时利用左右两侧的信息 生成式AI\nThree Architecture for Language Models\r三种结构：\ndecoder：上文中的架构。输入为一系列的token，依次迭代生成输出的token。用于自回归语言模型。\n生成文本等，如GPT encoder：用于掩码语言模型。输入为文本，输出为标签。\n分类文本等，如BERT encoder-decoder：输入为一串token，输出也是一串token。\n相比decoder来说，和输入输出token的关系更不紧密，这类模型用于在不同类型的标记之间进行映射\n机器翻译等\n他们都是基于神经网络构建的。\nConditional Generation of Text: The Intuition\r不管什么任务，都可以简化为给定prompt的下一个词的预测任务。\nPrompting\r指令微调\nprompt\nDemonstrations Few-shot prompting Zero-shot prompting 这种Demonstrations可以手动筛选，也可以由优化器如DSPy来自动选择。此外，Demonstrations似乎并不是一定要给正确的问题和答案，错误的也行，主要作用是格式。\nprompt：可以看做一个学习信号。提示词不会更新模型的权重，其改变的仅仅是模型的上下文信息以及网络中的激活状态。\nin-context learning：参数没有改变的学习 System prompt：影响全局的一个文本prompt，被添加到所有用户prompt或查询的前面 Generation and Sampling\r语言模型的内部网络会生成logits，再由softmax计算得到概率，随后在这些token中进行采样。\ndecoding：基于概率选择token生成的过程常称为decoding\n自回归生成 D\nGreedy decoding\r贪心解码：选择概率最高的那个token生成（argmax）\n效果不好——输入文本如果相同，结果是固定的。\n束搜索\nRandom Sampling\r按照分布采样，直到采样到EOS。\n效果也不好——有些token虽然占比小，但是这些token很多，导致占比也不小。如果被采样到了，句子会变得很奇怪\nTemperature Sampling\rlogits转化为probability的时候用带有temperature的softmax计算。\n当$\\tau \\le 1$的时候，会倾向将高概率拉得更高，低概率拉得更低，反之则更容易选择到低概率事件。\nTraining Large Language Models\r一般分为三个阶段：\n预训练 指令微调 偏好对齐 Self-supervised Training Algorithm for Pretraining\rTeacher forcing：永远给模型正确的序列，而不是按照模型的预测接着往下来预测下一个词\n如下图，网络中的权重会通过梯度下降进行调整，以最小化该批次上的平均交叉熵损失。\n这些权重包括嵌入矩阵 E。由此学习到的嵌入，将能最有效地预测后续词语。\nPretraining Corpora for Large Language Models\r训练数据可以用网络数据，并且加上一些精心筛选的数据。\ncommon crawl Colossal CLean Crawled Corpus The Pile Dolma 避免个人信息PII，去重，安全筛选，toxicity detection。注意版权、数据同意、隐私和偏差等问题。\nFinetuning\r对已经与训练过的模型加入一些新的知识进行微调。如果新的数据是预训练的后面，也可以叫continued pretraining。\nEvaluating Large Language Models\rPerplexity\r由于链式法则，使用对数似然来作为衡量语言模型性能的指标的话，测试集的概率大小会受到token数量的影响。文本越长，测试集的概率越小。\nPerplexity：困惑度，长度归一化的指标。困惑度的具体公式是测试集概率的倒数，再按标记数量进行归一化。 $$\\begin{aligned} \\text{Perplexity}_{\\boldsymbol{\\theta}}(w_{1:n}) \u0026 =P_{\\boldsymbol{\\theta}}(w_{1:n})^{-\\frac{1}{n}} \\\\ \u0026 =\\sqrt[n]{\\frac{1}{P_{\\boldsymbol{\\theta}}(w_{1:n})}} \\end{aligned}$$ 困惑度越低，模型越好。\n困惑度非常依赖tokens的数量，因此不能对两个使用不同tokenizer的模型进行比较，而是只能对使用同一个tokenizer的模型比较。\nDownstream Tasks: Reasoniong and World Knowledge\r准确率：可以直接使用下游任务来衡量。\nMMLU 但是问题是数据泄露。\nOther Factors for Evaluating Language Models\r模型大小，训练时间，推理时间，GPU数量\n公平\nleaderboards\nEthical and Safety Issues with Language Models\r大模型幻觉问题：RAG\n安全问题：安全微调和对齐\nrepresentational harms\n隐私问题\n情感依赖\n增长谎言、宣传、虚假信息等文本生成\n","date":"2025-08-28T19:28:49+08:00","permalink":"https://ionfeather.github.io/2025/speech-and-language-processing/","title":"书籍阅读 | Speech and Language Processing"},{"content":"江南好，风景旧曾谙。日出江花红胜火，春来江水绿如蓝。能不忆江南？\n在离开浙江之前，我没有想过我会思念它那炙热的夏天，寒冷的冬天，无休止的蝉鸣或者是家旁边平平无奇的山。这次回家是边旅游边回家，从北京一路向南（离开有你的季节），第一站是湖南长沙，第二站是江西南昌，第三站才是家。\n长沙和南昌的景点似乎是有点少。由于只有一个人玩，我只能在城区附近活动，景点似乎有点大差不差的意思，而且去的时候天总是灰蒙蒙，上面是阴雨，我从北京到长沙，从长沙到南昌，从南昌到家，越来越热越来越热。\n不同城市的氛围非常不同，长沙近川渝而南昌近江浙。在长沙凌晨12点似乎也只是夜生活的开始，而南昌在我11点的时候便再也找不到晚餐正餐的进食地了。\n在回家之后我还有第四站。妈妈和我带上外婆，我们前往杭州。有些人会对自己的大学产生感情，有些人会对自己的大学所在地产生感情，杭州是一个值得你为他产生感情的城市，它的情感很奇妙，在我读大学的时候，我还没有感受到，离开了之后也不曾想念。但当我回到这个地方，闻到空气中弥漫的水蒸汽，看到西湖边熙熙攘攘的人群，我会开始思念它。「未能抛得杭州去，一半勾留是此湖。」我可能思念的是之前的无忧无虑的青春，思念那段兵荒马乱的疫情岁月中的时光。\n我重访杭州，但妈妈和外婆应该是没有作为游客的身份来过。\n「最爱湖东行不足，绿杨阴里白沙堤。」我和外婆在湖滨银泰旁边玩，没有往湖的西侧游玩。她的腿脚并不是非常好，因此我们的酒店也是订在了龙翔桥旁边，出行非常方便。而灵隐寺是外婆最爱的景点，「老佛店」，她这么称呼，外婆却展现出了巨大的实力——她能够从前殿爬到最后的后殿，我都有些气喘吁吁，她还是一副非常开心、精力充沛的样子。但下午的动物园之行，外婆就直接宕机了，她不爱看大象老虎和长颈鹿，没有什么好奇。或许对于外婆来说，赋予旅行意义的，是她过往的经验而不是未知的好奇了。\n在我思念过往、沉湎于过往大学生活的时候，我不也是外婆吗？这么一想，赋予人生意义的，可能并不是未来，而是过去。\n","date":"2025-08-26T19:08:55+08:00","image":"https://ionfeather.github.io/2025/the-end-of-heat/cover_hu_1119a43d87964c61.jpg","permalink":"https://ionfeather.github.io/2025/the-end-of-heat/","title":"处暑 | \r江南好，风景旧曾谙"},{"content":"日常\r进入了健身的关键时刻！感觉最近的效果还是蛮不错的，好像有那么一些些形状要出来了。可能这就是那些大佬说的新手福利期？\n周五和彭打球，她问我有没有看过《罗小黑战记2》，我果断摇了摇头——虽然我大学有一个同学超级喜欢，在QQ空间里天天发它的二创——但我确实没有自己看过，只是知道它那个一年一集动漫的「龟速传说」。她非常推荐：「那你必须去看看，真的很好看。」我：「好的，收到。」\n于是乎，周五晚上问了在百度上班的王，我俩果断决定第二天超级合生汇看《罗小黑战记2》。在经典绕着超级合生汇转了一圈后，并吃了一顿好吃的（安三胖烤肉），最终来到了电影院门口。真的很好看！怪不得彭成为了它的「自来水」，我和王也都是赞赏有加——简洁但是不简单的画面，有脑子的反派和克制的对话，使得这个电影非常有质感，打斗精彩、笑点高级、剧情高潮接二连三，我们俩看完之后夸了一路。说实在的，我觉得比年初的《哪吒2》要好看。等我回到家，我已经俨然成为了昨天的彭，直接开始向好多人推荐：「SH、LB，等你们论文写完，你必须去看看！」「严老师请有空看看这个，还蛮好看的。」「\u0026hellip;\u0026hellip;」\n2025-08-02\n似乎有点最近太懈怠了。\n身体已经无法适应阳光了，成为了一个蜗居的「老鼠人」。不知道是怎么回事呢？可能我是一个没有压力就会有一些软趴趴的人。\n我是人类一败涂地里的小人。\n2025-08-05\n别管了，放暑假了！暑假的计划还没有——我是不是有点太p了。\n2025-08-07\n","date":"2025-07-22T15:44:41+08:00","image":"https://ionfeather.github.io/2025/greater-heat/cover_hu_4c7b94ca18f80979.jpg","permalink":"https://ionfeather.github.io/2025/greater-heat/","title":"大暑 | Shall I compare thee to a summer's day?"},{"content":"最近迷上了绝区零。\n嗯呢？嗯嗯呢！（可能是太久没玩过这种养成+动作类的游戏了，每天上线打一打，玩的时候不仅有一些弹刀、割草的爽感，而且还有和游戏里角色谈恋爱的期待。不仅如此，期待2.1版本到来抽星见雅的过程我感觉很有成就感。之前玩皇室战争，那个奖励简直\u0026hellip;\u0026hellip;我特别想把12级卡升级到14级，等压实在是严重得离谱，但是升级一张卡可能就要花费我的一个月之久，而且如果要算上觉醒卡的话，皇室战争的强度实在是高得离谱。绝区零相比于皇室战争的奖励要大方太多。）\n照片拍摄于奥森北园的向日葵花田中，那天天气正好，向日葵盛开，亲子马拉松在奥森公园举办，不少跑者在我的身旁路过，我漫步其中，看看盛开的荷花、一大团一大团的积云、河流上的皮划艇和城市绿道上的里程碑。\n有的时候，我就是需要一些出去将自己的腿走得筋疲力尽的日子。只有在自然里，一些即使在家里休息时也无法恢复的能量才能够充分地恢复。\n2025-07-05\n","date":"2025-07-15T15:00:41+08:00","image":"https://ionfeather.github.io/2025/lesser-heat/cover_hu_1e503f7cba450982.jpg","permalink":"https://ionfeather.github.io/2025/lesser-heat/","title":"小暑 | 在热浪的中心呼唤：嗯呢？"},{"content":"日常\r学校里的杏子成熟了。路上可以看到杏子挂满枝头，虽然旁边还会悬挂一个「喷洒农药，请勿采摘」的标语，但是事实上标语是吓唬人用的，书院都会举办采摘的活动。\n「梅子金黄杏子肥，麦花雪白菜花稀。」初夏的天气让人心情愉悦。在周一跳完爵士舞的活动之后，我路过杏子没忍住摘了几个，味道很不错。\n2025-06-04\n季会应该是最难开起来的会。在拖了两周之后，终于在6/22开了。\n会议举办地点是在本部电子楼，听老师们和师兄们聊了聊自己的毕业情况——感觉收入和去向都还是蛮好的。希望等我毕业的时候还能有这么好的市场。聊完之后大家去木铎处合影，我直接掏出了我从JT那里借的富士XT50，摄影师要重出江湖了。\n在吃饭之前，我还和SH在各个地方拍一些照片。感觉我最近摄影水平真的变高了诶，在邱季端体育馆门口我拍到了超级好看的照片。\n2025-06-22\n当当当！第一次开单接客。\n这次的客人是化学系的同学，研究生是在北大。其实是朋友了（她还在我的影响下买了一台尼康Z50II，我们尼康神教也是拉人入坑了）。人超级好，中午请我吃了一顿日料之后，她先去化妆，再和同学合照，最后和我会合，拍毕业照。\n我午饭吃完之后困倦得不行，找了教八楼我熟悉的阶梯教室睡觉，可是，人好多，我睡不着。那「闲着也是闲着，不如出去转一转？抓个人拍照？」，我这么想。很幸运，在牡丹园那儿直接就抓到了4个环境学院的同学——我和环境特别有缘啊——三女一男在拿一个小CCD相机互相拍照呢。\n人像三要素的含金量不言而喻。我拿着那台富士相机，感觉真的咔咔出片，其中的一个女生看着超级眼熟，笑起来更像，眉眼中带着娇俏，但是想不起来之前在哪里见过了。还有其中的一个男生和女生是小情侣，拍照时候的互动和小表情，感觉好青春。\n拍完这四个同学之后，接我朋友的单子感觉就自信不少——教八楼的爬山虎、「学为人师行为世范」的校训碑、木铎金声和向日葵花田，只可惜操场因为周四要开学校的毕业典礼临时关闭了。\n2025-06-24\nstatus.cafe\r特别感谢为博客制作猫咪拍立得与Status Cafe挂件 | 再會，謝謝所有的魚，代码全靠博主的分享和ChatGPT，实在是炒鸡感谢！\n","date":"2025-06-25T22:16:34+08:00","image":"https://ionfeather.github.io/2025/the-summer-solstice/DSCF1934_hu_ef107ab0bfc94f42.jpg","permalink":"https://ionfeather.github.io/2025/the-summer-solstice/","title":"夏至 | 摄影大师再出世"},{"content":"Reasoning-based drug repurposing\r问题\u0026amp;回答\r问题\rLLM在Repurposing中的应用情况 是否有基于文本结构，引入大模型知识作为一部分特征？ 测试集是什么 传统模型/基于网络的模型的准确率 回答\r有两种情况。第一种，引入LLM后的文本信息作为向量嵌入到GNN中；第二种，LLM作为筛选数据的方法，从数据层面提升最终效果。 有的。将文本以特定的形式组织，如\u0026quot;药物A[SEP]疾病B\u0026quot;，然后将其变成向量，嵌入到GNN中。 阅读了几篇，测试集有 B-dataset（SCMFDD-S/L）：来自于 Predicting drug-disease associations by using similarity constrained matrix factorization | BMC Bioinformatics | Full Text，这篇文章里主要介绍了一个相似性约束矩阵分解的方法，里面编制了 SCMFDD-S 和 SCMFDD-L 数据集，其中的前者是这里的 B-dataset，适用于常规场景。 C-dataset：来自于 Drug repositioning based on comprehensive similarity measures and Bi-Random walk algorithm | Bioinformatics | Oxford Academic，由 Dndataset 和 F-dataset 整合而成，结合了药物的 ATC 编码和疾病的 DO 术语。包含 963 种药物、1263 种疾病和 54921 个药物-疾病关系。适用于多源数据融合场景。 F-dataset：来自于 PREDICT: a method for inferring novel drug indications with application to personalized medicine | Molecular Systems Biology，侧重于模型对文本知识的分析能力。 R-dataset：整合 C-dataset、F-dataset 和 KEGG 数据库的信息，用于极端稀疏和不平衡数据下的鲁棒性。 模型准确率在不同的数据集上不同。这里的B、C、F数据集使用得比较多，一般来说衡量标准是AUC、AUPR、F1-score和Precision。 Action\rWeek\rweek1\n相关论文调研： 是否有用大模型推理在repurposing里做的？ 是否有基于文本\u0026amp;结构特征做repurposing，然后把大模型reasoning的知识作为一部分feature加入，能够提升其效果，找出可复现的工作。\nweek2\n找出可复现的工作（上周遗留）， repurposing任务的测试集是什么？ 在这个测试集上 传统大模型是多少准确率，基于网络的方式是多少准确率？\n相关论文调研\rLLM-DDA\rEmpowering Graph Neural Network-Based Computational Drug Repositioning with Large Language Model-Inferred Knowledge Representation - PubMed\n问题与回答\rQ1. LLM在Repurposing中的应用情况\nLLM的推理信息在这里被引入了GNN中作为一部分特征\nQ2. 是否有基于文本结构，引入大模型知识作为一部分特征？\n文本结构经过LLM编码后的向量，引入到GNN网络中\nQ3. 测试集是什么\n见下面的数据集部分。一共使用了四个数据集。\nQ4. 传统模型/基于网络的模型的准确率\n与下面四种baseline进行比较 机器学习方法：DDA-SKF、NIMCGCN； 矩阵分解方法：SCPMF、DRWBNCF； 深度学习/GNN方法：REDDA、LAGCN、HDGAT LLM 直接预测DirectPred 这里的前三种就是传统模型等其他方法。在下方的结果处说明了AUC、AUPR、F1-score和Precision的值。\n解决的问题\r现有图神经网络（GNN）方法过度依赖网络拓扑结构，受限于不完整、含噪声的网络数据，且忽略生物医学领域丰富知识的问题。通过整合大语言模型（LLM）推断的知识表示，提升 DDA 预测的准确性和可靠性。\n面临挑战\r数据层面：传统GNN方法以来药物-疾病异构网络存在稀疏性和标签不平衡的问题 模型层面：现有方法难以捕捉复杂关联 知识利用：LLM生成的离散文本如何转化为适合GNN推理的连续数值表示，并设计高效融合架构 核心方法\rTL;DR：关键在于知识挖掘（LLM 提示）→ 语义编码（嵌入生成）→ 图模型融合（关联预测）\nStep1：构建异质网络\nStep2：利用LLM的知识来拓展潜在知识\nStep3：基于LLM的嵌入生成\nStep4：构建LLM-DDA模型\n一、药物-疾病异质网络构建（Drug-disease heterogeneous network construction）\n数据来源：整合 DrugBank（药物数据库 ）、OMIM（人类孟德尔遗传数据库 ）、MeSH（医学主题词表 ）等生物医学数据库 。 构建逻辑： 先计算 药物-药物、疾病-疾病的成对相似度（Pairwise similarities）（如化学结构、基因功能相似性 ）。 再结合已知 药物-疾病关联（Drug-disease associations） ，构建成包含药物、疾病节点，以及相似度、关联关系边的异质图网络 ，为后续分析提供拓扑结构基础。 二、零样本提示工程（Zero-shot prompt engineering）\n核心动作：设计 基于化学生物特征的提示模板（Chemobiomedical characteristics-based prompt template design） ，把药物 / 疾病的专业属性（如靶点、作用通路、临床特征等 ）转化为大语言模型（LLM）可理解的指令。 功能价值：借助 GPT-4 的知识推理能力，生成目标药物 / 疾病的详细描述 ，挖掘数据库外的潜在知识关联，引入LLM知识。 三、基于 LLM 的嵌入生成\n双模型进行嵌入生成： 用 ChatGPT-4 Turbo（通用大模型）、BioBERTpt（生物医学专用预训练模型 ），对 GPT-4 生成的文本描述做编码。 输出 LLM 嵌入，把生物医学语义信息转化为数值向量，让后续图模型能 理解语言知识。 四、LLM-DDA 模型构建（LLM-DDA model construction）\n多架构覆盖：提供 3 种融合策略，适配不同场景需求： LLM-DDA (node feat)：直接把 LLM 嵌入作为节点特征，融入传统图神经网络，轻量且易部署。 LLM-DDA (graph-graph)：双图网络并行，分别处理 LLM 嵌入图与原始异质图，再融合结果，强化多源信息互补。 LLM-DDA (graph-ae)：结合图自动编码器（Graph-AE），优化嵌入特征与拓扑结构的融合，适合挖掘深层关联。 输入衔接：模型输入关联两部分： Similarity features（相似度特征 ）：来自第一步异质网络的拓扑计算。 Adjacency matrix（邻接矩阵）：刻画药物-疾病的关联结构，让模型同时学习知识语义（LLM 嵌入）和网络拓扑（图结构），提升药物-疾病关联预测精度。 数据集\r四个药物-疾病的基准数据集\nDataset Drugs Diseases Drug-disease Associations Pos-Neg Ratio B-dataset 269 598 18,416 11.45% C-dataset 663 409 2,532 1.57% F-dataset 593 313 1,933 1.05% R-dataset 894 454 2,704 0.67% B-dataset：来自于Predicting drug-disease associations by using similarity constrained matrix factorization | BMC Bioinformatics | Full Text，这篇文章里主要介绍了一个相似性约束矩阵分解的方法，里面编制了SCMFDD-S和SCMFDD-L数据集，其中的前者是这里的B-dataset，适用于常规场景。 C-dataset：来自于Drug repositioning based on comprehensive similarity measures and Bi-Random walk algorithm | Bioinformatics | Oxford Academic，由Dndataset和F-dataset整合而成，结合了药物的ATC编码和疾病的DO术语。适用于多源数据融合场景。 F-dataset：来自于PREDICT: a method for inferring novel drug indications with application to personalized medicine | Molecular Systems Biology，侧重于模型对文本知识的分析能力。 R-dataset：本文自己提出。整合C-dataset、F-dataset和KEGG数据库的信息，用于极端稀疏和不平衡数据下的鲁棒性。 指标结果\rAUC、AUPR、F1和五折交叉验证结果\n与下面四种baseline进行比较 机器学习方法：DDA-SKF、NIMCGCN； 矩阵分解方法：SCPMF、DRWBNCF； 深度学习/GNN方法：REDDA、LAGCN、HDGAT LLM 直接预测DirectPred 分别通过AUC、AUPR、F1-score和Precision来判断效果。\nLBMFF\rFrontiers | Drug–disease association prediction with literature based multi-feature fusion\n问题与回答\rQ1. LLM在Repurposing中的应用情况\n这里使用了BERT来挖掘文献中的关联关系。\nQ2. 是否有基于文本结构，引入大模型知识作为一部分特征？\nBERT经过预训练和微调之后，能够学习文献中的一些关联关系，输出包含语义关联信息的向量表示，从而文献中的药物-药物相似性关系和疾病-疾病相似性关系知识。\nQ3. 测试集是什么\n见下面的数据集部分。一共使用了两个数据集。\nQ4. 传统模型/基于网络的模型的准确率\n对比的模型分为如下部分，都是各自领域的SOTA模型。\n图神经网络（GNN）：DRHGCN、LAGCN、REDDA 矩阵分解与正则化：BNNR、DRWBNCF 矩阵补全与神经归纳：NIMCGCN 核融合与传统机器学习：DDA-SKF 对比结果如研究结果部分所示。\n解决的问题\r药物重定位中的关联预测问题\n面临的挑战\r文献信息利用不够充分：现在大多数计算方法都是依赖结构化数据库，但是科学文献中有很多未被充分挖掘的药物-疾病关联关系。 多特征难以有效融合：药物和疾病的特征具有异质性。 核心方法\r提出了LBMFF（基于文献的多特征融合方法）。\n多源融合：同时用结构化数据库（Drugbank、SIDER 等）和非结构化文献（BERT 处理），挖掘更全面的药物-疾病关联。 图网络 + 注意力：用 GCN 处理 “药物-疾病” 关联的图结构，注意力机制强化关键特征，提升预测准确性。 相似性计算 从多源数据提取特征，计算药物、疾病的相似性，构建关联矩阵 药物相似性（Drug-Drug）：\n整合 3 类结构化数据 + 文献语义： 化学结构（Drugbank 数据库，药物分子结构） 副作用（SIDER 数据库，药物-副作用关联） 靶点（Drugbank 数据库，药物-靶点关联） 文献语义（BERT 模型处理文献，挖掘药物间语义关联）\n最终加权融合（α、β、γ 为权重），输出药物相似性矩阵。 疾病相似性（Disease-Disease）：\n整合 2 类数据： -MeSH 数据库（疾病树编号，疾病分类体系） 文献语义（BERT 模型处理文献，挖掘疾病间语义关联）\n输出疾病相似性矩阵。 药物-疾病关联（Drug-Disease）：\n直接从 CTD 数据库（比较毒理学数据库）获取已知关联矩阵，标记已验证的药物-疾病对。 特征表示 将上一步得到的 药物相似性矩阵、疾病相似性矩阵、已知关联矩阵 拼接，形成模型输入的融合特征矩阵，统一表征药物和疾病的多源信息。 编码器 用带注意力机制的图卷积网络（GCN） 学习药物、疾病的嵌入表示： 两层 GCN（Graph Convolution Encoder Layer）逐层提取特征，ReLU 激活增加非线性。 注意力机制（Attention mechanism）动态分配权重，突出关键特征（比如强关联的药物-疾病对）。 最终输出 药物 + 疾病的嵌入向量（维度 (m + n)×d ，m 是药物数，n 是疾病数，d 是嵌入维度 ）。 解码器 通过矩阵乘法还原关联预测： 药物嵌入（$Eₘₓ×d$ ）、权重矩阵（$W_d×d$ ）、疾病嵌入（$Eₙₓ×d $）相乘，重建药物-疾病关联得分。 预测输出潜在关联 实线：已知关联（来自 CTD 数据库）。 虚线：模型预测的潜在关联（未被验证，但算法判定有高可能性的药物-疾病对 ），可辅助药物重定位研究。 BERT在文中\r预训练\n数据准备 筛选包含药物、疾病名称的科学文献（如从 PubMed 获取，文中提到的 673,665 篇相关文献 ），这些文献包含药物-疾病关联的语义描述（如治疗、关联、机制等表述 ）。 将文献中疾病、药物相关文本构建为“药物文本-疾病文本” 对作为输入序列。 Masked LM 让BERT预测被遮盖内容，如[MASK]可以治疗糖尿病，需要推断出是药物X。 NSP 真实对：药物A[SEP]疾病B 虚假对：药物C[SEP] 疾病D 需要判断文本对是否是真实对 模型微调\n数据 使用CTD等数据库中已知的药物-疾病关联对。存在关联为正例，无关联为负例 输出语义特征 经过预训练 + 微调后，BERT 对输入的 “药物-疾病文本对”，会输出包含语义关联信息的向量表示 研究结果\r测试集\rZhang（SCMFDD-S）：来自于Predicting drug-disease associations by using similarity constrained matrix factorization | BMC Bioinformatics | Full Text，在上面被称为B-dataset。包含269种药物、598种疾病和18416个药物-疾病关系。本文中还从其他数据库中填充了化学结构、药物-靶点关系和药物副作用、疾病树作为药物-药物相似性度量的数据。 TL-HGBI：来自于Drug repositioning by integrating target information through a heterogeneous network model | Bioinformatics | Oxford Academic。包含963种药物、1263种疾病和54921个药物-疾病关系。同样地，也进行了信息填充来进行相似性度量。 指标表现\r和SOTA模型进行比较，分别是\n图神经网络（GNN）：DRHGCN、LAGCN、REDDA 矩阵分解与正则化：BNNR、DRWBNCF 矩阵补全与神经归纳：NIMCGCN 核融合与传统机器学习：DDA-SKF LBMFF在所有指标上都体现出了领先。\n此外，LBMFF在TL-HGBI在多数指标上仍优于对比方法，证明方法在大规模数据集上的泛化能力。\nLLM进行阴性数据标注\rImproving drug repositioning with negative data labeling using large language models-PubMed\n问题与回答\rQ1. LLM在Repurposing中的应用情况\nLLM引入了自己的推理能力来修改训练数据。\nQ2. 是否有基于文本结构，引入大模型知识作为一部分特征？\n没有。这里大模型的作用是通过自身的知识，在文本中筛选出更好的数据来提升最终效果的。\nQ3. 测试集是什么\n测试集是由AACT 数据库得到。作者手动选择了一部分数据（5 阳性 + 11 阴性 ）作为测试集验证泛化能力。\nQ4. 传统模型/基于网络的模型的准确率\n见研究结果。本文通过提升数据集中真阴性数据的质量来提高了最终的模型效果。具体来说，提升的效果还是比较显著的。\n解决的问题\r在药物重新定位中，监督机器学习模型因缺乏可靠的阴性数据（即因无效或毒性失败的药物）而预测准确性和泛化能力不足。\n传统的 Positive-Unlabeled（PU）学习方法通过随机采样或简单分类未标记数据作为阴性，存在误分类或决策边界简化的问题，导致模型性能受限。\nLLM通过进行分析文献和数据，系统挖掘真阴性数据，提升了阴性数据的质量。\n面临的挑战\r阴性数据获取困难：多数数据库会把试验中止就等同于阴性，但终止原因可能并不是因为药物疗效/毒性问题，而是资金不足等原因。 PU学习方法的局限性：随机采样未标记数据作为阴性会引入分类偏差，而基于聚类等策略的阴性筛选假设正负样本边界清晰，与真实场景不符，导致模型泛化能力差。 临床数据处理复杂性：临床试验文本存在表述不规范、结果不完整等问题，传统方法难以准确解析并识别真正的阴性药物。 核心方法\rGPT-4 系统性挖掘真阴性数据，为药物重定位提供 “高质量标注 + 精准模型” 方案，可扩展到其他疾病。\n临床数据获取：从 AACT 数据库获取 2539 个前列腺癌相关临床试验，排除未公布结果和因非疗效原因终止的试验后，剩余 1442 个试验由 GPT-4 分析。 GPT-4的训练和验证：使用 22 个手动策划的试验对 GPT-4 进行验证，其中包括 14 个无关试验、4 个阳性试验和 4 个阴性试验，GPT-4 成功正确分类所有试验。 特征构建： 知识-based 特征：从 DrugBank 获取药物-基因、药物-靶点、药物-通路相互作用、结构特性和靶点类别等，转换为独热编码。 network-based 特征：构建包含药物-药物相似性、蛋白质-蛋白质相互作用和基因本体论的多层生物网络，提取拓扑特征。 对比策略： GPT-4 标注：使用 GPT-4 识别的 26 个阳性和 54 个阴性药物。 无采样 PU：将所有未标记药物视为阴性，导致类别不平衡。 下采样 PU：从未标记药物中随机采样 43 个阴性，重复 100 次以控制类别不平衡。\n训练集和测试集来自于作者自己收集和整理。\n研究结果\r使用六种机器学习算法和5折交叉验证训练模型，测试集验证泛化能力（5 阳性 + 11 阴性 ）。\n1. 训练集表现（图 A）：\rGPT-4（🔴）：多数算法（如 LogL1、RF、SVM ）平衡准确率接近 0.9，说明用 GPT-4 标注的高质量数据，模型在训练集上 “学的好”，分类稳定。 No sampling（🟢）：部分算法（如 ANN、NB ）准确率暴跌，因阴性样本太多，学不到有效模式。 Under sampling（🔵）：表现居中，虽平衡了类别，但随机丢数据，训练效果不如 GPT-4 标注。 2. 测试集表现（图 B）：\rGPT-4（🔴）：多数算法（如 LogL1、RF、SVM ）MCC 显著高于其他策略，尤其是 LogL1、RF，说明模型泛化能力强，用 GPT-4 标注数据训练的模型，在真实测试集上 “预测准”。 No sampling（🟢）：MCC 普遍低（如 NB 甚至接近 0 ），模型被类别不平衡拖垮。 Under sampling（🔵）：MCC 比 GPT-4 低，因下采样丢了很多真实阴性信息，模型学到的规律不完整。 其他\rDrugGen\rDrugGen enhances drug discovery with large language models and reinforcement learning | Scientific Reports\n这篇文章介绍了一个基于DrugGPT的模型DrugGen，通过监督微调、近端策略优化和强化学习来引导模型生成更高质量的分子。\nDrugReAlign\rDrugReAlign: a multisource prompt framework for drug repurposing based on large language models - PubMed\n提出了一个多源提示的LLM药物重定位框架。\nBenchmark：LLM在八项化学任务中的表现\rWhat can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks\n评估了LLM在八项具体的化学任务中的表现，采用零样本、少样本上下文学习设置，最后得出GPT-4最佳，Davinci-003次之，GPT-3.5第三。\nBioBERT\rBioBERT: a pre-trained biomedical language representation model for biomedical text mining | Bioinformatics | Oxford Academic\n介绍了一个用于生物医学领域的预训练语言表示模型，其在BERT的基础上使用生物医学领域语料库进行预训练。\n","date":"2025-06-10T14:24:22+08:00","permalink":"https://ionfeather.github.io/2025/rbdr/","title":"AIDR | Reasoning-based Drug Repurposing"},{"content":"介绍\r基于网络的AIDR方法基本都是利用网络的拓扑结构进行探索。一般来说是在异构网络上进行处理，对于异构网络，有这么一个表示图：\n其中，绿色节点表示蛋白质、红色节点表示药物、蓝色节点表示疾病。这可以看成一个三层架构。网络可以具有多种类型的节点和多层架构。同类型节点之间的实线表示它们之间存在一定的关系。以药物层为例，关系类型可以如下：\n药物之间的结构相似性 两种药物共享一个相同的靶点 两种药物有协同作用或副作用 \u0026hellip;\u0026hellip; 虚线的双箭头线表示两种不同类型的节点之间的交互关系。同样的原理也适用于药物层以外的其他层。而右边的黑色实线箭头指的是三种类型节点的大概图。\n许多机器学习算法和神经网络算法都可以以这种异构网络作为数据的载体，例如随机游走、图神经网络等。异构网络将药物重定位任务带到了另一个层次，可以更高效地提取输入实体的特征并将其固定在一定维度上。\n对于基于网络的AIDR方法，参考Drug-Disease Association Prediction Using Heterogeneous Networks for Computational Drug Repositioning，将基于网络的AIDR方法分成三种：\n图挖掘 矩阵分解/填充 深度学习 这里有一个简单的思维导图：\n图挖掘算法\r矩阵分解/填充\r深度学习\r","date":"2025-06-05T21:24:53+08:00","permalink":"https://ionfeather.github.io/2025/aidr-network-method/","title":"AIDR | 基于网络的方法"},{"content":"妹妹很喜欢养一只小动物，她养过乌龟、绿萝，最想养的是一只小猫咪。但是毕竟妈妈并不是特别喜欢，所以最后养了一只金丝熊。具体来说应该是卷毛波利，但是在一个冬天又一个夏天之后，毛已经变得直直的，不知道还是不是应该这么叫它。妹妹也没有给它取名字，所以还是叫它仓鼠吧。\n在最开始的那段时间里，仓鼠没有跑轮，住的空间也非常狭小。它总是啃咬、越狱、吃绿萝。直到我考完研究生复试，回到宁波之后，因为它就住在我的书房里，我看到它总是一副焦躁不安的样子，于是上网查了查，给它买了一个大大的铁丝笼和跑轮，仓鼠的家被我搭建得还算是不错。\n收到跑轮的那天，仓鼠感觉瞬间就不一样了。咕噜噜咕噜噜，在上面疾驰。就算放它出来，也会跑到跑轮上腿一蹬开始训练。\n那段时间我刚考完研究生，心中的斗志激昂，也非常开心，就像是刚爬上滚轮的仓鼠。就觉得可以做很多事情——背古诗、减肥，想在外滩那里大声喊：「我考上研究生了！」。\n但是随着离考试越来越远，我也慢慢变得懒散。我觉得我是一个比较适合一段时间做一段事情的人，我经常这么想：学习就像是仓鼠跑轮子，如果一直跑，效率一直保持很高。但是如果休息了，就会慢慢停摆。如果说考研期间我的诀窍是什么，那就是不停地跑动轮子，不休息。有些人推崇的做六休一对我来说可能并不是那么适用——我怕轮子停下来。\n现在，离我考研已经过去一年多了，但是我发现自己的轮子似乎有一些生锈了。中午的时候还和严老师聊天的时候说：「二战把我燃尽了。」虽然是一句调侃，但是在现在，我确实有一些没有目标感了。科研任务对我来说有一点虚无缥缈，昌平的逼仄又导致没有什么活动可以参加。我明显感觉到，从去年9月到现在，时间像是仓鼠轮子越跑越快。\n虽然说了这么多「试问岭南应不好」，不过总是会「却道：此心安处是吾乡」。说实在的，在北京的这些日子过得很开心，有一些焦虑其实也是正常的。\n自己有点习惯于懒懒散散的状态，之前一直觉得很幸福，但是怎么突然就变了呢？\n","date":"2025-06-03T12:41:41+08:00","image":"https://ionfeather.github.io/2025/grain-in-beard/cover_hu_ae9656b550e5c1b.jpg","permalink":"https://ionfeather.github.io/2025/grain-in-beard/","title":"芒种 | 时间像是仓鼠轮子越跑越快"},{"content":"综述\r打算先看一下有哪些综述，先了解一下整个工作的图景。\n会议\r会议有三个人讲自己的调研。我理清一下每个人的要讲的部分。\nPart 1\r我介绍了自己调研的论文和阅读的文献。详情看我的2025-05-19的组会的PPT。\n我大概介绍了一下我阅读的 Repurposed Drugs - Current State and Future Perspectives | IntechOpen，这是一本书，其中的第三章感觉非常有借鉴意义。\n在组学、图挖掘算法和大模型部分分别介绍了看到的三篇论文：\nAI-powered omics-based drug pair discovery for pyroptosis therapy targeting triple-negative breast cancer | Nature Communications Biomedical knowledge graph learning for drug repurposing by extending guilt-by-association to multiple layers | Nature Communications DrugReAlign: a multisource prompt framework for drug repurposing based on large language models | BMC Biology Part 2\r阅读的文献为：Applications of Artificial Intelligence in Drug Repurposing - Wan - 2025 - Advanced Science - Wiley Online Library\n这是师妹做的思维导图：\nPart 3\r师弟的PPT。\n阅读的论文有：\nMultimodal Molecular Pretraining via Modality Blending | ICLR 2024 Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge | SIGKDD 2024 Mol-AE: Auto-Encoder Based Molecular Representation Learning With 3D Cloze Test Objective | ICML 2024 Equivariant Flow Matching with Hybrid Probability Transport for 3D Molecule Generation | NeurIPS 2023 Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D | ICLR 2023 ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling | ICML 2024 On Pre-trained Language Models for Antibody | ICLR 2023 ","date":"2025-05-23T22:56:23+08:00","permalink":"https://ionfeather.github.io/2025/aidr/","title":"AIDR | 方向调研"},{"content":"好久没写博客了，错过了立夏和谷雨。沉迷拍照和发朋友圈，博客大业被落下了——仔细看一下照片目录里：郁金香、济南旅游、运动会、国家植物园、雁栖湖、广东旅游、蔷薇花、China P\u0026amp;E展览\u0026hellip;事情好像变得不妙，博客写完这些好像有点费劲。\n","date":"2025-05-21T20:34:06+08:00","image":"https://ionfeather.github.io/2025/lesser-fullness-of-grain/cover_hu_6815453822e7625.jpg","permalink":"https://ionfeather.github.io/2025/lesser-fullness-of-grain/","title":"小满 | 春天一晃而过，夏天已然到来"},{"content":"在三月的某一天，SH突然问我：「清明要不要去沙漠？」我有点诧异——这个主意好酷，但是会不会有点难？或者说有点什么问题？报团嘛？会不会有什么不好玩的地方？在考虑了一周之后，还是决定出发——因为这个沙漠的主意听起来确实是太酷了好嘛！于是SH的室友唐老师以及我们四个同门踏上了这场包头-库布齐之旅。\n多年以后，面对互联网加班和不当人的领导，我会想起这场说出发就出发的旅行的那三天快乐时光——没想到这场旅行会这么好玩好吃好逛。\n请原谅我写个流水账。\n第零天\r为了沙漠之旅，我们约定去商场购入一些必要的东西，顺便吃了一顿特别好吃的杭帮菜——桂满陇。其实和我在杭州吃的新白鹿还是有不少区别，当时在那边吃的最多的还是蛋黄鸡翅、冰激凌+吐司和小炒黄牛肉。但是在这里吃的是叫花鸡、桂花糯米藕、脆皮豆腐（虽然好像后面两样没点来着……），还真的蛮好吃的。\n我们五人在商场中大逛特逛——好吧，其实也就是迪卡侬和骆驼。我、SH和唐老师购入了同一款骆驼的裤子，戏称为姐妹装。我也是当上姐妹了。\n第一天\r到达了包头之后，我们打车前往酒店，在草草收拾了之后，便再次打车。在闹出一点儿乌龙之后终于吃上了香香的蒙餐。\n不得不说，蒙古菜和杭州菜的量果然不同，我们五人在桂满陇点了6人餐，但是在包头吃了4人餐就差不多饱了。\n随后五人组在包头市内骑起来共享电动车——这个城市竟然没有共享单车，全是共享电动车——到达了城中草原。草原最美的时间应该是在6-9月，水草丰美，风吹草低见牛羊，在我们去的时候，还是黄了一些，在马儿驰骋的时候，一些沙尘会激起、回旋。\n最后的安排是在湿地公园眺望包头市——这个重工业的城市的春天还没有到来，包钢烟囱中的白烟飘向天空，似乎永远也不会停止。但是伴随着白烟的还有逐渐变成紫色的天空、绿水、绵延的房屋、在夕阳下展翅的麻雀和高速公路上不断出现又消失的卡车。\n「要不录一个转场吧！」大家提议。在夕阳西下时，我们「一二一」地向前走，录了一个傻气十足的转场。\n这个转场后来剪出来的视频还蛮好看的！超级有爱啊，超级喜欢。\n第二天\u0026amp;第三天\r第二天和第三天就是沙漠徒步了。这真是个重头戏。\n徒步从一早就开始，嘟嘟嘟乘着大巴到了沙漠。大巴车上遇到了严老师，非常活泼，是在西二旗旁边一家公司上班，要我叫她姐姐。看到我拿了两个丝带说我是双马尾，问我要装沙子的瓶子，一直嚷嚷着要看黄河——结果我和她到黄河边的时候正好睡了过去。\n穿上沙套、拿上登山杖、背好相机，「准备出发，全程预计需要4小时」。沙漠里的景色似乎是重复的重复，越过种植的网格形固沙植物后，遇到的沙丘上是细密的沙子，脚往下一踩，就会陷进去，每一步需要耗费平时许多许多倍的力气。\n我（应该叫帅哥，路上有个小孩非说我特别帅\u0026hellip;）和唐老师（这里应该叫仙仙\u0026hellip;叫她仙女姐姐给我笑个半死）还拿着相机，沙漠对于相机的破坏性简直强得可怕，无处不在的风沙打在UV镜和机身上，如果不是唐老师带了两个袖套保护镜头，估计50-250镜头的伸缩环里面会有无数的沙子卡在里面——但是我的侥幸心理，让我一直在切换16-50镜头和50-250镜头，还好没有损坏我的CMOS，但是我的16-50镜头最后的对焦环卡住了好多的沙子。\n别管了！与其在意什么相机的使用，不如看看美丽的、壮观的沙漠风光。\n终于到达了目的地，开始建设营地——我们找的地方刚好在一个大大的沙丘后面，远处的基站信号被它挡住了。只要是爬上沙丘，就会有信号，但是躺在我们的睡觉的帐篷里面，就肯定是没有的。不过这个是值得的，因为到了后半夜的时候，沙漠中接近零度的气温差点让我冻得发抖，如果在沙丘顶部扎营，风大一些我可能整个人要缩成虾了。\n在漫长的沙漠徒步和费力的建设营地之后，日落时分我沿着沙丘的山脊漫步，看着风卷出的痕迹，极目远眺，看到了「大漠孤烟直，长河落日圆」——长河是自阴山下千百年来不断流淌的黄河，而孤烟原来是从太阳飞来的航迹云。\n日落之后是篝火晚会。沙漠进入夜晚后的气温骤降，靠近火堆的地方才会温暖一些。火焰跳跃、温暖着大家。烟花、篝火和歌声是前半夜的主题，我和严老师在后半程占据了话筒，在那里唱《思念是一种病》《精忠报国》，大家围坐在火堆旁，领队拿了一些仙女棒给我们。\n后半夜我和唐老师一直在忙着拍照，我们五个人在那里摆pose都摆了好久好久。星空和人很难完美地融合在一起，但最后终于终于，得到了一张非常满意的照片。\n星空和沙漠简直是无敌的出片场景，我现在看到那些照片也是能回忆起当时的美好——或许不来江南，不知烟雨西湖；不来内蒙古沙漠中心，真的不知道什么是大漠，什么是边疆。仰望星空的时候远处无穷无尽的沙丘绵延起伏，我们寻找着大熊座、长蛇座和北极星，互相指给对方看。\n接近11点时，我们在寒风中冻得发抖，聊天的时候都感觉嘴巴在咯咯咯地响。我和唐老师去把钻到睡袋的两人卸下的羽绒服穿上，来抵御这0度的低温和刺骨的寒风。「来都来了」，我们一边聊天，一边还是在沙漠接近0度的地表温度和刺骨的风中，用相机延时拍下了星空和沙漠。\n睡觉时间都快2点了，我又在早上6点左右起床看日出。在大家都没起床的时间，我站在最高的沙丘上望着远处慢慢升起的太阳。日出的沙漠格外清冷，天气晴朗，水汽似乎在沙漠表面凝结，太阳从阴山山脉上升起，黄河自阴山下流过，飞机从我的背后向着日出飞去，划出一道道航迹。在大家醒来之后，太阳已经有了大约10度左右的高度角，\n第三天我们的返程格外艰难，太阳正式升起来之后，沙漠立刻变得炎热，风沙很大，风携带沙子往眼睛、耳朵、鼻子中钻。我难以形容当时的风沙有多大，实在是睁不开眼睛——沙漠徒步请必须要带上一个护目镜。唐老师开始往她的头上套塑料袋，但是风沙会和你做搏斗，要一只手抓住登山杖，一只手抓住塑料袋可不是一件容易事。不久，塑料袋就随着风飞往远方。\n不知道大家怎么能走得那么快，我和SH他们四个人（SY早就自己走了）中间并没有休息，但是在路程中还是掉在了队伍的末端。但慢也是一种幸运，在这里我看到了绝景——一行大雁排成人字形，飞跃沙漠。我的眼前只有湛蓝的天空、金黄的沙丘和大雁。\n慢悠悠慢悠悠，最终也是到达了我们此行的目的地。\n","date":"2025-04-04T21:18:11+08:00","image":"https://ionfeather.github.io/2025/pure-brightness/cover_hu_858b35936e867c3e.jpg","permalink":"https://ionfeather.github.io/2025/pure-brightness/","title":"清明 | 在雨纷纷的节气中见到「大漠孤烟直，长河落日圆」"},{"content":"阅读综述\rAi-Driven Drug Repurposing: Uncovering Hidden Potentials OfEstablished Medications For Rare Disease Treatment Structure-based drug repurposing: Traditional and advanced AI/ML-aided methods Artificial intelligence for drug repurposing against infectious diseases | Artificial Intelligence Chemistry 2024 Artificial intelligence in COVID-19 drug repurposing | The Lancet Digital Health 可用的模型\r目前的再利用方法的实例主要集中在网络医学和大规模的患者数据分析上。这里有CoV-KGE、BenevolentAI、异质有向生物医学图等。\n阅读论文\rArtificial intelligence in drug development | nature medicine\r这是一篇综述，讲解了人工智能在药物发现中的应用取得了显著进展，特别是在靶点识别、虚拟筛选、新药设计（de novo design）、ADMET预测和合成规划等方面。\n这里主要关注他讲的AI在药物重定位中的作用。\nA foundation model for clinician-centered drug repurposing | nature medicine\r摘要\r文章介绍了药物再利用人工智能模型在临床应用中的局限性，并提出了本研究的核心——TxGNN模型。\n解决的问题\r目前的模型过于依赖已有疗法和关于此疾病详细的分子机制，对于多疾病、零样本情境下的药物再利用没有好用的模型。\n挑战是什么\r现有计算方法假设待预测疾病已有可用药物，然而大量疾病（研究中 92% 的 17,080 种疾病）缺乏已知适应症，罕见病中约 95% 无 FDA 批准药物，85% 甚至无有潜力的治疗药物， 药物重利用的适应症可能与最初研究的适应症无关，现有机器学习模型对数据不完整、稀疏且无已知疗法的疾病，识别治疗候选药物的能力大幅下降。 核心方法\rTxGNN。图基础模型，在医学知识图谱上训练，利用图神经网络和度量学习模块，将药物再利用问题转化为零样本预测问题。\nTxGNN模型主要由两个模块组成：TxGNN Predictor（预测模块）和TxGNN Explainer（解释模块）。\nTxGNN Predictor（预测模块）： 功能：预测药物的适应症（indications）和禁忌症（contraindications）。 架构：该模块基于图神经网络（GNN）构建，优化了医学知识图谱（KG）中的关系。通过大规模、自监督的预训练，GNN为KG中的所有概念生成有意义的表示。然后，通过微调，该预训练模型被适应于处理治疗任务，并预测跨多种疾病的药物候选适应症和禁忌症。 度量学习模块：用于零样本预测，利用疾病可以共享疾病相关的遗传和基因组网络这一见解，从可治疗的疾病向无治疗方法的疾病转移知识。 TxGNN Explainer（解释模块）： 功能：解析KG，提取并简洁地表示相关医学知识，以生成多跳可解释的路径，这些路径解释了模型预测背后的逻辑。 架构：使用名为GraphMask的自我解释性方法，生成一个稀疏但充分的医学概念子图，这些概念被认为是TxGNN预测的关键。它为KG中的每条边生成一个0到1之间的重要性分数，并通过组合药物-疾病子图和边重要性分数，产生多跳可解释的路径，将疾病与预测药物联系起来。 优势\r在零样本条件下，TxGNN预测效果相比别的模型体现出了广泛的泛化性和准确性。\n数据集\r医学知识图谱公开，包含 10 种类型的节点和 29 种类型的无向边，共有 123,527 个节点和 8,063,026 条边，涵盖 17,080 种疾病（92% 缺乏 FDA 批准药物）和 7,957 种潜在药物重利用候选药物，还包含生物过程、分子功能、蛋白质、表型等多种生物医学概念信息。\n知识图谱下载地址：PrimeKG 演示界面：TxGNN Explorer Github仓库：mims-harvard/TxGNN In Silico Drug Repurposing for Anti-Inflammatory Therapy: Virtual Search for Dual Inhibitors of Caspase-1 and TNF-Alpha\rIn Silico Drug Repurposing for Anti-Inflammatory Therapy: Virtual Search for Dual Inhibitors of Caspase-1 and TNF-Alpha - PubMed\n摘要\r炎症与多种疾病相关，caspase-1 和 TNF-alpha 在炎症发展中起关键作用，针对二者的双重抑制剂有望治疗多种炎症性疾病。本文构建了基于定量构效关系和多层感知器神经网络的多条件模型（mtc-QSAR-MLP）用于虚拟筛选抗炎药物。经数据集处理、模型开发与评估，该模型准确性超 88%。利用此模型筛选出如 linagliptin、icariin 和 rolipram 等潜在的 caspase-1 和 TNF-alpha 双重抑制剂，为抗炎治疗提供新方向，有望用于自身免疫疾病、癌症及 COVID-19 等疾病的治疗。\n解决的问题\rcaspase-1 和 TNF-alpha是炎症发展中的关键蛋白，针对它们的双重抑制剂有望治疗多种炎症性疾病。\n挑战是什么\r通过药物再利用的方式来加速抗炎药物的发现。\n核心方法\r构建基于定量结构 - 活性关系（QSAR）和多层感知器神经网络（MLP）的多条件模型（mtc-QSAR-MLP），用于虚拟筛选 caspase-1 和 TNF-alpha 的双重抑制剂。\n数据处理与分子描述符计算：从 ChEMBL数据库获取 caspase-1 和 TNF-alpha 的化学及抑制数据，经筛选得到 1476 个分子案例。用MODESLAB v1.5软件计算多种分子描述符，包括拓扑指数等。还通过 Box-Jenkins 方法融合化学和生物信息，得到能反映分子结构与实验条件关系的\\(D[GTI]cj\\)描述符，为模型构建提供丰富数据支持。 mtc-QSAR-MLP 模型构建 数据集划分：将数据集按 3:1 随机分为训练集（1111 个分子案例，占 75.27%）和测试集（365 个分子案例，占 24.73%） 。 描述符筛选：在训练集中，用信息增益比（IGR）对\\(D[GTI]cj\\)描述符排序，通过两两 Pearson 相关系数（PCC）筛选出相关性在\\(-0.6 \u003c PCC \u003c 0.6\\)的描述符，减少信息冗余。 模型训练与选择：以 MLP 神经网络为基础构建模型，利用 STATISTICA v13.5.0.17 软件分析不同 MLP 网络，依据敏感性（Sn (%)）、特异性（Sp (%)）、准确性（Acc (%)）和马修斯相关系数（MCC）等指标，选择性能最佳的 MLP 网络作为 mtc-QSAR-MLP 模型。 模型评估：采用 Bounding Box 方法确定模型适用性域，计算描述符的局部适用性域分数（\\(LSAD_D [GTI] cj\\)）和总分数（TSAD），判断分子是否在适用性域内。通过分析模型在训练集和测试集上的性能指标，如准确率、敏感性、特异性和 MCC 等，验证模型对不同实验条件下化学物质抑制活性的分类和预测能力。 虚拟筛选与结果验证：使用构建好的 mtc-QSAR-MLP 模型对 8922 种机构监管化学品数据库进行虚拟筛选，考虑 8 种实验条件，计算每个分子的 FA (%)（分子在 8 种实验条件下被预测为活性的频率）和 S (TSAD)（考虑 8 种实验条件下分子的总适用性域分数）指标，筛选潜在双重抑制剂。对筛选结果，通过在科学文献中搜索相关信息，结合实验证据验证其抑制 caspase-1 和 TNF-alpha 的可能性。 优势\r整合多源数据：基于扰动理论和机器学习构建模型，能够整合不同类型的化学和生物数据。通过 Box-Jenkins 方法对传统分子描述符进行处理，得到融合化学结构与实验条件信息的描述符，使模型能综合考虑多种因素。 多靶点多终点预测：可同时预测多个生物终点（如活性、毒性、药代动力学性质），并针对多个不同靶点（如 caspase-1 和 TNF-alpha）进行预测。 高预测准确性：模型在训练集和测试集上都表现出较高的准确性。训练集准确率达 92.44%，测试集准确率为 88.49%，马修斯相关系数（MCC）在训练集和测试集分别为 0.844 和 0.764。 提供理化和结构解释：通过对分子描述符的分析，能够从物理化学和结构层面解释模型的预测结果。 数据集\rChEMBL开放数据库。\nMachine learning–enabled virtual screening indicates the anti-tuberculosis activity of aldoxorubicin and quarfloxin with verification by molecular docking, molecular dynamics simulations, and biological evaluations | Briefings Bioinf\r解决的问题\r目前的抗结核药物一定程度上控制了结核病的传播，但耐药结核病的出现使得这些药物的疗效大打折扣。\n虚拟筛选技术，为快速识别潜在的抗结核药物候选物提供了一种有效的手段。研究旨在通过结合多种机器学习和深度学习模型，开发一种虚拟筛选工作流程，以重新利用现有药物来对抗结核分枝杆菌，从而加速抗结核药物的发现进程。\n核心方法\r研究者首先从ChEMBL数据库中收集了大量已知的抗结核药物的生物活性数据，用于训练和验证多种机器学习和深度学习模型。这些模型包括XGBoost和图神经网络（GNNs），用于预测化合物的最小抑制浓度（MIC）。接着，研究者使用这些模型对DrugBank数据库中的11,576种化合物进行虚拟筛选，筛选出具有潜在抗结核活性的化合物。通过多步过滤和实验验证，最终确定了两种具有显著抗结核活性的化合物：aldoxorubicin和quarfloxi，为了进一步验证作用机制，研究者通过分子对接、分子动力学模拟和表面等离子共振实验，确认了它们与结核分枝杆菌DNA gyrase的直接结合。\n","date":"2025-04-04T01:42:58+08:00","permalink":"https://ionfeather.github.io/2025/moleculerepurposing-2/","title":"AIDR | 综述与论文"},{"content":"数据库\rTTD\rTTD 治疗靶点数据库。它是一个专门收集和整理与治疗靶点相关信息的数据库。 下载地址：Full Data Download | Therapeutic Target Database 数据库比较大，这里筛选了药物-靶点-疾病关联数据： Target to drug mapping with mode of action TargetID：靶点的唯一标识符。可以在数据库中找到靶点的详细信息。 DrugID：药物的唯一标识符。可以获取该药物的相关信息。 Highest_status：表示药物在研发或临床应用中的最高阶段或状态，Approved说明该药物已经通过了相关监管机构的审批，被批准用于临床治疗。 MOA：即Mode of Action，作用模式的缩写，Modulator表明该药物对靶点的作用方式是作为调节剂。 Drug to disease mapping with ICD identifiers TTDDRUID：表示 TTD 药物 ID，即该药物的唯一标识。 DRUGNAME：药物名称。 INDICATI：适应症，即药物被用于治疗的病症。 Disease entry：疾病条目，这里使用了 ICD-11 编码来表示具体的疾病。 Clinical status：临床状态，指药物在临床试验或临床应用中的阶段或情况，如已批准、处于某一阶段的临床试验等。 Target to disease mapping with ICD identifiers TARGETID：代表 TTD 靶点 ID，为靶点在数据库内的唯一识别编号。 TARGNAME：靶点的具体名称。 INDICATI：适应症，即药物被用于治疗的病症。 Disease entry：疾病条目，这里使用了 ICD-11 编码来表示具体的疾病。 Clinical status：临床状态。\n表一：Target to drug mapping with mode of action\rTargetID DrugID Highest_status MOA T87024 D00RRU Approved Modulator 这个表格表示：ID为D00RRU的药物能够以调节剂的方式作用ID为T87024的疾病，并且已经被批准。\n表二：Drug to disease mapping with ICD identifiers\rTTDDRUID DRUGNAME INDICATI ICD-11 Clinical status DZB84T Maralixibat Pruritus EC90 Approved DZB84T Maralixibat Progressive familial intrahepatic cholestasis 5C58.03 Phase 3 DZB84T Maralixibat Alagille syndrome LB20.0Y Phase 2 这个表格表示ID为DZB84T的药物，名为Maralixibat，有对应几种适应症：\n适应症为Pruritus（瘙痒症），ICD-11 编码是EC90，临床状态为Approved（已批准）。 适应症为Progressive familial intrahepatic cholestasis（进行性家族性肝内胆汁淤积症），ICD-11 编码是5C58.03，临床状态为Phase 3（三期临床试验）。 适应症为Alagille syndrome（阿拉基综合征），ICD-11 编码是LB20.0Y，临床状态为Phase 2（二期临床试验）。 表三：Target to disease mapping with ICD identifiers\rTARGETID TARGNAME INDICATI INDICATI INDICATI T00033 Transforming growth factor alpha (TGFA) Phase 1/2 Chronic kidney disease [ICD-11: GB61] 这个表格表示 ID 为 T00033 的靶点，名为 Transforming growth factor alpha (TGFA)，有对应的临床关联信息：关联的疾病适应症为Chronic kidney disease（慢性肾脏病），ICD-11 编码是GB61，临床状态为Phase 1/2（一期 / 二期临床试验 ）。\nPubChem\rPubChem PubChem是美国国立卫生研究院的一个开放化学数据库。化合物信息数据库，收录大量化合物的结构、生物活性等信息，为科研、药物研发等提供数据支持，也用于化学知识普及和教学。这个数据库非常大。 用途参考：Nucleic Acids Research | 疗效药物靶标的比较性研究与数据平台构建。 FTP下载：Index of /pubchem 药物-疾病关联数据 ./Bioassay：生物测定数据，包含大量药物对不同生物靶点或细胞系的活性测试结果 ./Target：靶标数据，涵盖蛋白质、基因、通路和分类学等信息。 ./Commpound和./Compound_3D：化合物的数据信息包含结构信息。 ./Other：GooglePatents和IBM中包含专利信息。寻找专利即将过期/已过期的药物，并对这些药物进行再利用评估。 下面以./Bioassay数据为例。\n表 1：PubChem的./Bioassay的CSV文件的标题行的分类和标签说明\r分类 标签名称 说明 数据行 PUBCHEM_RESULT_TAG 行ID 数据行 PUBCHEM_SID PubChem SID 数据行 PUBCHEM_CID PubChem CID 数据行 PUBCHEM_ACTIVITY_OUTCOME PubChem活性结果（即，Inactive, Active, Inconclusive, Unspecified, or Probe） 数据行 PUBCHEM_ACTIVITY_SCORE PubChem活性得分，值越高表示活性越强 数据行 PUBCHEM_ACTIVITY_URL 测试结果特定的url 数据行 PUBCHEM_ASSAYDATA_COMMENT 测试结果特定的注释 数据行（可选） 测试结果名称1（如：name of test result 1） 测试结果1的数据 数据行（可选） 测试结果名称2（如：name of test result 2） 测试结果2的数据 可选标题行（测试结果） RESULT_UNIT 单位（e.g. MICROMOLAR, NANOMOLAR和其他PubChem上传系统使用的标签） 可选标题行（测试结果） RESULT_IS_ACTIVE_CONCENTRATION 如果测试结果表示有效浓度则为TRUE 可选标题行（测试结果） RESULT_IS_ACTIVE_CONCENTRATION_QUALIFIER 如果测试结果表示与有效浓度相关的终点限定词（e.g. \u0026lt;, \u0026lt;=, =, \u0026gt;, \u0026gt;=）则为TRUE 可选标题行（测试结果） RESULT_ATTR_CONC_MICROMOL 以微摩尔为单位的测试浓度 表二：PubChem CSV/Data/0000001_0001000/1.csv文件\rPUBCHEM_RESULT_TAG PUBCHEM_SID PUBCHEM_CID PUBCHEM_EXT_DATASOURCE_SMILES PUBCHEM_ACTIVITY_OUTCOME PUBCHEM_ACTIVITY_SCORE PUBCHEM_ACTIVITY_URL PUBCHEM_ASSAYDATA_COMMENT LogGI50_M LogGI50_u LogGI50_V IndnGI50 StddevGI50 LogTGI_M LogTGI_u LogTGI_V IndnTGI StddevTGI 1 66954 11122 CC1=CC(=O)C=CC1=O Inactive 10 http://dtp.nci.nih.gov/dtpstandard/servlet/doseresponse?searchtype=NSC\u0026searchlist=1\u0026systemname=NCI+Cancer\u0026idn1=1\u0026idn2=1 -4.5753 1 0 -4 1 0 这个表格表示数据行 ID 为 1 的记录，其中涉及的 PubChem SID 为 66954，PubChem CID 为 11122，化合物的 SMILES 表示形式为CC1=CC(=O)C=CC1=O，有对应的生物活性关联信息：\nPubChem 活性结果为Inactive（无活性），PubChem 活性得分为 10，表明在此次生物测定中该化合物活性较低。 测试结果特定的 url 为dtp.cancer.gov/services/nci60data/colordoseresponse/pdf/1，可通过该链接获取更多相关测试结果信息。 测试结果特定的注释为空。 关于 GI50（半数生长抑制浓度）的相关数据： 以摩尔为单位的 GI50 结果的对数LogGI50_M为 - 4.5753。 测试次数平均数量IndnGI50为 1。 对所有测试的 GI50 结果的对数（Log10）的标准偏差StddevGI50为 0，说明该测量值较为稳定。 关于 TGI（总生长抑制）的相关数据： 以摩尔为单位的 TGI 结果的对数LogTGI_M为 - 4。 测试次数平均数量IndnTGI为 1。 对所有测试的 TGI 结果的对数（Log10）的标准偏差StddevTGI为 0，表明该测量值稳定性较好 。 其他的如./Target、./Commpound和./Compound_3D等数据也呈现如此结构。\nDGIdb\rDGIdb 药物基因相互作用数据库，主要收集和整理药物与基因之间相互作用的相关信息。 这个数据库一共就四个表格，分别存储药物-基因相互作用、基因、药物和分类相关信息，使用tsv格式进行存储。数据大小约为24.9MB。 interactions.tsv：存储所有药物-基因相互作用声明数据，包含不同药物与基因之间相互作用的信息。 genes.tsv：记录了基因声明相关数据，可能涵盖基因的基本信息，如基因名称、基因 ID、基因功能注释等内容。 drugs.tsv：存放药物声明数据，包括药物的名称、药物 ID、药物的基本属性、药理作用等信息。 categories.tsv：用于存储与数据分类相关的信息。\n表一：interactions.tsv\r其中的各列的意思分别是\ngene_claim_name：基因的声明名称。 gene_concept_id：基因的唯一标识代码。 gene_name：基因的标准正式名。 interaction_source_db_name：基因与药物相互作用数据的来源数据库名。 interaction_source_db_version：来源数据库的版本。 interaction_type：基因和药物间相互作用的类别。 interaction_score：体现基因与药物相互作用强度的数值。 drug_claim_name：药物的特定称谓。 drug_concept_id：药物的唯一标识代码。 drug_name：药物的标准正式名。 approved：药物是否已获批上市的标识。 immunotherapy：药物是否属于免疫治疗药物的标识。 anti_neoplastic：药物是否具有抗肿瘤作用的标识。 gene_claim_name gene_concept_id gene_name interaction_source_db_name interaction_source_db_version interaction_type interaction_score drug_claim_name drug_concept_id drug_name approved immunotherapy anti_neoplastic CYP2D6 hgnc:2625 CYP2D6 DTC 9/2/20 NULL 0.017709164 RACLOPRIDE ncit:C152139 RACLOPRIDE FALSE FALSE FALSE PPARG hgnc:9236 PPARG DTC 9/2/20 NULL 0.84012274 KALOPANAX-SAPONIN F chembl:CHEMBL1833984 CHEMBL:CHEMBL1833984 FALSE FALSE FALSE 这个表格有两列，这里只对第一列进行解释：基因声明名称为CYP2D6，基因概念 ID 是hgnc:2625，基因名称同样为CYP2D6。相互作用来源数据库名称是DTC，数据库版本为9/2/20，相互作用类型为空（NULL），相互作用得分为0.017709164。药物声明名称是RACLOPRIDE，药物概念 ID 为ncit:C152139，药物名称是RACLOPRIDE。该药物未被批准（approved为FALSE），不是免疫疗法（immunotherapy为FALSE），也不是抗肿瘤药物（anti_neoplastic为FALSE） 。\n表二：drugs.tsv\r其中的各列的意思分别是\ndrug_claim_name：药物的特定称谓。 nomenclature：药物命名法类型。 concept_id：药物的唯一标识代码。 drug_name：药物的标准正式名。 approved：药物是否已获批上市的标识。 immunotherapy：药物是否属于免疫治疗药物的标识。 anti_neoplastic：药物是否具有抗肿瘤作用的标识。 source_db_name：药物数据的来源数据库名。 source_db_version：来源数据库的版本。 drug_claim_name nomenclature concept_id drug_name approved immunotherapy anti_neoplastic source_db_name source_db_version BRAF(V600E) Kinase Inhibitor RO5212054 Primary Drug Name ncit:C92591 BRAF(V600E) KINASE INHIBITOR RO5212054 FALSE FALSE TRUE NCIt 24.02d 药物声明名称为BRAF(V600E) Kinase Inhibitor RO5212054，药物命名法类型是Primary Drug Name，药物概念 ID 是ncit:C92591，药物名称为BRAF(V600E) KINASE INHIBITOR RO5212054。该药物未被批准（approved为FALSE），不是免疫疗法（immunotherapy为FALSE），是抗肿瘤药物（anti_neoplastic为TRUE） 。药物数据的来源数据库名称是NCIt，数据库版本为24.02d。\n表三：gene.tsv\r其中的各列的意思分别是\ngene_claim_name：基因的声明名称。 nomenclature：基因命名法类型。 concept_id：基因的唯一标识代码。 gene_name：基因的标准正式名。 source_db_name：基因数据的来源数据库名。 source_db_version：来源数据库的版本。 gene_claim_name nomenclature concept_id gene_name source_db_name source_db_version NGFIBA NCBI Gene Name NULL NULL BaderLab Feb-14 基因声明名称为NGFIBA，基因命名法类型是NCBI Gene Name，基因概念 ID 是NULL，基因名称同样为NULL。基因数据的来源数据库名称是BaderLab，数据库版本为Feb-14。\n表四：categories.tsv\r其中的各列的意思分别是\nname：某实体的名称（这里可能是基因或其他生物相关实体名称）。 name-2：该实体的另一个相关名称或描述。 source_db_name：数据的来源数据库名。 source_db_version：来源数据库的版本。 name name-2 source_db_name source_db_version PXR NUCLEAR HORMONE RECEPTOR BaderLab Feb-14 名称为PXR，另一个相关名称或描述是NUCLEAR HORMONE RECEPTOR。数据的来源数据库名称是BaderLab，数据库版本为Feb-14。\nDrugBank\rDrugBank 综合药物信息数据库，整合了药物的化学、药理、毒理等多方面信息。 一个巨大的数据库。这个数据库免费开放给学术用户，但商业使用收费。可以在网页上进行学生/老师认证注册。药物再利用在其中是很小的一个板块，可以在这里下载。 学生身份目前正在审核状态，无法下载。 ChEMBL\rChEMBL ChEMBL 是一个开源的生物活性分子数据库，专注于小分子化合物与生物靶点之间的相互作用信息。 数据大小大约2GB。有不同的版本，提供FTP方式下载，可以在这里下载最新的版本ChEMBL_35。 目录中的内容可以参考README文件。主要的部分可以见下图，组织时FASTA文件、SDF文件、HTML文件和TXT文件都有使用。数据可以加载到MySQL、PostgreSQL等数据库中。 有官方的数据库结构图如下，主要分成 化合物信息：主要以蓝色区域呈现，记录了化合物的结构、理化性质等，像分子式、分子量和二维结构这些 实验数据：用紫色区域表示，包含化合物与靶点相互作用的活性数据，还有实验条件，比如不同化合物对特定靶点的结合亲和力数值，以及实验采用的检测方法等 靶点和结合位点信息：红色区域代表这部分内容。靶点信息涉及蛋白质靶点的氨基酸序列、三维结构和功能分类；结合位点信息聚焦靶点与化合物结合的区域，包括氨基酸组成、空间结构和相互作用模式 药物代谢数据：浅绿色区域涵盖这部分，记录了药物在体内的代谢途径、产物和酶——我们可以通过这部分预测药物疗效、毒性以及药物之间的相互作用。 作用机制 / 药物注释：浅蓝色区域负责注释药物作用机制，关联化合物、靶点和生物效应。 来源和已批准药物数据：灰色区域中，来源信息保证数据可追溯，已批准药物 数据包含批准文号、适应症和生产厂家等，这个应该是用来避免专利壁垒的，可以查看专利即将到期的药物。 常规信息：浅黄色区域存放数据库的版本号、更新时间等基础信息。\nBindingDB\rBindingDB 主要收集药物靶点蛋白质和类药小分子之间的相互作用亲和力数据。数据库大小为484.07 MB（TSV版本）。 该数据文件只有一个表格，该表总共包含117列。 配体标识与结构（7 列）：如 “BindingDB MonomerID” 等，用于确定和描述配体，包括多种结构表示方式和自定义名称。 靶点基础信息（2 列）：“Target Name” 和 “Target Source Organism According to Curator or DataSource”，明确靶点名称及所属生物。 结合活性数据（6 列）：“Ki (nM)” 等，反映配体与靶点结合的强度和速率。 实验环境参数（2 列）：“pH” 和 “Temp (°C)”，体现结合数据测量时的环境条件。 数据与文献来源（7 列）：“Curation/DataSource” 及各类文献标识符，用于追溯数据出处和原始研究。 数据库交叉引用（14 列）：含 “PubChem CID” 等多个数据库的配体标识，方便数据整合查询。 蛋白质链详情（80 列）：先以 “Number of Protein Chains in Target” 记录链数量，后续多列针对每条链，涵盖序列、结构 ID、UniProt 相关名称和 ID 等信息。 表格中的信息以第一条记录（数据行 ID 为 1）为例，大概描述了：\n配体相关信息：BindingDB MonomerID 为 608734，Ligand SMILES 为 “O [C@@H] 1C@@HC@@HN (CCCCCC (O)=O) C (=O) N (CCCCCC (O)=O)[C@@H] 1Cc1ccccc1”，其对应的 BindingDB Ligand Name 是 “6-[(4R,5S,6S,7R)-4,7 - 二苄基 - 3-(5 - 羧基戊基)-5,6 - 二羟基 - 2 - 氧代 - 1,3 - 二氮杂环庚烷 - 1 - 基] 己酸::DMPC 环脲 1”。 靶点相关信息：Target Name 为 “Dimer of Gag-Pol polyprotein [501 - 599]”，表明靶点是 Gag-Pol 多聚蛋白二聚体的 501 - 599 区域；Target Source Organism 为 “Human immunodeficiency virus 1”，即靶点来源于人类免疫缺陷病毒 1。 结合活性数据：Ki (nM) 为 0.24 ，显示配体与靶点的结合亲和力较强。 实验条件：测量时 pH 为 5.5，温度为 37°C。 数据来源：由 BindingDB 从文献整理而来，相关文献的 DOI 是 10.1021/jm9602571，PMID 为 8784449，可据此追溯原始研究。 数据库交叉引用：PubChem CID 为 3009304，PubChem SID 为 483500124，方便在 PubChem 数据库中查找该配体更多信息。 蛋白质链信息：靶点含 1 条蛋白质链，其序列为 “PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF”，相关 PDB ID 有 “1W5Y”“1W5X” 等多个，UniProt（SwissProt）相关信息显示，其推荐名称是 “Gag-Pol polyprotein”，Entry Name 为 “POL_HV1BR”，Primary ID 为 “P03367”。 ","date":"2025-03-23T17:15:44+08:00","permalink":"https://ionfeather.github.io/2025/moleculerepurposing/","title":"AIDR | 数据库探索"},{"content":"北海公园\r周四晴朗，趁着早上刚开完会的懈怠，去北海公园探春。春色正好，看到了索尼、尼康、富士、佳能相机不下十余只对着阐福寺的牌匾和桃花「咔咔咔」地按下快门。\n北海公园的桃花和樱花不多，但河柳正是时候，被气温诈骗的柳树抽出了新芽，此时正是杨柳最美的时候，嫩嫩茸茸，纤巧细美，切切可人。柳丝被春风吹起，在红墙白塔的照映下，格外嫩绿。暖风熏得我沉醉其中。\n公园里的女孩格外得多。大清还没亡，格格、皇后的数量可以再住满紫禁城，手机相机Pocket3可以堆满武库。我揣着相机漫步其间，遇见了在北京工作的朋友二人、从山东来旅游的女生和姐妹俩、反扣着帽子的酷女孩、爱好摄影的Tony老师——形形色色，但都友好温柔。\n我端起相机，拍得或好或差，「咔」——留下了她们生命的一个瞬间，也同时在相机上留下了我的剪影。\n絮絮叨叨\r真的有点沉迷于认识新的朋友了——虽然里面有些人可能不会再相见，可能不会再联系，但是在认识、沟通的那个瞬间，真的很愉快。\n2025-03-23 14:00\n去汉光百货买了三件衣服，竟然如此昂贵！花了我一千多大洋，顿时一阵肉痛。不过妈妈说如果觉得好看，如果能穿比较久的话，也不亏。这么一想确实还可以。\n2025-03-29 23:46\n玉渊潭的春天真的好美。天空蓝得清澈，白云懒洋洋地漂浮在上面——冬天是绝对见不到这样的景色的，受到西伯利亚高压控制的北京在冬天干燥而又晴朗。\n柳条刚刚抽出，还透着稚嫩，樱花却已经开得烂漫了。玉渊潭里的鸟儿不怕人，在我们面前随意踱步、游泳、起飞。远处的白桥和中央电视塔在这样清澈湛蓝的湖水的衬托下熠熠生光。公园里的coser很多，看到了芙莉莲和勇者，看到了好多粉色的头发的女生——有些我已经不认识了，我感觉我也没多大呀，但好像已经是个上古时代的二次元了。我逐渐迈入现充了（）\n我也拿起我的相机，我们同门四人的一张张照片也在一声声快门声中被记录。在今天的玉渊潭的景色的衬托下，拍什么都明媚可人、青春洋溢。世界仿佛被蒙上了一层名为鲜艳青春的蒙版或者滤镜，湖蓝、天清、云白、树绿。\n2025-03-30 22:00\n","date":"2025-03-22T14:39:56+08:00","image":"https://ionfeather.github.io/2025/the-spring-equinox/cover_hu_8f0fddc5d57f45be.jpg","permalink":"https://ionfeather.github.io/2025/the-spring-equinox/","title":"春分 | 拍堤春水四垂天，红墙白塔，小花杨柳"},{"content":"盼望着，盼望着，春天的脚步近了。飞快，羽绒服已经太热，食堂门口的桃花准备抽枝，路过图书馆时望见乌鸦「哇——哇——」地向北飞。\n已经摸上我的相机，快乐地订好了出游计划。\n万物复苏，只期待一场春日游。春日游。杏花吹满头。陌上谁家年少，足风流——说不定是我。\n北京的天气也是神奇起来了。今天终于补上了一个冬天的遗憾，在春暖花开之前，冬天好像也知道自己的谢幕。在临别之际，给出了自己最后的一个吻，撒下了雪花。\n2025-03-15 15:34\n听到了今年第一声雷响！记录一下时间点。\n惊蛰到了。\n2025-03-15 22:33\n参与了黄老师的季会。我在里面是下午发言。\n季会人太多，每个人发言20分钟，竟然需要一天的时间，从早上8:50开始，到下午16:00结束。\n导师真的好厉害，英语水平好高，演讲的时候神采飞扬，而且英语句子地道流利。在台下给我看傻了。 我的演讲好像也不错，嘿嘿。同门给我录了一小段，我发现我的状态特别好——发型、手势和声音都不错。最近看自己是原来越顺眼了。 2025-03-16 17:00\n春天肆意地驱散冬天的肃杀氛围。首先被春天派遣的先行部队是山桃花、迎春花和玉兰花三位大将。请看：\n花儿不知道这是早春，用尽全身力气绽放。\n2025-03-20\n","date":"2025-03-05T14:37:00+08:00","image":"https://ionfeather.github.io/2025/the-waking-of-insects/cover_hu_ec51ee97198ccfeb.jpg","permalink":"https://ionfeather.github.io/2025/the-waking-of-insects/","title":"惊蛰 | 漫长的冬天终于到达了尾声"},{"content":"开展一些AI-assist Drug Design 的方向探索，需要进行一些论文阅读和调研，形成调研报告。\n参考：\n[2402.08703] A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation Hao Zhou（周浩) 目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 思维导图：\nA Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation\r[2402.08703] A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation\n摘要\r将从头药物设计（de novo drug design）归纳为两大主题：小分子生成和蛋白质生成。在每个主题下，我们识别出多种子任务和应用，重点介绍重要的数据集、基准测试、模型架构，并比较顶尖模型的性能。\n引言\r并非是虚拟筛选/定向进化。而是从头开始的自然界中并未存在的新的生物实体的生成。 文章中分成两个部分来讲解。小分子和蛋白质。 文章将介绍 生成式模型的种类：Diffusion/VAE/Flow-Based/GAN 将文章分成小分子和蛋白质两个领域，分别介绍 一般背景/任务定义 用于训练和测试的常见数据集 常用的评估指标 对过去和当前的机器学习方法的概述 对SOTA方法的性能的对比分析 总结 相关研究\r其他的方法都太专业，而这篇文章对小分子和蛋白质生成进行宏观层面的分析，有助于那些想要对化学创新领域中新兴的生成式AI模型有一个高屋建瓴的了解的人。\n前言：生成式AI模型\r介绍了VAE、GAN、Flow-Based Models和Diffusion，还介绍了一些其他模型，比如GNN、EGNN等。\n应用\r小分子\r任务背景\r分子生成聚焦于为药物设计创造新的分子化合物。这些生成的分子旨在具有 （1）有效性、（2）稳定性和（3）独特性，总体目标是具有药物适用性。“药物适用性” 是一个宽泛的术语，用于描述分子对各种生物靶标的结合亲和力。\n虽然前三个任务可能看起来微不足道，但仅仅生成有效和稳定的分子就存在各种挑战。因此，无靶向分子生成领域专注于生成有效的分子集合，而不考虑任何生物靶标。靶向分子生成（或配体生成）侧重于针对特定蛋白质结构生成分子，因此更关注药物成分。最后，3D 构象生成涉及在给定 2D 连接的情况下生成各种 3D 构象。\n无靶向分子设计\r必须满足前两个特点，也就是有效性和稳定性。需要满足很多的复杂的条件，所以其实还是挺难的。深度学习可以帮助人们更有效率地生成有更高可能性满足有效性的分子。\n任务：无输入，输出为生成一组新的、有效的、稳定的分子。\n数据集：QM9和GEOM-Drug\n指标：\n分子生成任务指标：原子稳定性、分子稳定性、有效性、独特性、（新颖性）、药物相似性度量估计值QED。 模型的评估方式：通过在QM9数据集上的一部分训练属性分类网络，然后对模型生成的分子进行评估，计算目标和评估属性值之间的平均绝对误差。 分子在特定化学性质方面的指标：极化率$α$、最高占据分子轨道能量 $\\varepsilon_{HOMO}$、最低未占据分子轨道能量 $\\varepsilon_{LUMO}$、$\\varepsilon_{HOMO}$ 和 $\\varepsilon_{LUMO}$ 的差值$\\Delta_\\varepsilon$、偶极矩 $\\mu$、298.15K 下的摩尔热容 $C_v$。 模型：\n过去几年间，分子生成任务的方法从一维的SMILES转变成二维的连接图，然后是三维的几何结构，最后到融合二维和三维信息的方法。\n1D SMILES字符串模型 早期方法：如CVAE、GVAE直接处理，但是SMILES因为是一维的，存在问题：两个化学结构相似的分子图可能会得到非常不同的 SMILES 字符串，这使得模型更难学习到这些相似性和模式。 2D 图生成模型 JTVAE：首个直接生成 2D 分子图的模型，通过树状骨架迭代扩展并验证结构有效性。 3D 结构模型 早期方法：Flow-Based方法ENF和自回归方法G-SchNet。 EDM：基于Diffusion的 3D 点云模型，利用 E (3) 等变性提升性能，避免原子排序依赖。 GCDM：结合几何深度学习与Diffusion，引入注意力机制优化消息传递。 联合2D和3D JODO：联合2D和3D的扩散模型，使用几何图形表示来捕获 3D 空间信息和连接信息，对这种联合表示应用分数随机微分方程，同时提出扩散图变换器来参数化数据预测模型，避免在每个独立通道独立添加噪声后相关性的丢失。 MiDi：应用了DDPM，提出了「松弛」的图神经网络（EGNN）。 这里给出了三个表格。\n表格1：生成模型在QM9数据集上的条件无关的分子设计任务上的性能表现。Diffusion方法比之前的方法好很多，但是在GEOM-Drugs上可能表现不佳。 表格2：EDM、MDM、MiDi 等模型在GEOM-Drugs数据集上的条件无关的分子设计任务上性能表现。MiDi能生成更稳定的复杂分子，但是有效性较低。 表格3：生成模型在条件分子生成任务上的性能表现。MDM、GCDM生成表现不错，MDM前四项较好，GCDM后两项较好。 靶向分子设计\r有两种，一种是基于配体的药物设计（LBDD），另一种是基于结构的药物设计（SBDD）。LBDD利用目标蛋白质的氨基酸序列，借助已知的配体特征来构建；SBDD利用目标蛋白质的三维结构来设计。\n任务：给定氨基酸序列/蛋白质的三维结构，生成对应的有高结合亲和力以及潜在相互作用的分子。\n数据集：CrossDocked2020、ZINC20和Binding MOAD。\n指标：Vina Score、Vina Energy、高亲和力百分比High Affinity Percentage、合成可及性分数SAscore和多样性Diversity。\n模型：\nLBDD 结合了Transformer结构，例如DrugGPT，训练的时候输入为SMILES和蛋白质氨基酸序列，从而训练输出可行的SMILES配体。 SBDD LiGAN：三维目标感知分子输出的概念，将分子适配到网格格式，以便利用卷积神经网络（CNN）进行学习，并在变分自编码器（VAE）框架下训练模型 TargetDiff模型：基于EGNN进行Diffusion，在结构上和EDM相似，目标是学习条件分布。特别地，研究人员通过原子嵌入的熵来将原子类型的灵活性降低，从而提高结合亲和力。 DiffSBDD：DiffSBDD-cond是一种DDPM，而在基准测试中，DiffSBDD-inpaint则进行了图像增强，使用掩蔽和替换等方法对配体-蛋白质的部分区域进行处理。 这里给出了一个表格。展示了不同的模型的结果。\n蛋白质\r任务背景\r蛋白质可以通过其3D结构或者氨基酸序列来表示，氨基酸序列类似人类的语言，可以应用于自然语言模型。可以定义几个子任务：1）表示学习。2）结构预测。3）序列生成。4）主干设计。此外还讨论了抗体生成和肽生成。\n蛋白质表示学习\r使用氨基酸序列/原子坐标学习一个嵌入从而为其他生成模型创建更丰富的数据空间以供训练。类似于自然语言处理中的word2vec。\n结构预测\r从氨基酸序列来预测结构是极具挑战性和重要的工作。\n任务：从氨基酸序列来预测蛋白质结构。\n数据集：主要来自蛋白质结构预测的关键评估（CASP）。有PDB、CASP14和CAMEO。\n指标：均方根误差RMSD、全局距离测试总得分GDT-TS、模板建模得分TM-score和局部距离差异测试LDDT。\n模型：\nAlphaFold2：里程碑模型。采用端到端的方式集成了多层Transformer，融合多序列比对和成对表示的信息。基于氨基酸之间的成对距离探索折叠空间、氨基酸的潜在取向和整体结构。 trRosetta：transform-restrained Rosetta。输入MSA之后，预测残基对之间的距离和取向，然后利用Rosetta协议构建3D结构。 RoseTTAFlod：在CASP14上表现效果比肩AlphaFold2，特别是生成速度很快，仅需10分钟，相较AlphaFold2快了100倍。 ESMFold：利用ESM-2的输出嵌入到自注意力「折叠块」中，并通过以哦个具有SE（3）的transformer架构的结构模块生成最中国的结构预测。 EigenFold：应用Diffusion生成蛋白质结构的模型。它将蛋白质表示为一个谐振子系统，在正向过程中可以将结构投影到该系统的本征模式上，在反向过程中先采样粗糙的全局结构再细化局部细节。作为一种基于分数的模型，EigenFold 计算强度不高，但在准确性和范围方面仍不如其他模型。 抗体结构预测：\n这里的MSA结构不能作为抗体的输入。因此通用的模型如AlphaFold2效率非常低。\nIgFlod：使用来自AniBERTy的序列嵌入和不变点注意力机制来预测。 tFlodAb：减少了对Rosetta能量函数等外部工具的依赖。 序列生成\r序列生成，也被称为反向折叠或固定骨架设计，是结构预测的逆任务。生成能折叠成目标结构的氨基酸序列，对于设计具有期望结构和功能特性的蛋白质至关重要。由于有效序列的空间巨大，且蛋白质折叠过程复杂难以预测，因此需要多种深度学习方法来解决这些挑战\n任务：给定固定的蛋白质骨架结构，生成能折叠成该结构的相应氨基酸序列。\n数据集：模型主要使用 CATH 进行训练，部分会利用 UniRef 和 UniParc 进行数据增强，评估时常用 CATH 和 TS500。此外，Yu 等人创建了一组 14 个已知的从头蛋白质结构，用于避免数据污染。\n指标：\nAAR（氨基酸恢复率）：生成序列与天然序列中匹配氨基酸的比例。 多样性（Diversity）：通过 Clustalw2 测量生成序列对之间的平均差异。 RMSD（均方根偏差）：将生成序列折叠成结构后，与天然骨架结构进行比较的结构差异指标。 非极性损失（Nonpolar Loss）：衡量折叠结构中极性氨基酸类型合理性的指标，表面非极性氨基酸含量越高，损失越大。 PPL（困惑度）：交叉熵损失的指数化，代表天然序列出现在预测序列分布中的逆可能性。 模型：\n初步的一类模型在不考虑固定骨架目标的情况下生成蛋白质序列。但这些模型无法考虑关键的结构信息。 ProteinVAE 利用 ProtBERT 将原始输入序列转化为潜表示； ProT-VAE 使用不同的预训练语言模型 ProtT5NV； ProteinGAN 则采用 GAN 架构。 主要的模型接收固定骨架目标作为输入来生成氨基酸序列。 ProteinSolver 将生成骨架结构与解决数独问题联系起来，使用 GNN 架构； PiFold 引入更全面的特征表示； Anand 等人设计 3D CNN 直接学习条件分布； ABACUS-R 结合预训练的 transformer 来推断残基的氨基酸类型； ProRefiner 通过引入熵分数改进预测。 GPD 使用 Graphormer 架构， GVP-GNN 采用新颖的几何表示， ESM-IF1 扩展表示并在扩展数据集上训练， ProteinMPNN 实现了顺序无关的自回归方法。 在这些模型中，ProteinMPNN 在序列恢复、RMSD 和非极性损失方面表现最佳，GPD 则是最省时的方法。 主干设计\r生成全新的蛋白质可以直接扩充蛋白质库，实现高度复杂和多样的功能，是从头设计的核心。蛋白质设计在结构和序列上存在差异，有的模型生成 1D 氨基酸序列，有的直接生成 3D 结构，还有的同时设计两者。\n任务：从无输入或基于现有背景设计蛋白质骨干结构，即生成每个氨基酸的骨干原子（氮、$\\alpha$ - 碳、羰基和氧原子）坐标，外部工具可用于侧链填充。包含上下文无关生成（生成多样的蛋白质结构）和上下文给定生成（根据天然蛋白质的基序填充缺失残基）两个子任务。\n数据集：常用的数据集有 PDB、AlphaFoldDB、SCOP（及其扩展 SCOPe）和 CATH。\n指标：\nscTM（自洽 TM 分数）：通过将提议的结构输入序列预测模型（通常是 ProteinMPNN）生成相应氨基酸序列，再将其输入结构预测模型（通常是 AlphaFold2）生成样本结构，计算生成结构与样本结构之间的 TM 分数。分数大于 0.5 的结构通常被认为是可设计的。 scRMSD（自洽 RMSD）：与 scTM 类似，但使用 RMSD 进行评估，分数小于 2 通常作为截止值。 AAR（氨基酸恢复率）：比较生成的氨基酸序列与真实序列的相似程度。 RMSD（均方根偏差）：衡量生成的残基坐标与真实值之间的距离。 模型：\n较短蛋白质 ProtDiff 使用 3D 笛卡尔坐标表示每个残基和粒子滤波扩散方法，但 3D 笛卡尔点云不能反映蛋白质折叠过程； FoldingDiff 则使用角度表示，更接近蛋白质折叠过程中的旋转能量优化，通过 DDPM 和 BERT 架构从随机未折叠状态去噪到折叠结构； LatentDiff 先使用带 GNN 的等变蛋白质自动编码器将蛋白质嵌入潜在空间，再用等变扩散模型学习潜在分布，在潜在空间采样比在原始蛋白质空间快十倍。 长蛋白结构：基于框架的构建方法 Genie 使用由平移和旋转元素确定的框架云进行离散时间扩散来生成骨干结构； FrameDiff 基于框架流形参数化骨干结构，使用基于分数的生成模型； RFDiffusion 结合 RoseTTAFold 的强大结构预测方法和扩散模型，通过微调 RoseTTAFold 权重并输入掩码输入序列和随机噪声坐标来迭代生成骨干结构，还进行自我条件约束，性能优异； GPDL 使用 ESMFold 代替 RoseTTAFold 作为基础结构预测模型，并结合 ESM2 语言模型提取进化信息，生成骨干结构速度比 RFDiffusion 快 10 - 20 倍。 同时设计蛋白质序列和结构 GeoPro 使用 EGNN 编码和预测 3D 蛋白质结构，并设计单独的解码器解码蛋白质序列； Protpardelle 在反向扩散过程中对可能的侧链状态进行 “叠加” 并在每次迭代更新时进行塌缩； ProtSeed 使用三角函数感知编码器计算约束和相互作用，并通过等变解码器更新序列和结构； Anand 等人使用 IPA 在框架空间中进行扩散，高效生成蛋白质序列和结构 。 抗体CDR-H3生成：\n特别地，抗体生成聚焦于一个被称为 CDR-H3 区域的生成。最开始使用的是LSTM方法，后来转变为RefineGNN方法。此外，一些模型超越了CDR-H3生成任务，而是一次性处理抗体生成的多个环节。dyMEAN是一种端到端的方法将结构预测、对接和CDR-H3生成整合到一个模型中。\n多肽设计\r虽然已经有在蛋白质生成方面的重要、强大的模型，但是由于多肽结构的复杂和依赖于上下文已经下游应用的多样性，因此有必要为多肽的需求来定制模型。\n多肽生成：从头生成新型多肽\nMMCD：基于Diffusion的治疗性多肽生成模型，它联合设计多肽序列和结构（骨干坐标），采用Transformer编码器处理序列，EGNN 处理结构，并运用对比学习策略对齐序列和结构嵌入，区分治疗性和非治疗性多肽嵌入。 多肽-蛋白质互相作用：预测提议的多肽 - 蛋白质对的物理结合位点\nPepGB：基于GNN的模型。它利用图注意力神经网络学习多肽和蛋白质之间的相互作用。 多肽表示学习：将原始多肽序列转换为能捕获有价值信息的潜在表示\nPepHarmony：使用序列编码器（ESM）和结构编码器（GearNet），多视图对比学习模型，集成序列和结构信息以增强多肽表示学习。 多肽测序：解决质谱分析中从含噪数据提取氨基酸序列的挑战\nAdaNovo：从头多肽测序模型，由质谱编码器和两个受Transformer架构启发的多肽解码器组成。它利用条件互信息和自适应训练策略，在多种物种的多肽水平和氨基酸水平精度上显著优于之前的模型。 最近趋势\r生成式AI正在深刻地改变药物设计。\n生成式AI领域：GNN和基于图的方法的出现，推动了从基于序列的方法向基于结构的方法的转变，最终促使在生成任务中实现了序列和结构的整合。 分子生成领域：基于图的Diffusion模型作为主导。利用E（3）等变形来实现最先进的性能。 GeoLDM、MiDi——无靶点分子设计 TargetDiff、Pocket2Mol、DiffSBDD——有靶点分子设计 Torsional Diffusion——分子构象生成 此外，有靶点分子设计中也出现了从基于序列的方法到基于结构的方法的出现。 蛋白质生成领域：也出现了从序列到结构的转变。 GearNET：基于结构的表示学习模型 ESM-1B、UniRep：3D结构的重要性 AlphaFold2：结构预测的最先进模型 一些Diffusion方法也致力于蛋白质骨架构建。 挑战\r分子生成领域：\n复杂性 适用性 可解释性 蛋白质生成领域：\n基准测试 性能 结论\r介绍了生成式AI在从头开始的药物设计上的全貌，特别关注分子和蛋白质生成。\nRegularized Molecular Conformation Fields | NeurIPS 2022\rRegularized Molecular Conformation Fields | OpenReview\n属于上面综述文章里里面的分子生成里面的3D构象生成的部分。\n解决的问题\r给定2D分子图，预测有机分子能量最有利的3D构象。\n挑战\r分子在三维欧氏空间的 SE (3) 变换下具有不变性，同一分子的构象在刚性运动下有无限可能，增加了建模难度 分子在环境条件下存在多种动力学，导致高维且复杂的势能面，使得机器学习模型难以识别局部最小值来生成能量有利的构象 现有方法使用的不变特征可能冗余、相互依赖，导致数值不稳定和不合理的构象预测，且专门的等变层可能降低神经网络的表达能力，部分模型处理环状图存在困难 核心方法和优势\r核心方法\r构建正则化构象场。依据最少内部自由度（DoF）原则将分子分割。蓝色圆圈代表片段构型，一般来说是低内部柔性的。 利用MRF建模。红色圆圈是二面角构型，黑色方块是相邻构型之间的相互作用。利用马尔可夫随机场（MRF）对片段构型和二面角构型的联合概率分布进行建模。 推理和采样。推理时无环RMCF采用多年动态规划进行最大后验解码，有环使用LBP算法。采样使用Gibbs采样，每次采样后固定其他节点。采样后用特定距离度量样本差异，通过 K-means 聚类，从每个聚类中随机抽取样本，提升生成构象的多样性。 构象组装。将片段和二面角进行组装。 优势\r通过分子切割减少了构象空间维度，避免生产许多无关变量对模型的影响 MRF更好地捕捉相邻片段间的关系并对构象不确定性进行建模 数据集\rGEOM-QM9和GEOM-Drugs数据集。\nGEOM-QM9 评测方式\r分子构象的质量和多样性：覆盖分数（COV-R）和匹配分数（MAT-R） 预测精度：COV-P和MAT-P。 Zero-Shot 3D Drug Design by Sketching and Generating | NeurIPS 2022\r[2209.13865] Zero-Shot 3D Drug Design by Sketching and Generating\n文章属于综述里面的分子生成任务下的靶向分子设计的内容。提出零样本3D药物设计方法DESERT（Drug dEsign by SkEtching and geneRaTing）。\n目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 解决的问题\r目前的药物设计中传统方法和深度学习方法都有很多局限性。\n挑战是什么\r目前的方法都有一些局限性。\n传统方法遍历大规模药物库，耗时且难以产生新的候选药物 现有的深度学习方法依赖稀缺的实验数据，但是蛋白质口袋的生物活性数据大多缺乏，另一些依赖对接模拟，但是这个非常耗时，且准确性不够会影响模型的泛化能力 核心方法\r这里提出的方法是DESERT。把药物设计分成草图绘制和生成两个阶段。\n草图绘制阶段：获取与目标口袋互补的合理的分子形状。 有参考配体时，直接使用配体形状 在无参考配体时，基于生物学观察从蛋白质口袋中采样合理形状\n生成阶段：通过预训练的SHAPE2MOL模型将形状转换为具体的3D分子。 这里SHAPE2MOL将问题形状到分子的生成问题转换为了图像到序列的生成问题。也就是输入3D图像，给出一个序列，表示3D分子。\n内部结构是3D拓展后的ViT结构。\n优势\r减少数据和模拟依赖：DESERT 不严重依赖对接模拟，仅在后期可选使用对接进行后处理，同时抛弃了昂贵的实验数据，通过在大规模分子数据库（如 ZINC 数据库）上训练模型，降低了对实验数据的需求，避免了过拟合问题。 高效快速：相比基于 MCMC 的 GEKO 模型，DESERT 利用生物知识修剪搜索空间，能更快速地找到较好的解决方案，生成速度比 GEKO 快约 20 倍。 生成高质量分子：基于形状的设计方式使 DESERT 能够生成质量更高的分子。 数据集\r训练：使用了ZINC数据中的数据对SHAPE2MOL模型进行训练，包含了10亿对分子及其相应形状的数据。\n评估模型性能：12 种蛋白质（PDB IDs: 1FKG, 2RD6, 3H7W, 3VRJ, 4CG9, 4OQ3, 4PS7, 5E19, 5MKU, 3FI2, 4J71）相关的数据\n评测方式\r设计结果覆盖的分子空间：唯一性（Uniqueness）、新颖性（Novelty）、多样性（Diversity）、成功率（Success rate）和乘积（Product） 高活性分子的能力：通过比较Vina评分的分布，使用 Median Vina Score（Median）来量化分布。 On Pre-trained Language Models for Antibody | ICLR 2023\r[2301.12112] On Pre-trained Language Models for Antibody\n文章属于综述里面的蛋白质生成里面的抗体生成部分。\n目标：\n主要解决什么问题 挑战是什么 我们提出的核心方法，与同类问题比较的优势在哪 数据集是什么，是否公开 评测方式是什么，有无数据集 解决的问题\r目前的难以探究目前不同的预训练语言模型在抗体任务中的表现。 没有引入生物机制在模型之中。 挑战\r缺乏可靠的抗体特异性基准用于性能评估； 对当前蛋白质预训练语言模型（PPLMs）和抗体预训练语言模型（PALMs）的综合研究不足； 难以判断引入生物机制是否能真正有益于抗体表示学习； 确定预训练表示在实际应用（如药物发现和免疫过程理解）中的作用存在困难 预训练蛋白质语言模型PPLMs：\n利用蛋白质序列探索大语言模型。\n如ProtTrans和ESM-1b将单个蛋白质序列作为输入，使用Transformer架构进行预训练。 MSA-Transformer/MSA-1b模型通过多序列比对（MSA）作为输入。在结构预测方面，该模型优于 ESM-1b，这表明进化信息有助于蛋白质表征学习。 预训练抗体语言模型PALMs：\nAntiBERTy：提出首个抗体特异性语言模型，对在OAS数据库中的5.58亿条天然抗体序列使用Transformer架构进行预训练。 Abalang-H/L：恢复抗体序列中缺失的残基上的迁移学习。 AntiBERTa：在OAS数据库上预训练，并进行微调以用于抗原结合位点位置预测。 核心方法\r提出了抗体理解评估（AnTibody Understanding Evaluation，ATUE）基准和包含特定进化信息的EATLM模型。\n创建了一个全面的抗体基准测试工具ATUE。 抗原结合预测：二分类序列分类任务，确定抗体的CDR区域能否与特定抗原结合。 原因：通过对抗体 CDR 区域的分析，预测其与特定抗原的结合情况，有助于筛选出具有潜在治疗效果的抗体。 互补决定区预测：确定抗体序列上的结合位置的序列标注任务为 CDR 片段的每个残基预测 0/1 标签。 原因：确定抗体与抗原的结合位置，有助于深入理解抗体与抗原的相互作用机制。 B 细胞成熟分析：是一个 6 分类任务，区分 B 细胞抗体序列的成熟阶段，每个序列属于 {未成熟、过渡、成熟、浆细胞、记忆 IgD+、记忆 IgD-} 中的一种。 原因：有助于理解免疫进化过程中的机制。 抗体发现：是一个二分类序列分类任务，区分哪个抗体直接对 SARS-CoV-2 结合负责。 原因：从大量抗体中找出能与 SARS-CoV-2 结合的抗体，对于开发针对该病毒的治疗方法意义重大。 得出了关键观察结果，提供了如何更好地表示抗体的一些指导方针。 PPLMs 在与结构高度相关的抗体任务中表现良好，但在具有高抗体特异性的任务中表现不佳 在大多数情况下，PALMs 在预训练数据较少时表现得与 PPLMs 一样好甚至更好 通过结合进化过程可以改进 PALMs，但来自 MSA 的进化信息并不总是对抗体任务有益 探究引入生物机制对模型的影响，提出EATLM模型。 在传统掩码语言建模（MLM）的基础上，引入两个新的预训练目标以模拟抗体进化的生物机制。 祖先种系预测AGP 突变位置预测MPP 优势\rATUE基准涵盖了多个具有不同特异性的真实任务，能更加全面地评估模型。\n对于EATLM模型来说，引入了AGP和MPP两个预训练目标之后，有以下优势：\n抗原结合预测：对AUC和F1指标有改进 表达预测：在F1和MCC指标上优于其他模型 B细胞成熟分析任务：显著优于其他PALM模型 抗体发现任务：是识别所有钱再结合物最有效的方法，确定了11种潜在的SARS-CoV-2结合抗体，展示了在实际应用中的潜力。 数据集\r抗原结合预测 Mason等人（2021）的数据集 互补决定区预测 使用从 Liberis 等人（2018）收集的包含 1662 个 CDR 片段的数据进行研究。由于只有部分抗体来自进化，所以该任务具有中等特异性。 B细胞成熟分析 数据集来自 Mroczek 等人（2014），有 6 个成熟阶段的 88094 个序列。特异性高，抗体进化与 B 细胞成熟高度耦合。 抗体发现 研究人员收集了 133 名 SARS-CoV-2 患者和 87 名健康人的抗体序列，按照特定流程处理数据，并与 CoV-AbDab 数据库中的序列匹配，以确定潜在的结合抗体。由于来自同一疾病的抗体具有强烈的趋同种系信号，所以该任务特异性高。 评测方式\r对于不同任务，分别进行评估：准确度ACC、马修斯相关系数MCC、F1值、AUC。\nLearning Harmonic Molecular Representations on Riemannian Manifold | ICLR 2023\r[2303.15520] Learning Harmonic Molecular Representations on Riemannian Manifold\n这篇文章所属的领域属于分子生成领域中的表示学习。\n解决的问题\r现在的基于欧几里得空间的分子表示方法需要借助等变网络保证分子在表示旋转和平移的时候的一致性，这里的等距变换群就是E(3)/SE(3)。 目前的分子表示学习多采用自下而上，难以提供不同分辨率的特征。 挑战\r设计一种绕过等变要求，并且能够准确编码3D分子结构的表示。 开发在不同分辨率下为不同任务提供合适特征的多分辨率消息传递机制，特别是复杂的大分子。 核心方法\r用分子表面的拉普拉斯 - 贝尔特拉米（Laplace - Beltrami）特征函数来表示分子，在 2D 黎曼流形上实现多分辨率的分子几何和化学特征表示，并引入谐波消息传递方法进行高效的谱消息传递。\n大模型的解释：\r改变表示空间：传统方法在 3D 欧几里得空间中编码分子结构，为保证分子表示在旋转和平移时的正确性，需要借助等变网络。而该方法将分子表示在 2D 黎曼流形上。可以把黎曼流形想象成一个可以弯曲、变形，但局部性质类似欧几里得空间的特殊空间。在这个空间上，分子的表示天生就具有旋转和平移不变性。就好比把分子放在一个有弹性但又有自身规律的 “网” 上，无论分子怎么旋转、平移，这个 “网” 对分子的描述都不会改变，不需要额外的等变网络来调整，从而绕过了等变要求。 利用拉普拉斯 - 贝尔特拉米特征函数：分子表面可看作黎曼流形，拉普拉斯 - 贝尔特拉米（LB）特征函数是这个流形的固有属性。不同的分子表面有不同的 LB 特征函数，它们就像分子的 “指纹”，能反映分子的形状和结构特点。这些特征函数在刚性变换下保持不变，所以可以用来准确编码分子结构。例如，我们可以把分子表面想象成一个有很多不同纹理的曲面，LB 特征函数就像是描述这些纹理分布规律的工具，不管分子怎么转动、移动，这些纹理的分布规律是不变的，通过分析这些规律就能准确编码分子结构。 多分辨率表示与信息传递：通过对 LB 特征函数的线性组合，可以实现分子表面的多分辨率表示。不同频率的 LB 特征函数可以捕捉分子不同尺度的特征，低频部分反映分子的整体、大致的形状，高频部分则能体现分子的细节特征。在进行信息传递（类似消息在分子表面不同区域传播）时，利用基于 LB 特征函数构建的谐波消息传递机制，能在不同尺度上传播信息。这就好像在一个城市中，有不同规模的道路来传递信息，主干道（低频特征）传递整体的、大致的信息，小巷（高频特征）传递详细的、局部的信息，从而全面、准确地编码 3D 分子结构 优势\r在2D黎曼流形上的分子天然具有旋转和平移不变性，无需依赖等变网络 采用自上而下的方式，能提供多分辨率特征 分子形状定义了黎曼流形，原子构型决定流形上的相关函数，更全面反映分子性质 数据集\rQM9：小分子性质回归任务 配体结合口袋数据集，数据划分方式参考这里 刚性蛋白质对接数据集作为训练集，Docking Benchmark 5.5作为测试集。 评测方式\rQM9 小分子性质回归：通过计算预测结果与真实值的平均绝对误差（MAE）来评估模型性能，对比其他不变性和等变网络模型，如 SchNet、NMP 等。 配体结合口袋分类：使用平衡准确率评估模型预测蛋白质口袋结合配体类型的能力，与 MaSIF-ligand 模型对比。 刚性蛋白质对接：采用 Complex RMSD、Interface RMSD、DockQ 和成功率等指标评估对接性能。Complex RMSD 和 Interface RMSD 衡量预测结构与真实结构的偏差；DockQ 是基于多个标准化标准的综合评分；成功率表示预测结果达到 “可接受” 或更高水平的比例 。 Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D | ICML 2023\r[2305.13266] Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D\n文章属于综述里面的分子生成中的3D分子生成。\n解决的问题\r现有的3D分子生成方法在生成大尺寸分子的时候存在结构质量差的问题。\n挑战\r自回归模型：按照人工设定的顺序逐个生成原子，如同语言生成过程。但分子在 3D 空间具有自然的几何结构，这种方法引入的人为顺序与分子的自然结构不匹配，并且会产生规模和误差累积问题。 非自回归模型：原先的原子级生成方法虽然灵活性较高，但是在片段级生成时，由于化学价态限制，片段冲突常见，且避免片段冲突的复杂度高，随着结构尺寸增加，复杂度呈指数上升，难以获得可靠的分子结构。 核心方法\r提出基于分层Diffusion的分子生成方法Hierarchical Diffusion-based 模型（HierDiff）。\n将3D分子生成问题看作约束生成问题。\n先通过等变Diffusion过程生成粗粒度分子几何结构，其中每个粗粒度节点反映分子中的一个片段 通过消息传递过程和一个新设计的迭代细化采样模块，将粗粒度节点解码为细粒度片段 将细粒度片段组装起来，得到完整的原子分子结构 详细解释\r粗粒度片段扩散：HierDiff 将 3D 分子生成视为约束生成问题，首先定义粗粒度节点的表示，包括不变化学特征和等变位置特征。通过精心设计化学特征（如基于属性和元素的特征）和位置特征（使用中心坐标），利用扩散模型生成粗粒度片段表示及其笛卡尔坐标。在这个过程中，通过特殊设计的初始分布和转移核，保证模型的 SE (3) 不变性，从而有效生成合理的粗粒度分子几何结构。 细粒度片段生成：基于生成的粗粒度节点，通过一系列步骤生成细粒度片段类型和边。具体包括选择焦点节点、预测新边、确定细粒度片段类型以及迭代细化。利用消息传递神经网络和迭代细化模块，不断纠正细粒度节点中的偏差，提高生成分子的真实性。 组装成原子构象：根据细粒度生成过程确定的节点和链接关系，选择合适的原子合并方式构建原子级构象。利用 RDkit 生成局部构象，并通过 Kabsch 算法计算旋转矩阵和平移向量，将局部构象对齐到采样的中心位置，逐步生成完整的原子构象。 优势\r与同类方法相比，HierDiff 能保持非自回归方法的全局建模特性，显著降低寻找可连接片段的复杂度，有效避免片段冲突，生成的分子更具现实性和药物样属性，在多个评估指标上优于现有方法。\n数据集\r使用GEOM-Drugs和CrossDocked2020数据集。\n评测方式\r药物相似性 QED 逆向合成可及性RA 药物化学过滤器MCF 合成可及性分数SAS LogP $\\Delta$LogP 分子重量MW 构象质量 计算覆盖度Cov 匹配度Mat Accelerating Antimicrobial Peptide Discovery with Latent Structure | SIGKDD 2023\r[2212.09450] Accelerating Antimicrobial Peptide Discovery with Latent Structure\n这篇文章属于综述下面的蛋白质生成下的肽设计部分。\n解决的问题\r加速抗菌肽（AMP）的发现。现有的深度学习模型只考虑序列属性，但是忽略了结构对于活性的关系。\n挑战\r目前深度学习模型只考虑序列特征，忽略结构-活性关系。结构对于肽的活性具有重要影响，但是却没有应用。\n核心方法\r提出 Latent Sequence-Structure 模型（LSSAMP）。\n将序列特征和二级结构映射到潜空间中 采用VQ-VAE为每个位置分配一个潜在变量 通过在潜空间采样生成理想序列组成和结构的肽 优势\r考虑了序列和结构信息，肽的抗菌性更强\n数据集\rUniversal Protein Resource（UniProt）中的蛋白质序列和通过ProSPr预测的二级结构 Antimicrobial Peptide Database中的抗菌肽数据集 评测方式\r自动评估指标 使用开源的AMP预测工具估计生成序列的AMP概率。 依据电荷、疏水性、疏水矩这三个对AMP机制至关重要的序列属性评估生成性能。 通过唯一性、多样性和相似性来衡量生成的肽的新颖性。 湿实验室实验 通过上面的自动评估指标从5000个肽中筛选出21个肽进行合成实验 合成了之后在培养皿中采用肉汤微量稀释法测定最小抑菌浓度（MIC）,从而验证。 Equivariant Flow Matching with Hybrid Probability Transport for 3D Molecule Generation | NeurIPS 2023\r这篇文章属于分子生成领域中的3D分子生成。\n解决的问题\r3D 分子生成需要同时确定原子类型和坐标。\n目前的模型，特别是Diffusion方法存在采样速度低和概率动力学不稳定的问题。\n挑战\r现在的扩散模型存在概率动力学不稳定和采样速度低的问题。\n核心方法\r提出等变流匹配（EquiFM）框架。\n引入等变最优传输（Equivariant Optimal-Transport）引导原子坐标的生成概率路径。这方法蕴含最小化坐标变化的先验，能稳定训练并提升生成性能。 基于信息量构建混合生成路径解决模态不一致问题：根据不同组件的信息量差异，设计不同的生成概率路径，形成混合生成路径。 原子特征空间包含多种数据模态，如电荷、原子类型和坐标分别属于离散、整数和连续变量。 利用 ODE 参数化模型提升推理效率：模型基于连续归一化流，由 ODE 参数化。相比Diffusion使用的SDE提高了推理效率。 优势\r稳定 效率高 生成分子质量更好 数据集\rQM9、GEOM-DRUG数据集\n评测方式\r分子建模与生成 通过预测键类型评估生成分子的化学可行性，计算原子稳定性、分子稳定性、有效性、唯一性等指标衡量生成质量 条件分子生成 在 QM9 数据集上测试模型根据给定属性生成分子的能力，通过计算生成分子属性值与目标属性值的平均绝对误差（MAE）来评估性能。 Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks | ICLR 2024\r这篇文章属于分子生成领域的3D分子生成部分。\n解决的问题\r生成模型在应用于3D分子几何生成时面临的挑战，尤其是多模态和噪声敏感性问题。\n挑战\r多模态 分子几何的原子级描述依赖多种数据形式，不同模态数据的统一处理较为困难。 噪声敏感性 微小的坐标噪声就可能导致分子层面的信号急剧下降，影响模型性能 核心方法\r提出GeoBFN。\n贝叶斯流网络BFNs\n假设：数据样本的信息应沿着潜变量的马尔科夫链逐步增加，且信息变化尽可能平滑。 基于引入噪声变量的潜变量模型，通过优化变分下界学习概率分布，在参数空间操作以保证信息变化平滑 统一概率建模 使用统一的概率建模公式处理分子几何中的不同模态。 将3D分子表示为$g=\u0026lt;x,h\u0026gt;$，其中$x$为原子坐标矩阵，$h$包含原子类型和原子电荷等节点信息。 通过EGNN对输出分布进行参数化 保持SE(3)不变性 零质心空间约束下，通过设计满足特定条件的概率模型，使似然函数 \\(p_{\\phi}\\) 具有平移和旋转不变性， 克服噪声敏感性 GeoBFN 在参数空间中通过贝叶斯更新过程来降低方差。在更新中，噪声程度较低的样本会被赋予更小的权重。 优化离散变量采样 使用NEAREST_CENTER函数将输入和中心桶进行比较，并为每个输入值返回值返回最近的中心。 优势\r生成质量上表现卓越。 可以在任意采样步数下达到效率和质量的最优平衡 采样效率较高 数据集\rQM9和GEOM-Drugs\n评测方式\r无条件分子生成 条件分子生成 Multimodal Molecular Pretraining via Modality Blending | ICLR 2024\r这篇文章属于分子生成领域中的表示学习方向。\n解决的问题\r现在的多模态分子预训练方法在对齐2D和3D模态时，仅进行粗粒度分子级对齐，未充分挖掘内在关系。\n挑战\r准确捕捉2D和3D分子中原子关系的内在联系 充分整合多模态信息的模型架构和训练方法 核心方法\r提出blend-then-predict自监督学习方法。\n将不同模态的原子关系混合成统一矩阵进行联合编码，注入到Transformer的自注意力模块中 再恢复特定模态信息 优势\r能在细粒度原子级别对齐和整合2D和3D模态，全面描绘分子\n数据集\r预训练使用 PCQM4Mv2 数据集，来自 OGB Large - Scale Challenge.\n评估在多个公开数据集上进行\nMoleculeNet（用于 2D 分子性质预测，涵盖多种分子性质相关数据集） QM9 量子性质数据集（包含 13.4 万个小有机分子，用于 3D 任务评估） 评测方式\r2D任务：使用支架分割（scaffold split）方式划分数据集 分类任务以ROC-AUC分数为指标 回归任务以RMSE为指标 3D任务：随机划分验证集和测试集 以MAE为评估指标 Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge | SIGKDD 2024\r这篇文章属于分子生成领域的表示学习方向。\n解决的问题\r现有分子表示学习（MRL）模型在学习多视图分子表示时存在不足。难以从化学结构、生物医学文本和知识图谱等异构源中有效捕捉分子知识，且无法充分利用不同视图间的共识和互补信息，不能很好地适应不同应用场景。\n多视图分子表示指的是从不同角度捕捉分子信息，如从微观角度，分子有原子、化学键等；从宏观角度，分子的晶体结构、物理状态等。\n挑战\r现有分子表示学习（MRL） 模型需将视图信息显式融入表示中，以适应广泛应用，但此前模型多通过 “包装文本” 或微调隐式整合，影响对不同视图知识关系的理解 要处理分子结构、生物医学文本和知识图谱等质量和数量各异的信息源的异质性，以往将知识图谱转换为文本的方法可能因预训练数据分布不均衡引入偏差。 核心方法\r提出 MV-Mol 模型\nMV-Mol 模型架构 (a) 基于视图的分子编码器：此编码器是获取视图相关分子信息的关键组件，输入为分子结构和文本提示。 （b）分子分支 结构由\\(M=(V, E, C)\\)表示，利用预训练的 Uni-Mol 进行编码，将其转化为原子的特征表示\\(z(a)\\)。 Q-Former 的分子分支以 K 个可训练的查询向量作为输入嵌入，借助跨注意力机制，在每隔一个 Transformer 层时，从原子表示\\(z(a)\\)中提取关键信息。 （c）文本分支 提示\\(T=[x_{1}, x_{2}, \\cdots, x_{L}]\\)代表不同的分子视图。 Q-Former 的文本分支对文本提示T进行处理。 Q-Former 两个分支的自注意力层共享，使分子表示能够融合不同视图的信息，最终输出基于视图的分子表示。 （d）模态对齐 通过跨模态对比和跨模态匹配进行模态对齐。 跨模态对比损失用于最大化分子结构与文本表示间的互信息， 跨模态匹配损失则通过预测分子结构和文本是否对应同一分子，来培养模型对两者的细粒度理解 （e）多视图知识融合 将关系建模为一种文本提示，从特定视图约束分子知识。 设计知识图谱嵌入和知识图谱不全目标来实现多视图知识融合。 多模态解码器：采用 BioT5 的解码器作为多模态解码器。 输入是基于视图的分子编码器输出。 通过因果生成，将输入转化为自然语言文本，实现对分子表示的自然语言解释 。 优势\rMV-Mol 能更好地捕捉不同视图的分子知识。\n在分子属性预测任务上，比最先进的 Uni-Mol 模型平均绝对增益 1.24% 在跨模态检索任务中，相比最佳基线模型，top-1 检索准确率平均提高 12.9% 在跨模态生成任务中预测也更准确。 数据集\r预训练采用大规模分子-文本对和知识图谱。\n分子 - 文本对通过对 350 万篇科学出版物进行命名实体识别和实体链接获得 知识图谱由多个公共数据库合并构建。 下游实验使用多个数据集\n用于分子属性预测的 MoleculeNet 中的 8 个分类数据集 用于跨模态检索的 PCdes 和 MVST 数据集 用于跨模态生成的 ChEBI-20 数据集等。 代码和数据可在https://github.com/PharMolix/OpenBioMed获取。\n评测方式\r在分子属性预测中 采用 Scaffold split 划分数据集 在多个数据集上微调模型并报告 AUROC 分数 跨模态检索包含结构到文本和文本到结构检索两个子任务， 使用 PCdes 和 MVST 数据集 按 Scaffold split 划分 报告 MRR（平均倒数排名）和 Recall at 1/5/10 跨模态生成包括结构到文本生成和文本到结构生成 在 ChEBI-20 数据集上按原始划分进行实验 采用 BLEU、ROUGE、METEOR 等指标评估分子字幕任务 用精确率、有效率等指标评估文本到分子生成任务 MolCRAFT: Structure-Based Drug Design in Continuous Parameter Space | ICML 2024\r这篇文章属于分子生成领域的靶向药物分子设计方向中的基于结构的药物设计（SBDD）。\n解决的问题\r靶向分子设计有两种，一种是基于配体的药物设计（LBDD），另一种是基于结构的药物设计（SBDD）。LBDD利用目标蛋白质的氨基酸序列，借助已知的配体特征来构建；SBDD利用目标蛋白质的三维结构来设计。\n当前基于SBDD生成模型在生成分子时，常出现不符合要求的情况。\n生成的分子不能同时满足高亲和力、良好的类药性质和合理的 3D 构象这几个关键标准，产生假阳性结果，阻碍了 SBDD 模型在实际中的应用 。\n挑战\r分子模式坍塌：自回归模型在生成分子时倾向于产生有限数量的特定（子）结构，从化学和几何角度来看，其生成的独特分子比例较低，对某些环结构存在偏好，且在模拟不同键类型的键长时表现不佳，无法有效捕捉参考分布的多模态特征。 混合连续 - 离散空间：Diffusion模型虽然通过非自回归生成在一定程度上缓解了模式坍塌问题，但混合连续 - 离散空间使得模型难以准确捕获分子的复杂数据流形。在这个空间中进行去噪时，不同模态之间的不一致性会导致生成的分子存在高应变和不可行的情况，中间噪声潜在值容易超出流形范围。 核心方法\r提出MolCRAFT模型，是首个在连续空间运行的SBDD模型。\n特点 统一的 SE-(3) 等变生成模型 在连续参数空间中进行分子生成 实现 统一参数化：将连续原子坐标和离散原子类型分别进行参数化。 不同模态噪声处理：由于参数的连续性，即使对于离散原子类型也能应用连续噪声。 SE(3)等变网络：使用 SE-(3) 等变网络对蛋白质 - 分子复合物的相互作用进行建模，确保模型在平移和旋转下的不变性。 噪声减少采样策略： 传统的采样方式在每个时间步都对连续原子坐标和离散原子类型进行采样，易引入过多噪声。 MolCRAFT 设计了在参数空间内的噪声减少采样策略，用估计的\\(\\hat{m}=[\\hat{x},\\hat{v}]\\)（\\(\\hat{v}\\)直接采用连续输出的类别值而不采样）直接更新下一步的参数。 优势\r结合亲和力。 能达到参考水平的 Vina 评分（-6.59 kcal/mol），远超其他强基线模型 构象稳定性 在模拟局部模式时表现出色，在键长和角度分布上排名第一，且生成的配体 - 蛋白质复合物中的冲突更少，重新对接后的 RMSD 表现最佳，46% 的生成分子在无需力场优化或重新对接的情况下就接近准确的对接姿势 采样效率更高 速度更快 数据集\r使用 CrossDocked 数据集，经过基于 RMSD 的过滤和 30% 序列同一性拆分后，得到 100,000 个训练对和 100 个测试蛋白质。\n评测方式\r结合亲和力 构象稳定性 类药性质 QED 合成可及性SA 多样性Div 整体评估 结合可行性（合理亲和力+构象稳定的分子比例） 成功率（满足Vina Dock、QED和SA阈值） 生成效率 ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling | ICML 2024\r这篇文章属于蛋白质生成领域中的表示学习领域。\n解决的问题\r当前蛋白质语言模型主要在残基尺度运行，无法提供原子尺度信息。\n挑战\r统一分子建模难题：残基和原子尺度使用的词汇表不兼容，直接在原子尺度对蛋白质进行表示和预训练效率低下，难以实现有效的统一分子建模。 位置编码设计困难：设计合适的位置编码来准确描述同一蛋白质中残基和原子之间的关系颇具挑战，涉及残基与残基、残基与原子、原子与原子之间的多种关系，而现有蛋白质语言模型的编码方法无法满足需求。 核心方法\r提出ESMAA（ESM All-Atom），实现原子尺度和残基尺度统一分子建模的新方法。\n在多尺度代码转换蛋白质序列上进行预训练 利用多尺度位置编码来捕捉残基和原子之间的关系 优势\rESM-AA 能同时处理残基和原子尺度信息\n在蛋白质 - 分子任务上表现更优。 如在酶 - 底物亲和力回归、药物 - 靶点亲和力回归等任务中超越了其他模型，实现了最先进的结果。 在蛋白质任务和分子基准测试中也有良好表现 数据集\r预训练数据集\n蛋白质 AlphaFold DB 数据集： 800 万个由 AlphaFold2 预测的高置信度（PLDDT \u0026gt; 90）蛋白质序列和结构 小分子 使用 Zhou 等人提供的数据集：含有 1900 万个分子和 2.09 亿个由 ETKGD 和默克分子力场生成的构象 评测方式\r蛋白质 - 分子任务：在酶 - 底物亲和力回归、药物 - 靶点亲和力回归和酶 - 底物对分类任务上进行微调评估，将模型预测结果与实验数据对比，使用均方误差（MSE）、决定系数（R²）、皮尔逊相关系数、准确率（ACC）、马修斯相关系数（MCC）、受试者工作特征曲线下面积（ROC-AUC）等指标衡量性能。 蛋白质任务：通过二级结构预测和无监督接触预测任务测试模型对蛋白质结构的理解能力，使用准确率等指标评估，且模型输入为纯残基序列。 分子任务：利用标准分子基准测试 MoleculeNet 中的任务，如分子性质分类和回归任务，使用平均绝对误差（MAE）、AUC 等指标评估模型性能。 Mol-AE: Auto-Encoder Based Molecular Representation Learning With 3D Cloze Test Objective | ICML 2024\r这篇文章属于分子生成领域中的3D分子表示学习方向。\n解决的问题\rencoder-only model在预训练和下游任务目标之间存在不一致性，导致预训练学到的特征在下游任务中迁移性差 坐标去噪目标会引发训练不稳定以及引入不真实噪声，同时原子坐标在该过程中作为内容和标识符的双重角色产生冲突，影响模型性能。 挑战\r克服预训练和下游任务目标不一致带来的难题，使模型在不同阶段学到的特征能有效应用于实际任务 解决坐标去噪导致的训练不稳定和原子标识符混乱问题，让模型能够稳定训练并准确学习分子结构信息。 核心方法\r提出 MOL-AE 模型，核心包含基于 Transformer 的 3D 信息感知自编码器结构以及 3D Cloze Test 目标。\n整体架构： MOL-AE 模型处理 3D 分子时，主要聚焦 3D 结构和原子类型信息。 原子类型建模：可借助原子 MLM 目标轻松实现 3D 结构的建模：模型由编码器\\(q_{\\phi}\\)、解码器\\(p_{\\theta}\\)构成，且二者均以 Transformer 架构为基础。 3D 信息感知自编码器 Transformer Block：Transformer 由多个 Transformer 块组成，每个块包含多头自注意力层和前馈层。此过程能有效捕捉分子信息。 3D 感知成对特征：因普通 Transformer 难以处理 3D 信息，MOL-AE 采用将原子对之间的欧几里得距离编码为额外成对特征的方法。帮助模型更好地理解分子的3D结构。 编码器和解码器：编码器\\(q_{\\phi}\\)由\\(L^{enc}\\)层 Transformer 块构成，3D 坐标信息C经其处理后编码为\\(X^{L^{enc}}\\) ，并将其作为潜在表示Z 。解码器\\(p_{\\theta}\\)由\\(L^{dec}\\)层 Transformer 块组成，由于 3D 结构已编码在Z中，其输入成对特征初始化为零。 3D Cloze Test 目标 添加位置编码（PE）到解码器：为解决坐标去噪中原子标识符混乱问题，MOL-AE 在解码器中添加 PE。 在打乱坐标时，PE 作为稳定的标识符，帮助模型区分不同原子。同时，仅在解码器添加 PE 可避免引入的顺序信息对编码器学习分子高质量表示产生影响。 这里可以类比GPT会添加位置编码。 原子丢弃：传统去噪目标可能使模型学习不可靠的噪声分布，MOL-AE 通过随机丢弃部分原子及其坐标（如从输入坐标C中随机移除k行得到\\(D(C)\\) ）来干扰数据，使模型专注于剩余无噪声的子结构，从而更好地学习原子空间关系。 优势\r缓解了预训练和下游任务目标不一致的问题，提升了特征的迁移能力 解决了坐标去噪带来的不稳定训练和原子标识符混乱问题，使模型训练更稳定 在多个分子理解任务中性能显著优于当前最先进的 3D 分子建模方法，在分子分类和回归任务的基准测试中取得了优异成绩 数据集\r预训练数据集：使用 Zhou 等人提供的大规模分子数据集，包含 1900 万个分子和 2.09 亿个构象，由 ETKGD 和 Merck 分子力场生成，每个分子有 11 个随机生成的构象，为提高计算效率，预训练时去除了氢原子，未明确该数据集是否公开。 微调数据集：采用广泛使用的 MoleculeNet 基准数据集，包括 9 个分类数据集和 6 个回归数据集，该数据集公开。 评测方式\r分子分类任务：使用 ROC-AUC 作为评估指标，在 9 个分类数据集上进行实验，数据集为 MoleculeNet 中的相关分类数据集。 分子回归任务：采用平均绝对误差（MAE）和均方根误差（RMSE）作为评估指标，在 6 个回归数据集上进行实验，数据集为 MoleculeNet 中的相关回归数据集。通过与多个监督和预训练方法的对比，评估 MOL-AE 的性能。 ","date":"2025-02-26T22:17:58+08:00","permalink":"https://ionfeather.github.io/2025/ai-assist-drug-design/","title":"AIDD | 论文阅读"},{"content":"一直想用一些shortcodes来优化一下页面的呈现方式。\n特别感谢：\n在 Stack 主题上可行的短代码们 一些Hugo短代码 Hugo |另一篇Stack主题装修记录 | 小球飞鱼 Hugo | 在文章中插入轮播图片 | 小球飞鱼 文字\r重点标记\r好喜欢蓝色！\r文本折叠\r点击展开\r这是第一个段落的内容。\n这是第二个段落的内容，位于折叠部分，实际使用别忘了双括号！\n文字黑幕\r数据删除！数据删除！\n但总之换行的话就加个空标签。\n高斯模糊\r一些手动打码效果！\n但总之换行的话就加个空标签。\n文本位置\r文字居左\n文字居中\n文字居右\n摘录引用\r羊皮卷上所载一切自永远至永远不会再重复，因为注定经受百年孤独的家族不会有第二次机会出现在大地上出现。\n加西亚·马尔克斯\r《百年孤独》 居中引用\r有些人说\n换行敲多了\n就是\n诗\n键盘样式\rCtrl+Alt+Del\n卡片\r可以在这里插入链接假装是卡片式链接。\n好像不能插入图片？\n换行需要空标签。实际使用需要双括号。\n标签块\rWarning：需要双括号。\ninfo：这是一条信息。\nnote：可以标注一下，但是没必要。\ntip：在示例里胡说八道会使观看者会心一笑。\n对话框\r导师\u0026nbsp;\u0026nbsp;\u0026nbsp;2024-10-12 14:30\r做一个论文阅读的大模型。 2024-10-12 14:45\u0026nbsp;\u0026nbsp;\u0026nbsp;我\r好的老师。 时间轴\r2024-10-20\r博客\r创建ionfeather\u0026#39;Log\r使用Hugo的Stack主题\r2024-11-04\r博客\r增加评论区\r部署Waline，但还有一些问题\r2025-02-13\r博客\rBug修复\r修复了一些bug，如邮箱显示错误、搜索功能失效等\r图片滚动\rBilibili\r网易云音乐\r标签墙\r其实这个不算是shortcodes，但是我不知道把这个放在哪里。所以这个犄角旮旯就不错！\n特别感谢：\nHugo | 月球基地开发历程 Blog | 主题重新施工，和书影游展示墙 | 小球飞鱼 标签墙最后放在了我的「关于」页面里。\n欢迎大家去看！\n","date":"2025-02-18T13:40:27+08:00","permalink":"https://ionfeather.github.io/2025/shortcodes/","title":"博客探索 | 搬运一些Shortcodes"},{"content":"论文阅读 | 多智能体协作机制：大语言模型综述\r[2306.03314] Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents\n摘要\r随着大语言模型（LLMs）的最新进展，代理式人工智能（Agentic AI）在现实应用中取得了显著进展，朝着基于多个大语言模型的智能体迈进，实现感知、学习、推理和协同行动。这些基于大语言模型的多智能体系统（MASs）使得一组智能体能够协作解决复杂任务，并以大规模方式实现集体行动，从孤立的模型转向以协作为核心的方法。\n本文提供了关于多智能体系统协作方面的广泛综述，并提出了一个可扩展的框架来指导未来的研究。我们的框架根据关键维度对协作机制进行表征：参与者（涉及的智能体）、类型（例如，合作、竞争或合作竞争）、结构（例如，点对点、集中式或分布式）、策略（例如，基于角色或基于模型）以及协调协议。通过对现有方法的回顾，我们的研究成果为揭示和推动基于大语言模型的多智能体系统向更加智能和协作的解决方案发展，特别是在复杂的现实应用中，提供了基础。\n此外，本文还探讨了多智能体系统在不同领域的各种应用，包括5G/6G网络、工业5.0、问答系统、以及社会文化环境，展示了它们的广泛应用和更深远的影响。最后，我们总结了关键经验教训，分析了多智能体系统面临的开放挑战，并指出了朝着人工集体智能发展的潜在研究方向。\n文章大纲\r应用\r方法 领域 主要贡献 优点 缺点 参考文献 LLM-SC 物联网 作为知识生成器增强语义解码器 利用大语言模型，实现显著的编码增益 由于使用大语言模型，计算资源需求高 [130] LaMoSC 物联网 提出一种大语言模型驱动的多模态融合语义通信 在低信噪比条件下表现稳健 由于使用大语言模型和视觉 Transformer，计算资源需求高 [157] LAM-MSC 物联网 为多模态数据设计联合编码器；大语言模型作为知识生成器 一个编码器和解码器可处理多种类型的数据；实现更好的编码率和重建误差 由于使用大语言模型，计算资源需求高 [65] GMAC 物联网 利用大语言模型实现观察状态与自然语言之间的语义对齐，并压缩语义信息 提高收敛速度；实现无通信的多智能体协作 由于使用大语言模型，计算资源需求高 [160] LLM-Blender 自然语言生成 采用多种大语言模型代理的集成方法进行候选排序 能够生成比现有候选更好的输出 为实现最优解，需要进行 O (n) 次推理，导致计算开销大 [64] SOT 自然语言生成 并行生成每个答案框架；完成答案内容（需要规划结构） 通过并行加速推理速度；适用于需要长结构答案的问题 答案质量评估远非完美，由于提示集有限；不同代理的并行请求可能会影响服务吞吐量 [95] Meta-Prompting 自然语言生成 构建高级元提示来指导大语言模型 保持连贯的推理思路；挖掘各种专家角色 多次模型调用成本较高；需要大量的规模和相当大的上下文窗口 [119] MAD 自然语言生成 两个代理表达各自的论点；一个评判者监控和管理辩论 减少偏差和扭曲的认知；鼓励无限的外部反馈 由于辩论时间长，计算成本高；大语言模型在长场景中难以保持连贯性和相关性 [77] FORD 自然语言生成 包括三个阶段的辩论：公平辩论、不匹配辩论、圆桌辩论 通过辩论让大语言模型探索自身理解与他人概念化之间的差异 除常识推理外，无法涵盖各种任务；严重依赖多项选择任务，限制了其泛化能力 [140] ChatDev 自然语言生成 采用聊天链将每个阶段分解为更小的子任务，实现代理之间的多轮通信，以协作开发解决方案 最大限度减少代码幻觉（提供的源代码缺失的情况） 没有清晰、详细的要求时，代理难以理解任务想法；通用软件的自动化评估非常复杂；多个代理需要更多的令牌和时间，导致计算需求大 [105] AgentVerse 自然语言生成 由专家招募、协作决策、行动执行、评估四个阶段组成 提高大语言模型在不确定情况下的泛化能力；提高代理的适应性 协作决策过程中代理之间的通信存在挑战 [24] AgentCoord 社会与文化领域 为协调策略提供结构化表示；采用三阶段方法将一般目标转化为可执行策略 简化协调策略的表示和探索；最小化代理的重复实例 仅支持在纯文本环境中协调代理协作；仅支持静态协调策略设计 [97] OpenAI\u0026rsquo;s Swarm 自然语言生成 用于多智能体编排的例程和交接；轻量级协调与执行框架 适用于需要可扩展性的应用；交接机制允许在专门代理之间实现无缝过渡 主要关注基于角色的协议和集中式 / 分布式结构；尚未准备好投入生产 见原文 TE 社会与文化领域 在主题研究中模拟人类参与者的代表性样本 能够模拟不同的人类行为，并揭示模拟中的一致偏差 需要研究更多的人类行为和额外的大语言模型，以确保关键发现的准确性 [36] AgentInstruct 社会与文化领域 通过迭代的跨代理细化生成多样化的自然语言数据，包括文化数据 能够通过工具使用、代理能力等从生成的数据中训练更强大的模型 需要人工构建生成流程 [88] SocialMind 社会与文化领域 整合言语、非言语和社交线索，通过增强现实眼镜生成现场建议 设计并利用多模态、多层协作代理系统 需要先进的边缘硬件来处理复杂系统 [144] CulturePark 社会与文化领域 促使基于大语言模型的代理进行跨文化交流模拟 生成的数据可用于训练具有不同文化背景的模型，减少偏差并实现民主化 仍然依赖大语言模型对每种文化的了解，因此对资源较少的文化效果有限 [73] Mango 社会与文化领域 通过对概念和文化的提示，从基于大语言模型的代理中提取高质量知识 自动化方法可生成大量资源 人类评估需要来自更多样化的背景 [94] 六个思考帽的设计\r白色思考帽\r功能：收集客观信息。 实现方式 对论文进行解析。 从论文文本中抽取结构化数据。 从网络中搜索作者之前的研究成果。 从网络中搜索同类研究的对比数据。 绿色思考帽\r功能：对论文提出创新性改进，探索论文的可能性 实现方式 未定。 黄色思考帽\r功能：积极角度评估论文，找出论文的优点和贡献。 实现方式 用优点和创新点微调后的大模型。 黑色思考帽\r功能：批判性思考，找出论文的问题和不足。 实现方式 用批判性数据集微调后的大模型。 红色思考帽\r功能：主观感受和直觉判断。 实现方式 让智能体多阅读论文，找到好的论文之间的共性和形成自己的「偏好」。 蓝色思考帽\r功能：控制评审流程。 实现方式 未定。 蓝色思考帽智能体应该如何控制？\r基于工作流管理的集中式控制方法。蓝色智能体明确规定了其他智能体的工作顺序、时间和交互方式。 基于协商机制的分布式控制方法。在评审开始时，蓝色智能体发起评审任务，各思考帽智能体根据自身能力和状态反馈可承担的工作及预计时间。比如白色告诉蓝色需要5分钟完成，绿色说在白色完成后需要10分钟\u0026hellip;通过这些反馈，蓝色智能体来制定计划。 基于事件驱动的动态控制方法。不同的智能体换成之后会触发不同的事件，如白色完成后让绿色工作，黑色和黄色在辩论后无法达成共识，就再次进行辩论等。这个事件定义较难。 ","date":"2025-02-15T21:58:31+08:00","permalink":"https://ionfeather.github.io/2025/multiagentcollaboration/","title":"论文阅读 | 多智能体协作机制：大语言模型综述"},{"content":"雨水：表示降水开始，雨量逐步增多。雨水节气天气变化不定，是全年寒潮过程出现最多的时节之一，忽冷忽热，天气乍暖还寒。\n北京最近的天气确实是这样。在北京能感受到24节气的准确，能感受到四季分明是什么感觉。\n但是！！雨水，雨水，北京来点儿雨吧。自从去年的12月以来，我没有见过一滴雨落到北京的地表，这里的晴天就像是默认背景，太阳和月亮每天都是固定角色出现在地平线和天空中。北京的「雨水」是艳阳高照。\n上了研究生学术没有做多少，兴趣爱好培养了不少——台球、博客、摄影、健身…可能还打算学个吉他和乒乓球。忙不过来，实在是忙不过来了。还是得多放点时间在学习上呀。\n今天是情人节，这么一想，晚上的健身房应该会比较空，可惜昨天跑步跑太狠，把脚掌磨出了一个水泡，走路都有点儿疼，今晚回去可以做做力量训练，就先不做有氧了。\n好羡慕甜甜的爱情。\n2025-02-14 15:02\n买了辆电瓶车，之后出行方便多了。 终于不用来回奔波那么久了。 去地铁站终于不用思考用共享单车还是公交车了。\n2025-02-14 21:58\n今天把原先的房子里的东西全部打扫干净，让它恢复成原来的样子。我才意识到，虽然住了半年，但是我们在这个房子里的印记能在一天之内被打扫得无影无踪。原来人是流动的水。\n和室友买了一些做饭的家伙，之后可能会在出租房里（我其实也愿意称它为「家」）做一些简单的菜。室友的女友可能过两天会来，到时候可以期待一下她的厨艺（好像听说也是新手，那还是期待我自己的进步吧\n上了称，感觉自己胖得不行了。但是过两天又是组会，头疼，我这周什么也没干。\n2025-02-15 19:26\n房东来看房，她（果然）嫌弃我们打扫得不干净，表示要找保洁来清理。MSY、SH和我商量了一下，最后让她扣了300元。我该怎么说——其实这个人还算好说话。反正扣完了之后也没说什么，现场就转了钱。我本来还以为她会拿门钥匙这件事说事。\n帽子到啦！我的MBTI帽子到了，我直接往上贴了一个「ENFP」，然后周游工位告诉每个见到我的人，可惜今天是周日，没见到几个人。大家都去哪儿啦？\n不过感觉自己也不用担心（什么担心，我这叫好奇）大家都去哪儿了。明天就是组会，目前还没有进展，现在就开始看论文吧！\n2025-02-16 13:35\n听说有个师姐被求婚了，被求婚了？被求婚了！看到了照片，男帅女靓啊，又是羡慕别人爱情的一天。\n不过我还是有一点儿恍惚：原来这也是我这个年纪该听到的事情吗？我还以为自己还小。问了一下同门，她说她也不介意研究生读完就结婚，我开玩笑地说：「那明年可以暗示你一下你的男朋友了。」\n对我来说，可能还是有点早了吧？我心里还没准备好。但是这种事情谁说得好呢？\n2025-02-16 22:33\n开完组会了。\n最近组会的气氛比较轻松愉快。但是在我看到同门和老师写的一篇论文之后，还是有些不淡定——我还没有将idea实现、落地的能力。在上面散发着新鲜油墨气味的论文被交到我手上的时候，我内心还是有些无奈。\n我也不想安慰自己说未来也能写出来，我对这件事情甚至没有任何认知。写一篇论文到底需要什么？就我来说，和如何把冰箱里的大象拿出来一样，是一个全新的领域。\n2025-02-17 17:07\n又是10点才起床的一天。\n室友的女友来我们这里借住几天，没想到是个精力十足、爱笑、笑起来是「嘿嘿嘿」的山东女生。她的笑声确实听起来很愉快，能够感受到她很开心，听起来穿透力很强。没想到这么响亮的笑声在之前都没听过，看来之前在出租屋的小房间里，她也是忍耐住了自己的笑（笑\n中午的时候，她和室友下厨，做了油泼面，吃着还可以，比学校的面也不遑多让——或许是学校食堂太糊弄。我拿了我的碗，刷刷刷吃了一碗半。\n晚上又碰见他们了。室友和他女友在跑步机旁边，室友跑步，女友爬坡。\n2025-02-19 23:35\n昨天去中国电影博物馆看了《哪吒2之魔童脑闹海》。电影在15:25开始，我是在12:00出发，但是博物馆的位置确实有点儿偏——也可能是我的位置太偏——我到达那里已经是14:00了。\n走马观花地参观了一下中国电影博物馆的展厅，进门是主旋律正能量主题展厅。往里走，里面有一个巨大的环形的展厅，地板、墙壁都是LED屏幕，一共有四层楼高，有一个环形的缓缓上升的参观路线，绕着墙壁，通向其他二楼三楼的展厅。\n在展厅里，对我这个电影盲来说，只能看到那些最脍炙人口的电影，比如《警察故事》《小蝌蚪找妈妈》，才会知道「啊！原来是它！」\n有没有书的展览馆！搜了一下文学没有像是中国电影博物馆一样这种大而全的，但是专题类的很多：中国现代文学馆、北京鲁迅博物馆、老舍纪念馆、巴金故居、上海文学博物馆\u0026hellip;感觉又种草了不少。\n不愧是IMAX GT屏幕，真的很壮观。我坐得特别前面，还有点儿偏，哪吒也并非最适合它的1.43:1的屏幕比例，因此上下还是有黑边，《奥本海默》和《沙丘》会更适合它。但，但是（申公豹式强调）这不妨碍我看电影的时候感慨画质和细节，云雾、锁链、粒子\u0026hellip;在我正前方130°范围里，全都是哪吒。\n电影好！屏幕好！看电影的人好！真是一次美好的体验。\n2025-02-21 21:30\n有的时候也很焦虑诶。又是一天什么也没做。昨天给妙妙讲了讲道理——学习就像是跑仓鼠的轮子，滚起来就会一直向前。\n我也该把我的轮子滚起来了。\n2025-02-22 21:15\n朋友的生日，唱歌从11点唱到凌晨4点。\n五个人算是把自己拿手的歌全唱了——我发现我以前的五音不全病似乎有好转的迹象——《思念是一种病》《一事无成的伟大》我宣布现在已经是我的拿手曲目了。\n2025-02-28 12:00\n","date":"2025-02-14T14:36:39+08:00","image":"https://ionfeather.github.io/2025/rainwater/cover_hu_38fae1dac40a8c28.jpg","permalink":"https://ionfeather.github.io/2025/rainwater/","title":"雨水 | 北京的「雨水」是艳阳高照"},{"content":"春节快乐！\n工科研究生的假期有点短暂了。我已经回校了。\n到学校了感觉自己好多东西需要购置。Apple Watch的表带现在明显太松了，但是官方太贵，第三方又有点儿硬，计划看一下Bilibili上的测评，进行一波购置。还有看上了影视飓风的一款帽子，上面可以贴上你的mbti，这对我这个enfp根本无法拒绝。\n又来到了北方，又遇到了高铁上一望无际的平原，感觉是另一种大海，在这片海里，有冰封的河流，有枯黄的树枝，还有炊烟和蜗居的人们。有一种说法是南方人向往雪，北方人向往海，这么一看，我应该是个不那么彻底的南方人。我两个都很向往。\n今天是元宵节，我本科的时候是灯谜社社长。元宵节，英语叫Lantern festival，也就是灯节，是灯谜社最重要的节日。在传统文化节里，我会张贴灯谜，擂起鼓，在鼓声滚滚中，同学需要猜出我出的谜题——有些是我们社团自己写的，所以很难猜，需要脑子有点儿回路，特别是与英雄联盟或者是与本科学校相关的那种谜题。\n虽然现在天气还很冷，但是感觉春意渐浓，即便树梢仍然枯枝，阳光灿烂的时候也能把寒意给驱散。\n我已经开始期待春天了。玉兰、银柳、山茶、樱花、樱桃花、木绣球、垂丝海棠。我要抄起我的相机，出门拍花拍鸟。\n不管怎么说，虽然已经上了几年（学校一年，外界一天）班了，现在还是春节期间，我一会儿说不定可以出门，去颐和园旁边拍花灯。\n","date":"2025-02-12T17:51:19+08:00","image":"https://ionfeather.github.io/2025/lanternfestival/cover_hu_8c6d9f78018048fa.jpg","permalink":"https://ionfeather.github.io/2025/lanternfestival/","title":"元宵节 | 东风夜放花千树，更吹落、星如雨"},{"content":"之前一直在用Typora来写文章，发现有的时候也太难用了，不仅插件少，还要付费。这个时候看到很多的博客都用Obsidian来写，不得不心动了。\nObsidian的插件\rLinter\rObsidian Linter插件：打造统一、美观的笔记环境 - 知乎\n我不得不赞赏一下这个Linter，真的很好用，格式化目前的Markdown内容一直是我的心头痒，对于我这个强迫症来说，现在只需要按一下Ctrl+S就可以让我的敲击的内容都非常规范化，这实在是伟大的发明。\nExcalidraw\r这个插件还挺好看的，可以绘制手绘风格的图像，我绘制一些想法会更加方便。\n其他\rAdvanced Tables：对写表格比较有帮助。 Customizable Menu：自定义右键快捷键。 ","date":"2025-02-12T15:59:30+08:00","permalink":"https://ionfeather.github.io/2025/obsidian/","title":"博客探索 | 使用Obsidian来写博客"},{"content":"全书结构\r预备知识\r张量\r张量表示一个由数值组成的数组，这个数组可能有多个维度。\n具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。\n张量的创建\rimport torch x = torch.arange(12) x.shape x.numel() X = x.reshape(3, 4) torch.zeros((2, 3, 4)) torch.ones((2, 3, 4)) torch.randn(3, 4) torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) 运算符\r按元素运算\r常见的运算符这里用作按元素运算。\nx = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y # **运算符是求幂运算 可以得到\n(tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) 还有很多的一元运算符都可以用在按元素运算。\n线性代数运算\r求和/平均值\r直接调用sum函数，会将其变成一个标量，也可以指定axis = 1维度来指定轴来进行降维。\nA.sum() A.sum(axis = 1) A.sum(axis = [0, 1])# 对于矩阵来说，相当于A.sum() 同理，A.mean()也是一样的。\n如果希望能够在求和或者平均值的时候保持轴数不变，可以使用keepdims = True。\n如果希望能够沿着某个轴计算A元素的累计总和，可以使用cumsum函数。\nsum_A = A.sum(axis=1, keepdims=True) A.cumsum(axis=0) 点积\rx = torch.arange(4) y = torch.ones(4, dtype = torch.float32) torch.dot(x, y) 矩阵-向量积\r当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。\n矩阵-矩阵乘法\r我们可以将矩阵-矩阵乘法AB看作简单地执行m次矩阵-向量积，并将结果拼接在一起，形成一个n×m矩阵。\n在下面的代码中，我们在A和B上执行矩阵乘法。 这里的A是一个5行4列的矩阵，B是一个4行3列的矩阵。 两者相乘后，我们得到了一个5行3列的矩阵。\nB = torch.ones(4, 3) torch.mm(A, B) 张量连结\r在这里，dim=0说明是第一个维度进行拼接；dim=1说明是第二个维度进行拼接。\nX = torch.arange(12, dtype=torch.float32).reshape((3,4)) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) 广播机制\r特别需要注意这个，可能会导致错误发生。\na = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) a, b 由于a和b分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的3×2矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。\n索引和切片\r与Dataframe中相似。\n节省内存\r如果直接使用X = X + Y就是重新创建一个元素。但是，有些时候希望执行原地操作。\n如果希望执行原地操作的话，可以使用两种方式，此时不会占用新的空间：\nX[:] = X + Y X += Y 转换为其他对象\r转换为Numpy非常容易：A = X.numpy()\n转换为Python标量：a.item()或者使用内置函数float(a)等。\n自动求导\r自动求导是计算一个函数在指定值上的导数。\n如何实现？ 计算图：将代码分解成操作子，将计算表示成一个无环图。 关于计算图，有显式构造 vs 隐式构造两种构造方式。\n特性 显式构造 隐式构造 计算图构建方式 显式定义 隐式定义 计算图类型 静态图 动态图 典型框架 TensorFlow 1.x, Theano PyTorch, TensorFlow 2.x (Eager) 有两种求导的方式，对于一个链式法则，我们可以采取正向累积和反向累积（也称反向传递）。\n示例说明\n以 \\( y = (x_1 + 2x_2)^2 \\) 为例：\n反向传递： 前向计算 \\( z=11, y=121 \\)。 反向计算 \\( \\partial y/\\partial z=22 \\rightarrow \\partial y/\\partial x_1=22, \\partial y/\\partial x_2=44 \\)。 正向传递： 前向计算 \\( z=11 \\)，同时记录 \\( \\partial z/\\partial x_1=1, \\partial z/\\partial x_2=2 \\)。 前向计算 \\( y=121 \\)，同时记录 \\( \\partial y/\\partial z=22 \\)。 直接组合导数得到 \\( \\partial y/\\partial x_1=22 \\times 1=22 \\)，\\( \\partial y/\\partial x_2=22 \\times 2=44 \\)。 反向累积\r使用反向传递的时候，在我们计算$y$关于$x$的梯度之前，需要一个地方来存储梯度。\n重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量$x$的梯度是向量，并且与$x$具有相同的形状。\n","date":"2025-01-08T16:00:34+08:00","image":"https://ionfeather.github.io/2025/d2l-01/assets/cover_hu_a71966db35cff8be.png","permalink":"https://ionfeather.github.io/2025/d2l-01/","title":"学习笔记 | 《动手学深度学习》"},{"content":"虚拟环境配置经历\r我之前配置好了一个虚拟环境名为vllm，专门用于vllm的启动，我还特意将其中的虚拟环境中的所有包的版本保存到vllm_requirements.txt文件中。\n但是我一顿操作之后，原本配置好的环境现在也没办法使用了。此时我庆幸自己想到用vllm_requirements.txt文件保存。但是在进行pip install -r vllm_requirements.txt的时候，出现了报错的情况，竟然说里面有一个包的版本是yanked version（撤回版本），无法下载，给我气晕了。\n吃一堑，长一智。配置好的环境就不要变了，应该另外复制一个环境，在复制的环境上进行修改。\n此外，我每次进行配置环境我都会忘记怎么配置和删除。是我最近记性变得太差了吗？总之我写一个文档，记不住就查一下。\n配置环境\r使用conda配置虚拟环境\r创建新的环境\r使用Terminal创建新的环境。\nconda create -n \u0026lt;new_env_name\u0026gt; python=3.10.0 激活虚拟环境\nconda activate \u0026lt;new_env_name\u0026gt; 安装包\nconda install \u0026lt;package\u0026gt; pip install \u0026lt;package\u0026gt; 从已有的文件中安装包/虚拟环境\r如果想要安装requirements.txt文件，就可以直接\npip install -r requirements.txt 如果想要安装的是environment.yml文件，应该改用conda来创建虚拟环境\nconda env create -f environment.yml 查看虚拟环境列表\nconda env list 复制原来已有的虚拟环境\r如果有一个环境已经配置好，我不希望破坏它，可以复制一个一模一样的环境，再在上面进行修改，这样就不会导致原来那个环境产生问题。\nconda create --name \u0026lt;new_env_name\u0026gt; --clone \u0026lt;old_env_name\u0026gt; 删除虚拟环境\r删除指定的虚拟环境\nconda activate base conda remove -n \u0026lt;env_name\u0026gt; --all 在conda中配置Jupyter内核\r安装Jupyter内核\r总是忘记Jupyter内核如何配置。记录一下：\n安装ipykernel。\nconda install ipykernel 将虚拟内核添加到jupyter内核中。\npython -m ipykernel install --user --name \u0026lt;your_env_name\u0026gt; 删除jupyter内核\r查看目前有的jupyter内核\njupyter kernelspec list 删除指定的jupyter内核\njupyter kernelspec remove \u0026lt;your_kernel_name\u0026gt; 封面\r照片是2024/12/7的时候同门团建的时候我拿大疆Pocket3拍的。拍的建筑是东郊民巷的圣弥厄尔大教堂。非常开心的一天。\n","date":"2024-12-15T20:24:19+08:00","image":"https://ionfeather.github.io/2024/virtual-environment-config/cover_hu_f46c1ea00cae3cce.jpg","permalink":"https://ionfeather.github.io/2024/virtual-environment-config/","title":"操作记录 | 虚拟环境配置操作记录"},{"content":"为什么要学习LangChain\r我希望能够构建一个能阅读PDF论文的Agent，并且能够输出对论文优缺点的评价。\n导师\u0026nbsp;\u0026nbsp;\u0026nbsp;2024-10-12 14:30\r做一个论文阅读的大模型。 2024-10-12 14:45\u0026nbsp;\u0026nbsp;\u0026nbsp;我\r好的老师。 使用LangChain听说比较方便。\nLangChain是用来做什么的？\rLangChain是一个用于开发由LLM驱动的应用程序的框架。也就是说我们可以把LLM作为内核，LangChain作为外壳，搭建一个程序出来。\nLangChain提供了\n组件：处理LLM的组件的抽象； 定制链：把组件拼起来，实现一个特定用例。 对于阅读PDF，目前有两个想法：\n将PDF转为JSON，然后输入到LLM中； 构建RAG。使用LangChain能够比较方便地实现这个功能，听ZLB说这个也不是很难。我之前的畏难情绪可能太重了，现在写一个文档，激励和记录一下自己学习。 RAG是什么？\r虽然LLM非常强大，但它们对于它们未经训练的信息一无所知。如果您想使用LLM来回答它未经训练的文档相关问题，您需要向其提供这些文档的信息。最常用的方法是通过“检索增强生成”（ retrieval augmented generation，RAG ）。\n检索增强生成的思想是，在给定一个问题时，首先进行检索步骤以获取任何相关文档。然后将这些文档与原始问题一起传递给语言模型，并让它生成一个回答。然而，为了做到这一点，首先需要将文档以适合进行此类查询的格式呈现。\n构造一个语义搜索引擎\rBuild a semantic search engine | 🦜️🔗 LangChain\n读取PDF\rHow to load PDFs | 🦜️🔗 LangChain\n这里，文档中推荐使用了pypdf库。这里\n在实际应用中可以使用其他提取效果更好的库。LangChain支持的PDF格式很多，可以选择一下。\nDocument Loader Description Package/API PyPDF Uses pypdf to load and parse PDFs Package Unstructured Uses Unstructured\u0026rsquo;s open source library to load PDFs Package Amazon Textract Uses AWS API to load PDFs API MathPix Uses MathPix to load PDFs Package PDFPlumber Load PDF files using PDFPlumber Package PyPDFDirectry Load a directory with PDF files Package PyPDFium2 Load PDF files using PyPDFium2 Package PyMuPDF Load PDF files using PyMuPDF Package PDFMiner Load PDF files using PDFMiner Package 此外，导师之前还给我推荐了titipata/scipdf_parser库，能够更好地处理图像和扫描文本，并且运行在docker上，便于部署。\npypdf的介绍\rWelcome to pypdf — pypdf 5.1.0 documentation\nPyPDF 是一个用于处理 PDF 文件的 Python库。它提供了一组工具和功能，用于读取、解析和操作 PDF 文件的内容。\nSplitting\r原文\rFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \u0026ldquo;washed out\u0026rdquo; by surrounding text.\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nWe set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”.\nSee this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\n对于问题提问的文本来说，直接回答一整页肯定是太粗略了。我们最终的目标是检索回答输入查询的文档对象，进一步拆分 PDF 将有助于确保文档相关部分的含义不会被周围的文本“冲淡”。\n所以接下来应该用文本分割器来进行分割（Splitting）处理。这里用一个RecursiveCharacterTextSplitter进行分割。这里使用常见分隔符来对文档进行分割，适用于一般的文本。\n使用RecursiveCharacterTextSplitter无法读取图像或特定区域的文本。\nEmbeddings\r接下来将文本嵌入到向量中去，便于进行相似度指标来识别相关文本。\n这里LangChain支持数十种Embeddings方法。这里我选择了使用Hugging Face，可以选择将模型下载至本地或者使用Hugging Face Inference API来调用接口。这里可以直接使用HuggingFaceEmbeddings来进行处理。非常方便。\nfrom langchain_huggingface import HuggingFaceEmbeddings embeddings_model = HuggingFaceEmbeddings(model_name=\u0026#34;sentence-transformers/all-mpnet-base-v2\u0026#34;) embeddings = embeddings_model vector_1 = embeddings.embed_query(all_splits[0].page_content) vector_2 = embeddings.embed_query(all_splits[1].page_content) assert len(vector_1) == len(vector_2) print(f\u0026#34;Generated vectors of length {len(vector_1)}\\n\u0026#34;) print(vector_1[:10]) Vector Stores\rLangChain的Vector Stores对象包括了一些把文本和Document对象加入到Stores中的方法，然后通过相似性进行一个排列。\nfrom langchain_core.vectorstores import InMemoryVectorStore vector_store = InMemoryVectorStore(embeddings) ids = vector_store.add_documents(documents=all_splits) 此时就完成了存储和排列。\n这里向量存储一般来说是可以连接到现有的Vector Stores中的。\nUsage\r查询和这句话相似的句子 results = vector_store.similarity_search( \u0026#34;Diffusion is a image generation method.\u0026#34; ) ) print(results[0]) 异步查询（用于流程控制） results = await vector_store.asimilarity_search(\u0026#34;What is diffusion?\u0026#34;) print(results[0]) 返回分数 # Note that providers implement different scores; # the score here is a distance metric that varies inversely with similarity. results = vector_store.similarity_search_with_score(\u0026#34;What is Diffusion?\u0026#34;) doc, score = results[0] print(f\u0026#34;Score: {score}\\n\u0026#34;) print(doc) 通过和embedded query的相似度进行查询 embedding = embeddings.embed_query(\u0026#34;What is diffusion\u0026#34;) results = vector_store.similarity_search_by_vector(embedding) print(results[0]) Retrievers\r检索器（Retriever）可以从向量存储中进行构建，但是也可以和非向量形式进行交互。如果我们要构建一个能够检索文档的方法的话，我们可以创建一个runnable的检索器。\nfrom typing import List from langchain_core.documents import Document from langchain_core.runnables import chain @chain def retriever(query: str) -\u0026gt; List[Document]: return vector_store.similarity_search(query, k=1) retriever.batch( [ \u0026#34;What is diffusion?\u0026#34;, \u0026#34;What is forward process?\u0026#34;, ], ) 至此，我们构建了一个能够读多篇PDF文章的、能够对PDF文章进行查询的语义搜索引擎。\nChat Models和Prompt模板\r这里通过Vllm启动LLM，以Qwen2.5-7B-Instruct模型为例。\nfrom langchain_community.llms import VLLM llm = VLLM(model=\u0026#34;/home/ubuntu/jjq/Qwen/Qwen2.5-7B-Instruct/\u0026#34;, trust_remote_code=True, max_new_tokens=512, top_k=10, top_p=0.95, temperature=0.8, max_model_len = 30000, ) print(llm(\u0026#34;What is the capital of France ?\u0026#34;)) 接下来设计Prompt模板。\nfrom langchain import LLMChain from langchain.prompts import PromptTemplate from langchain.memory import ConversationBufferMemory from langchain.chains import ConversationalRetrievalChain from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate template = \u0026#39;\u0026#39;\u0026#39; 【任务描述】 请仔细阅读论文，回答用户给出的问题，尽量具有批判性。 【论文】 {{context}} ----------- {question} \u0026#39;\u0026#39;\u0026#39; # 检索器 retriever = db.as_retriever() # 记忆 memory = ConversationBufferMemory(memory_key=\u0026#34;chat_history\u0026#34;, return_messages=True) # 构建Agent qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory) qa({\u0026#34;question\u0026#34;: \u0026#34;能不能用中文给出论文的优势或者前景？\u0026#34;}) ","date":"2024-11-26T13:45:58+08:00","image":"https://ionfeather.github.io/2024/langchain-learning/cover_hu_d18fac2552526471.png","permalink":"https://ionfeather.github.io/2024/langchain-learning/","title":"学习笔记 | LangChain学习笔记"},{"content":"梦\r这是一个英雄辈出的时代。所谓阴阳师，就是使用牌组与其他人对战来决定胜负的职业。在这里，国之阴阳师是一国中最强的阴阳师，中日韩三国每年都会选拔国之阴阳师，并且对战，决出最后的冠军。\n我和梦梦是青梅竹马，从小便展露了阴阳师的天赋。所谓“绕床弄青梅，郎骑竹马来”，我和梦梦那就是“绕床打牌组，郎打牌组来”。从小与其他人对战，胜利了之后可以选择是否获取一张新卡\n牌放进自己的卡组，最终打一个最强者之间的对战。这个对战从来都是我和梦梦之间的私人聊天与沟通时间。\n随着我们渐渐长大，我和梦梦之间也互生情愫。但是，认真打牌，赢得中日韩三国之间的对战，获得至高无上的荣耀是我们的最大目标。儿女情长，英雄气短，阴阳师需要克制。\n……\n梦梦要去日本打探消息了。她去那里，是为了我们中国能够更了解日本的特殊卡牌。可是，一个人在异国他乡，离开最亲近的人，是那么容易的事情吗？\n终于到了中日韩会赛的时间。\n我期待着到达了梦梦的住所，敲门，迎接我的果然是笑靥如花的梦梦。我们见到对方，思念已久的澎湃难抑制，但我们都克制住了自己，只是眼睛里互相诉说着彼此。\n但梦梦的房间里有股不详的气息。她好像被监视了。“你的房间里曾经有过一个男人躲在里面”，我说。\n梦梦害怕极了，但为了国家能够去刺探信息的人必然非常坚强，她脸色发白，不住地颤抖，但声音很小：“还在吗……这怎么办…”\n我安慰她：“这没什么，这是日本人监视你的手段，但梦梦你肯定也有没被看破的地方。”随后我离开了梦梦的家，脸色发青。\n……\n接下来就是我和八重岛神子最终对战了。近几年韩国式微，只剩中日交战。日本去年赢过了我国。去年对战使用的是30张左右的小牌组，打到后面基本就是6张左右一个循环，对方的强度比我们高。\n八重岛神子太强了，所有人都不相信我能打败他，包括我自己。我用尽心血，准备了一套120张卡牌的超大牌组进行对战。奇妙的是，八重岛神子也拿出来120张的卡组。这极其少见。我们来了一场古典的交锋。\n……\n突然，我想起之前在什么地方，在很久很久的从前，有个人和我说我会赢的，于是我便充满了信心，我的阴霾一扫而空。\n我尽量诚实地描述了我的梦境，可惜忘记了太多。我感觉里面有蛮多意象的。围棋中日韩会战、恋爱、谍战监视、杀戮尖塔烧牌循环、炉石传说对战\u0026hellip;很有意思的一个梦，醒来之后回味了很久。但是看起来可能没有那么有趣，或许我应该加一点戏剧性要素？\n封面\r照片是我2024/11/3的时候在地坛拍的。我想那一天是北京秋天最美的一天。北京的秋天是短暂的，11/3之前雾霾太重，看什么都朦胧；11/3那天的风很大，无数灿烂的叶子不断地落下来，在这一天之后，树梢上就稍微有一些秃了。\n","date":"2024-10-20T13:45:58+08:00","image":"https://ionfeather.github.io/2024/dream/cover_hu_d7f15ad4361f3333.jpg","permalink":"https://ionfeather.github.io/2024/dream/","title":"一个有趣的梦 | 三国阴阳师"}]