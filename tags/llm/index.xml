<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLM on ionfeather&#39;Log</title>
        <link>https://ionfeather.github.io/tags/llm/</link>
        <description>Recent content in LLM on ionfeather&#39;Log</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>ionfeather&#39;Log</copyright>
        <lastBuildDate>Thu, 28 Aug 2025 19:28:49 +0800</lastBuildDate><atom:link href="https://ionfeather.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>书籍阅读 | Speech and Language Processing</title>
        <link>https://ionfeather.github.io/2025/speech-and-language-processing/</link>
        <pubDate>Thu, 28 Aug 2025 19:28:49 +0800</pubDate>
        
        <guid>https://ionfeather.github.io/2025/speech-and-language-processing/</guid>
        <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction
&lt;/h2&gt;&lt;p&gt;阅读&lt;a class=&#34;link&#34; href=&#34;https://web.stanford.edu/~jurafsky/slp3/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Speech and Language Processing&lt;/a&gt;这本书的一些笔记，以供后来的自己参考。&lt;/p&gt;
&lt;h2 id=&#34;words-and-tokens&#34;&gt;Words and Tokens
&lt;/h2&gt;&lt;p&gt;我们需要一个东西来建模语言，下面是我们的选择：&lt;/p&gt;
&lt;h3 id=&#34;words&#34;&gt;Words
&lt;/h3&gt;&lt;p&gt;为什么不用词？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有些语言没有orthographic words&lt;/li&gt;
&lt;li&gt;词的数量会随着文章增长，词汇表永远都会覆盖不足&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;morphemes&#34;&gt;Morphemes
&lt;/h3&gt;&lt;p&gt;语素类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;屈折语素：inflectional morphemes&lt;/li&gt;
&lt;li&gt;派生语素：derivational morphemes&lt;/li&gt;
&lt;li&gt;附着语素：clitic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;语言类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analytic&lt;/li&gt;
&lt;li&gt;polysynthetic&lt;/li&gt;
&lt;li&gt;fusional&lt;/li&gt;
&lt;li&gt;agglutinative&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为什么不用语素？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;语素很复杂，很难定义&lt;/li&gt;
&lt;li&gt;不同语言不同且难以统一&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unicode&#34;&gt;Unicode
&lt;/h3&gt;&lt;p&gt;Unicode的历史&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ASCII&lt;/li&gt;
&lt;li&gt;CJKV&lt;/li&gt;
&lt;li&gt;不断更新中，越来越多，Unicode 16.0已经包含超过150000个字符&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;code-points&#34;&gt;Code Points
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;U+：表示接下来要用Unicode十六进制表示一个code point&lt;/li&gt;
&lt;li&gt;U+0061：0x0061一个意思，也就小写字母a。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;utf-8&#34;&gt;UTF-8
&lt;/h4&gt;&lt;p&gt;目前最常用的encoding字符的方式。中文字符 “中” 的 Unicode 码点是&lt;code&gt;U+4E2D&lt;/code&gt;，UTF-8 编码后为 3 个字节：&lt;code&gt;0xE4 0xB8 0xAD&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;UTF-8是一种变长编码，兼容ASCII。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如「世」，UTF-8 编码是&lt;code&gt;0xE4 B8 96&lt;/code&gt;，其中E4的二进制为&lt;code&gt;11100110H&lt;/code&gt;，开头的&lt;code&gt;1110H&lt;/code&gt;表示这是一个3字节字符的第一个字节。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;subword-tokenization-byte-pair-encoding&#34;&gt;Subword Tokenization: Byte-Pair Encoding
&lt;/h3&gt;&lt;p&gt;上面的三个候选都不行，word和morpheme难以规范定义，character可以通过unicode来定义，但又对于作为tokens来说太小了。&lt;/p&gt;
&lt;p&gt;为什么要tokenize输入？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将输入转换为一组确定的、固定的单元（Token），能让不同的算法和系统在一些简单问题上达成共识。例如困惑度的计算。&lt;/li&gt;
&lt;li&gt;对可复现很重要&lt;/li&gt;
&lt;li&gt;为了消除unknown words的问题&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了消除unknown words问题，现代tokenizers自动引入了token包含那些比words小的token，叫subword。&lt;/p&gt;
&lt;p&gt;使用&lt;a class=&#34;link&#34; href=&#34;https://platform.openai.com/tokenizer&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Tokenizer - OpenAI API&lt;/a&gt;中的&lt;code&gt;GPT-4o &amp;amp;  GPT-4o mini&lt;/code&gt;来分词下面这一大段话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, if we had happened not to ever see the word lower, when it appears we could segment it successfully into low and er which we had already seen. In the worst case, a really unusual word (perhaps an acronym like GRPO) could be tokenized as a sequence of individual letters if necessary.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;最终得到的是
&lt;img src=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250828205105982.png&#34;
	width=&#34;711&#34;
	height=&#34;279&#34;
	srcset=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250828205105982_hu_b760c9dcae731844.png 480w, https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250828205105982_hu_3812253da13ed9c0.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;611px&#34;
	
&gt;
现在最流行的tokenization algorithm有两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Byte-Pair Encoding(BPE)&lt;/li&gt;
&lt;li&gt;Unigram Language modeling(ULM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;bpe&#34;&gt;BPE
&lt;/h4&gt;&lt;p&gt;通过分析训练语料，自动学习出一套子词集合（词汇表），使得高频出现的字符 / 子词组合被合并为更大的子词单位。&lt;/p&gt;
&lt;p&gt;训练方法介绍。&lt;/p&gt;
&lt;h4 id=&#34;bpe-encoder&#34;&gt;BPE encoder
&lt;/h4&gt;&lt;h4 id=&#34;bpe-in-practice&#34;&gt;BPE in practice
&lt;/h4&gt;&lt;p&gt;通常，我们会对 UTF-8 编码文本的&lt;strong&gt;单个字节&lt;/strong&gt;执行 BPE 操作。BPE 处理 “中” 时，输入并非&lt;code&gt;U+4E2D&lt;/code&gt;这个码点，而是&lt;code&gt;E4&lt;/code&gt;、&lt;code&gt;B8&lt;/code&gt;、&lt;code&gt;AD&lt;/code&gt;这三个独立字节。&lt;/p&gt;
&lt;p&gt;仅在&lt;strong&gt;预先切分出的单词内部&lt;/strong&gt;执行 BPE 操作，有助于避免潜在问题。&lt;/p&gt;
&lt;p&gt;一些英语里的小发现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大多数单词的tokens是他们自己，包含词前空格。这样可以避免独立单词和单词内部的subword。&lt;/li&gt;
&lt;li&gt;附着语素Clitics在名字后面分开单独成token，但在常见的词语后面会是token的一部分&lt;/li&gt;
&lt;li&gt;数字通常三位一组&lt;/li&gt;
&lt;li&gt;一些词，如Anyhow和anyhow会有不同的分割方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个和预处理有关系。&lt;/p&gt;
&lt;p&gt;SuperBPE会合并常规的BPE子词分词，效率更高。&lt;/p&gt;
&lt;p&gt;特别地，低资源语言的tokens更碎，就会输出边长，最终LLM的效率变低。&lt;/p&gt;
&lt;h3 id=&#34;rule-based-tokenization&#34;&gt;Rule-based tokenization
&lt;/h3&gt;&lt;p&gt;Penn Treebank Tokenization Standard）：事实性规范。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分开附着语素&lt;/li&gt;
&lt;li&gt;保留连字符连接的词&lt;/li&gt;
&lt;li&gt;分开所有的标点符号&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;sentence-segmentation&#34;&gt;Sentence Segmentation
&lt;/h4&gt;&lt;p&gt;sentence tokenization可以和word tokenization联合处理。&lt;/p&gt;
&lt;h3 id=&#34;corpora&#34;&gt;Corpora
&lt;/h3&gt;&lt;p&gt;语料库和语言数量、使用者的特征都有关。&lt;/p&gt;
&lt;p&gt;code switching：在一次持续的交流）中，说话者或作者交替使用两种或多种 “语码”的现象。&lt;/p&gt;
&lt;p&gt;datasheet：存储一句话的特征，如时间、说话人性格、阶级&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;regular-expressions&#34;&gt;Regular Expressions
&lt;/h3&gt;&lt;p&gt;正则表达式的具体实现。包含字符析取、计数、可选性、通配符、锚点和边界、替换和捕获组、前向断言等。&lt;/p&gt;
&lt;h3 id=&#34;simple-unix-tools-for-word-tokenization&#34;&gt;Simple Unix Tools for Word Tokenization
&lt;/h3&gt;&lt;p&gt;可以在Unix、Linux系统中使用正则表达式。如&lt;code&gt;tr -sc &#39;A-Za-z&#39; &#39;\n&#39; &amp;lt; sh.txt&lt;/code&gt;表示从 &lt;code&gt;sh.txt&lt;/code&gt; 文件中提取所有英文字母，并将非字母字符替换为换行符，同时压缩连续的非字母字符为单个换行符。&lt;/p&gt;
&lt;h3 id=&#34;minimum-edit-distance&#34;&gt;Minimum Edit Distance
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;最小编辑距离&lt;/strong&gt;：将一个字符串通过 “插入”“删除”“替换” 三种基本操作转换为另一个字符串所需的最少操作次数&lt;/p&gt;
&lt;h4 id=&#34;the-minimum-edit-distance-algorithm&#34;&gt;The Minimum Edit Distance Algorithm
&lt;/h4&gt;&lt;p&gt;一个经典的动态规划问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;字符对齐&lt;/strong&gt;：通过回溯编辑距离矩阵中的 “最优路径”，反向推导出将一个字符串转换为另一个字符串的具体操作序列。也就是&lt;strong&gt;路径可视化&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;exercies&#34;&gt;Exercies
&lt;/h3&gt;&lt;h5 id=&#34;21&#34;&gt;2.1
&lt;/h5&gt;&lt;p&gt;Write regular expressions for the following languages.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The set of all alphabetic strings.&lt;/li&gt;
&lt;li&gt;The set of all lowercase alphabetic strings ending in &amp;ldquo;b&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;The set of all strings from the alphabet {a, b} such that each &amp;ldquo;a&amp;rdquo; is immediately preceded by and immediately followed by a &amp;ldquo;b&amp;rdquo;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;22&#34;&gt;2.2
&lt;/h5&gt;&lt;p&gt;Write regular expressions for the following languages. By &amp;ldquo;word&amp;rdquo;, we mean an alphabetic string separated from other words by whitespace, relevant punctuation, line breaks, etc.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The set of all strings with two consecutive repeated words (e.g., &amp;ldquo;Humbert Humbert&amp;rdquo; and &amp;ldquo;the the&amp;rdquo; but not &amp;ldquo;the bug&amp;rdquo; or &amp;ldquo;the big bug&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;All strings that start at the beginning of the line with an integer and end at the end of the line with a word.&lt;/li&gt;
&lt;li&gt;All strings that have both the word &amp;ldquo;grotto&amp;rdquo; and the word &amp;ldquo;raven&amp;rdquo; in them (but not, e.g., words like &amp;ldquo;grottos&amp;rdquo; that merely contain &amp;ldquo;grotto&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;Write a pattern that places the first word of an English sentence in a register. Deal with punctuation.&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;23&#34;&gt;2.3
&lt;/h5&gt;&lt;p&gt;Implement an ELIZA-like program, using substitutions such as those described on page 27. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your program can legitimately engage in a lot of simple repetition.&lt;/p&gt;
&lt;h5 id=&#34;24&#34;&gt;2.4
&lt;/h5&gt;&lt;p&gt;Compute the edit distance (using insertion cost 1, deletion cost 1, substitution cost 1) of &amp;ldquo;leda&amp;rdquo; to &amp;ldquo;deal&amp;rdquo;. Show your work (using the edit distance grid).&lt;/p&gt;
&lt;h5 id=&#34;25&#34;&gt;2.5
&lt;/h5&gt;&lt;p&gt;Figure out whether &amp;ldquo;drive&amp;rdquo; is closer to &amp;ldquo;brief&amp;rdquo; or to &amp;ldquo;divers&amp;rdquo; and what the edit distance is to each. You may use any version of distance that you like.&lt;/p&gt;
&lt;h5 id=&#34;26&#34;&gt;2.6
&lt;/h5&gt;&lt;p&gt;Now implement a minimum edit distance algorithm and use your hand-computed results to check your code.&lt;/p&gt;
&lt;h5 id=&#34;27&#34;&gt;2.7
&lt;/h5&gt;&lt;p&gt;Augment the minimum edit distance algorithm to output an alignment; you will need to store pointers and add a stage to compute the backtrace.&lt;/p&gt;
&lt;h2 id=&#34;n-gram-language-models&#34;&gt;N-gram Language Models
&lt;/h2&gt;&lt;p&gt;本章介绍最简单的语言模型：&lt;strong&gt;N元语法语言模型&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;n-grams&#34;&gt;N-Grams
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;概率链式法则&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;how-to-estimate-probabilities&#34;&gt;How to estimate probabilities
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;马尔科夫假设&lt;/strong&gt;：假设一个单词的出现概率只和前面的一个单词有关。那么n-gram即只和前面的$n-1$个单词有关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最大似然估计&lt;/strong&gt;：已知前一个词$w_{n−1}$​时，当前词$w_n$​的概率&lt;/p&gt;
&lt;p&gt;终止符号（end-symbol）：所有可能句子的概率总和为 1，否则是特定长度的所有句子概率之和为 1。&lt;/p&gt;
&lt;h4 id=&#34;dealing-with-scale-in-large-n-gram-models&#34;&gt;Dealing with scale in large n-gram models
&lt;/h4&gt;&lt;p&gt;Log probabilities&lt;/p&gt;
&lt;p&gt;N元语法的计算现在甚至能达到无限元。&lt;/p&gt;
&lt;p&gt;对N元语法模型进行修剪也是很重要的。&lt;/p&gt;
&lt;h4 id=&#34;evaluating-language-models-training-and-test-sets&#34;&gt;Evaluating Language Models: Training and Test Sets
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;内部评估&lt;/strong&gt;和外部评估。&lt;/p&gt;
&lt;p&gt;训练集、开发集和测试集。&lt;/p&gt;
&lt;h4 id=&#34;evaluating-language-models-perplexity&#34;&gt;Evaluating Language Models: Perplexity
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;Perplexity（PPL）&lt;/strong&gt;：困惑度越低，说明模型对文本的预测越准确（即模型越 “不困惑”）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;具体来说，是“联合概率倒数的几何平均值”。&lt;/li&gt;
&lt;li&gt;在计算的时候常常会取对数来将求乘积变为求和，避免数值问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;perplexity-as-weighted-average-branching-factor&#34;&gt;Perplexity as Weighted Average Branching Factor
&lt;/h5&gt;&lt;p&gt;困惑度也可以理解为&lt;strong&gt;加权平均分支系数&lt;/strong&gt;。其中，语言的 “分支系数”指的是 “任何一个词之后可能出现的下一个词的数量”。&lt;/p&gt;
&lt;h3 id=&#34;sampling-sentences-from-a-language-model&#34;&gt;Sampling sentences from a language model
&lt;/h3&gt;&lt;p&gt;“0-1 数轴 + 区间映射”来理解采样的基本原理。&lt;/p&gt;
&lt;h3 id=&#34;generalizing-vs-overfitting-the-training-set&#34;&gt;Generalizing vs. overfitting the training set
&lt;/h3&gt;&lt;p&gt;对于莎士比亚文本和华尔街日报的文本，两者差异过大以至于不能分别作为训练集和测试集。&lt;/p&gt;
&lt;p&gt;所以说要确保训练集和测试集的领域要相似。&lt;/p&gt;
&lt;h3 id=&#34;smoothing-interpolation-and-backoff&#34;&gt;Smoothing, Interpolation, and Backoff
&lt;/h3&gt;&lt;p&gt;zero probability n-grams有两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;低估了词语序可能出现的可能性，导致最终的性能变差&lt;/li&gt;
&lt;li&gt;困惑度无法计算，因为无法除以0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此需要Smoothing或者discounting&lt;/p&gt;
&lt;h4 id=&#34;laplace-smoothing&#34;&gt;Laplace Smoothing
&lt;/h4&gt;&lt;p&gt;其实也就是add one smoothing，就是对于所有的N元语法都加一。&lt;/p&gt;
&lt;p&gt;对于语言模型来说，结果并不是很好。对文本分类有效。&lt;/p&gt;
&lt;h4 id=&#34;add-k-smoothing&#34;&gt;Add-k Smoothing
&lt;/h4&gt;&lt;p&gt;也就是对所有的都加K。&lt;/p&gt;
&lt;p&gt;对语言模型来说仍然效果一般。&lt;/p&gt;
&lt;h4 id=&#34;language-model-interpolation&#34;&gt;Language Model Interpolation
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;n 元语法插值法：加权融合不同阶数 n 元语法的概率&lt;/strong&gt;，避免高阶的n元语法零概率导致的预测失效。&lt;/p&gt;
&lt;p&gt;加权的$\lambda$应该设置成多少呢？可以从预留集held-out corpus中学习。使用EM（期望最大化）算法来学习。&lt;/p&gt;
&lt;h4 id=&#34;stupid-backoff&#34;&gt;Stupid Backoff
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;回退模型&lt;/strong&gt;：高阶n阶的模型无法使用的时候，回退到低阶模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discount&lt;/strong&gt;：要让回退模型（backoff model）输出合理的概率分布，我们必须对高阶 n 元语法的概率进行 “折扣处理”（discount），从而预留出部分概率余量（probability mass），供低阶 n 元语法使用。但在实际应用中，人们常使用一种更简单的 “无折扣回退算法”—— 即名为&lt;strong&gt;Stupid Backoff&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;advanced-perplexitys-relation-to-entropy&#34;&gt;Advanced: Perplexity&amp;rsquo;s Relation to Entropy
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;熵&lt;/strong&gt;：不确定性的度量方式。可以理解是编码某个决策或某条信息所需的最小平均比特数。越不确定，熵越大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;熵率&lt;/strong&gt;：平均的不确定性。自然语言的熵率定义为 “&lt;strong&gt;无限长序列中，每个词的平均熵&lt;/strong&gt;”，反映语言的长期不确定性。例如，英文的熵率约 1-2 比特 / 词，意味着平均每个词需要 1-2 比特来编码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;平稳性&lt;/strong&gt;：序列概率不随着时间改变。自然语言不是，但是N元语法是平稳的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;遍历性&lt;/strong&gt;：长序列中包含了所有的短序列。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shannon-McMillan-Breiman theorem&lt;/strong&gt;：如果语言满足某些正则条件（准确地说，是平稳且遍历的），&lt;strong&gt;序列长度趋近于无穷大时，“序列的平均对数概率的负值” ，即经验熵率会以概率1收敛敛到该过程的理论熵率&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;交叉熵（Cross-Entropy）&lt;/strong&gt;：我们虽然不知道数据的真实概率分布p，但是可以用模型m来近似p。（即我们虽然不知道自然语言的真实情况，但是可以用N元语法来近似。）&lt;strong&gt;交叉熵越小，模型越接近真实分布&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;困惑度&lt;/strong&gt;：&lt;strong&gt;困惑度是熵的指数形式&lt;/strong&gt;。比较直观。&lt;/p&gt;
&lt;h3 id=&#34;excercies&#34;&gt;Excercies
&lt;/h3&gt;&lt;h5 id=&#34;31&#34;&gt;3.1
&lt;/h5&gt;&lt;p&gt;Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 40.&lt;/p&gt;
&lt;h5 id=&#34;32&#34;&gt;3.2
&lt;/h5&gt;&lt;p&gt;Calculate the probability of the sentence &lt;code&gt;i want chinese food&lt;/code&gt;. Give two probabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on page 42, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(i|&amp;lt;s&amp;gt;) = 0.19$ and $P(&amp;lt;/s&amp;gt;|food) = 0.40$.&lt;/p&gt;
&lt;h5 id=&#34;33&#34;&gt;3.3
&lt;/h5&gt;&lt;p&gt;Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why.&lt;/p&gt;
&lt;h5 id=&#34;34&#34;&gt;3.4
&lt;/h5&gt;&lt;p&gt;We are given the following corpus, modified from the one in the chapter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;/s&amp;gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; I do not like green eggs and Sam &amp;lt;/s&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using a bigram language model with add-one smoothing, what is $P(Sam | am)$? Include $&amp;lt;s&amp;gt;$ and $&amp;lt;/s&amp;gt;$ in your counts just like any other token.&lt;/p&gt;
&lt;h5 id=&#34;35&#34;&gt;3.5
&lt;/h5&gt;&lt;p&gt;Suppose we didn’t use the end-symbol $&amp;lt;/s&amp;gt;$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $&amp;lt;/s&amp;gt;$:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; a b  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; b b  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; b a  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; a a
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet a,b is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet a,b is also 1.0.&lt;/p&gt;
&lt;h5 id=&#34;36&#34;&gt;3.6
&lt;/h5&gt;&lt;p&gt;Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating $P(w3|w1,w2)$, where $w3$ is a word which follows the bigram$ (w1,w2)$, in terms of various n-gram counts and V. Use the notation $c(w1,w2,w3)$ to denote the number of times that trigram $(w1,w2,w3)$ occurs in the corpus, and so on for bigrams and unigrams.&lt;/p&gt;
&lt;h5 id=&#34;37&#34;&gt;3.7
&lt;/h5&gt;&lt;p&gt;We are given the following corpus, modified from the one in the chapter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;/s&amp;gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;s&amp;gt; I do not like green eggs and Sam &amp;lt;/s&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $λ₁ = 1/2$ and $λ₂ = 1/2,$ what is $P(Sam|am)$? Include $&amp;lt;s&amp;gt;$ and $&amp;lt;/s&amp;gt;$ in your counts just like any other token.&lt;/p&gt;
&lt;h5 id=&#34;38&#34;&gt;3.8
&lt;/h5&gt;&lt;p&gt;Write a program to compute unsmoothed unigrams and bigrams.&lt;/p&gt;
&lt;h5 id=&#34;39&#34;&gt;3.9
&lt;/h5&gt;&lt;p&gt;Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?&lt;/p&gt;
&lt;h5 id=&#34;310&#34;&gt;3.10
&lt;/h5&gt;&lt;p&gt;Add an option to your program to generate random sentences.&lt;/p&gt;
&lt;h5 id=&#34;311&#34;&gt;3.11
&lt;/h5&gt;&lt;p&gt;Add an option to your program to compute the perplexity of a test set.&lt;/p&gt;
&lt;h5 id=&#34;312&#34;&gt;3.12
&lt;/h5&gt;&lt;p&gt;You are given a training set of 100 numbers that consists of 91 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?&lt;/p&gt;
&lt;h2 id=&#34;logistic-regression-and-text-classification&#34;&gt;Logistic Regression and Text Classification
&lt;/h2&gt;&lt;p&gt;经典任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sentiment analysis&lt;/li&gt;
&lt;li&gt;spam detection&lt;/li&gt;
&lt;li&gt;language id&lt;/li&gt;
&lt;li&gt;authorship attribution&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;machine-learning-and-classification&#34;&gt;Machine Learning and Classification
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;人工规则很脆弱，数据一变化就无法使用&lt;/li&gt;
&lt;li&gt;LLM的弱点：幻觉、无法解释。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此最常见的分类方法是&lt;strong&gt;有监督机器学习&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;概率分类器：输出&lt;strong&gt;样本属于每个类别的概率&lt;/strong&gt;而不是类别标签，保证在合并的系统里不过早地输出结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分类器的核心组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A feature representation of the input&lt;/li&gt;
&lt;li&gt;A classificaition function that computes $\hat{y}$&lt;/li&gt;
&lt;li&gt;An objective funcion that we want to potimize for learning
&lt;ul&gt;
&lt;li&gt;loss function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An algorithm for optimizing the objective function
&lt;ul&gt;
&lt;li&gt;stochastic gradient descent algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-sigmoid-function&#34;&gt;The Sigmoid Function
&lt;/h3&gt;&lt;p&gt;二分类逻辑回归的目标是：计算样本属于正类的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一步：计算线性得分$z=w⋅x+b$，值域为$[-\infty, +\infty ]$&lt;/li&gt;
&lt;li&gt;第二步：&lt;strong&gt;通过 Sigmoid 函数转换为概率&lt;/strong&gt;：  将线性得分 z 映射到 $[0,1] $区间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$z$常常被称作Logit（对数几率）。Logit就是Sigmoid的反函数。可以提醒我们后续要加上Sigmoid进行转换，因为$z$并不是一个真实的值。&lt;/p&gt;
&lt;p&gt;特别地，&lt;strong&gt;“正类的对数几率” 与特征呈线性关系&lt;/strong&gt;。也就是当其他条件不变时，特征$x_1$每增加1，Logit就增加$w_1$。这非常有可解释性。&lt;/p&gt;
&lt;h3 id=&#34;classification-with-logistic-regression&#34;&gt;Classification with Logistic Regression
&lt;/h3&gt;&lt;p&gt;当概率大于0.5的时候，就把它分类到正类里。&lt;/p&gt;
&lt;h4 id=&#34;sentiment-classification&#34;&gt;Sentiment Classification
&lt;/h4&gt;&lt;p&gt;举了一个例子。&lt;/p&gt;
&lt;h4 id=&#34;other-classification-tasks-and-features&#34;&gt;Other Classification Tasks and Features
&lt;/h4&gt;&lt;p&gt;Period disambiguation：确定句号是EOS还是其他。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Designing v.s. Learning features：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;刚刚的例子，特征都是人工设计的。此外还有：
&lt;ul&gt;
&lt;li&gt;feaure interactions：基础特征组合成的复杂特征&lt;/li&gt;
&lt;li&gt;feature templates：抽象的特征规范来定义特征。这里的特征空间是稀疏的，此外特征一般是字符串描述的Hash值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;人工设计太复杂了。因此现代的NLP系统都是用Representation Learning来解决。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;standardize和normalize。&lt;/p&gt;
&lt;h4 id=&#34;processing-many-examples-at-once&#34;&gt;Processing many examples at once
&lt;/h4&gt;&lt;p&gt;如果有许多的值要计算，可以使用matrix arithmetic来一次计算完。&lt;/p&gt;
&lt;h3 id=&#34;multinomial-logistic-regression&#34;&gt;Multinomial Logistic Regression
&lt;/h3&gt;&lt;p&gt;多项逻辑回归也称softmax regression，老的教材上也叫maxent clasifier。&lt;/p&gt;
&lt;p&gt;在多项逻辑回归中，直接输出结果而不是一个概率值。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hard classification&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;softmax&#34;&gt;Softmax
&lt;/h4&gt;&lt;p&gt;Sigmoid函数在多分类情况下的推广。&lt;/p&gt;
&lt;h4 id=&#34;applying-softmax-in-logistic-regression&#34;&gt;Applying Softmax in Logistic Regression
&lt;/h4&gt;&lt;p&gt;可以使用矩阵运算方式加快计算。&lt;/p&gt;
$$\hat{y}=softmax(Wx+b)$$&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2506.11035&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Doumbouya et al., 2025&lt;/a&gt;是这么认为的：逻辑回归将矩阵的每一行 $w_k$视为&lt;strong&gt;第 $k$ 类的原型（prototype）&lt;/strong&gt;，由于两个向量的相似度越高，它们的点积（dot product）值就越大，因此点积可作为衡量向量相似度的函数。模型最终将输入分配给相似度最高的类别。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;features-in-multinomial-logistic-regression&#34;&gt;Features in Multinomial Logistic Regression
&lt;/h4&gt;&lt;p&gt;特征权重同时依赖于输入文本和输出类别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830163440597.png&#34;
	width=&#34;744&#34;
	height=&#34;837&#34;
	srcset=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830163440597_hu_dcd37b06fe097e65.png 480w, https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830163440597_hu_87f8d0d9a05a6baf.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;88&#34;
		data-flex-basis=&#34;213px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;learning-in-logistic-regression&#34;&gt;Learning in Logistic Regression
&lt;/h3&gt;&lt;p&gt;逻辑回归是如何实现学习的？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使system output（classifier output）和gold output（correct output）越接近越好。两者之间的距离可以称作&lt;strong&gt;损失函数&lt;/strong&gt;或者&lt;strong&gt;代价函数&lt;/strong&gt;。下面介绍交叉熵。&lt;/li&gt;
&lt;li&gt;需要一个算法来最小化损失函数。下面介绍随机梯度下降算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-cross-entropy-loss-function&#34;&gt;The Cross-entropy Loss Function
&lt;/h3&gt;&lt;p&gt;条件最大似然估计：在给定$x$下，选择参数$w$和$b$使得$y$的对数概率最大。&lt;/p&gt;
&lt;p&gt;这里损失函数是&lt;strong&gt;负对数似然损失（negative log likelihood loss）&lt;/strong&gt;，通常也被称为&lt;strong&gt;交叉熵损失（cross-entropy loss）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;介绍了一下为什么&lt;strong&gt;最小化交叉熵损失可以使得真实分布和预测分布更加接近&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent
&lt;/h3&gt;&lt;p&gt;梯度下降算法的原理。介绍了梯度、学习率。&lt;/p&gt;
&lt;h4 id=&#34;the-gradient-for-logistic-regression&#34;&gt;The Gradient for Logistic Regression
&lt;/h4&gt;&lt;p&gt;逻辑回归的梯度就是
&lt;/p&gt;
$$\frac{\partial L_{\mathrm{CE}}(\hat{y},y)}{\partial w_{j}}=-(y-\hat{y})x_{j}$$&lt;p&gt;
也就是预测值$\hat{y}$和实际值$y$之间的差乘输入值$x_j$。&lt;/p&gt;
&lt;h4 id=&#34;the-stochastic-gradient-descent-algorithm&#34;&gt;The Stochastic Gradient Descent Algorithm
&lt;/h4&gt;&lt;p&gt;随机梯度下降算法是一种在线算法，可以边接收数据边学习。&lt;/p&gt;
&lt;p&gt;SGD每次用&lt;strong&gt;单个随机样本&lt;/strong&gt;计算梯度。&lt;/p&gt;
&lt;h4 id=&#34;mini-batch-training&#34;&gt;Mini-batch Training
&lt;/h4&gt;&lt;p&gt;batch training和mini-batch training的区别。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;batch gradient：所有的随机样本计算梯度。&lt;/li&gt;
&lt;li&gt;mini-batch gradient：小批量梯度下降算法。每次选择&lt;strong&gt;一小批随机样本&lt;/strong&gt;计算梯度。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;learning-in-multinomial-logistic-regression&#34;&gt;Learning in Multinomial Logistic Regression
&lt;/h3&gt;&lt;p&gt;多项式逻辑回归其实和二项式逻辑回归差不多。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本质是使用独热标签+概率向量的形式进行计算。&lt;/li&gt;
&lt;li&gt;核心是 “对正确类别的预测概率取负对数”，得到交叉熵损失，其越小则预测概率越高。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation-precision-recall-f-measure&#34;&gt;Evaluation: Precision, Recall, F-measure
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;confusion matrix&lt;/li&gt;
&lt;li&gt;accuracy&lt;/li&gt;
&lt;li&gt;precision&lt;/li&gt;
&lt;li&gt;recall&lt;/li&gt;
&lt;li&gt;F-measure
&lt;ul&gt;
&lt;li&gt;F1&lt;/li&gt;
&lt;li&gt;a weighted harmonic mean of precision and recall.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830172049893.png&#34;
	width=&#34;928&#34;
	height=&#34;394&#34;
	srcset=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830172049893_hu_5b835770261a7097.png 480w, https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830172049893_hu_9163edd8010643ed.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;565px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;microaveraging v.s. macroaveraging&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;微观平均：更关注 “整体样本的预测准确性”，&lt;strong&gt;少数类错判代价低于多数类&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;宏观平均：更关注所有的类的错判代价的公平，&lt;strong&gt;少数类和多数类的代价相等&lt;/strong&gt;
&lt;img src=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830173257005.png&#34;
	width=&#34;1084&#34;
	height=&#34;468&#34;
	srcset=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830173257005_hu_bca6ffdfb72f8971.png 480w, https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830173257005_hu_899e55b55133737a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;231&#34;
		data-flex-basis=&#34;555px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;test-sets-and-cross-validation&#34;&gt;Test sets and Cross-validation
&lt;/h3&gt;&lt;p&gt;Cross-validation：解决测试集不足的问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;固定训练集和测试集。&lt;/li&gt;
&lt;li&gt;训练集中进行分割。
&lt;img src=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830173844382.png&#34;
	width=&#34;895&#34;
	height=&#34;462&#34;
	srcset=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830173844382_hu_4558bdf3b4166cbe.png 480w, https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830173844382_hu_e53d98bd5ba967c8.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;193&#34;
		data-flex-basis=&#34;464px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;statistical-significance-testing&#34;&gt;Statistical Significance Testing
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;统计显著性检验&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不只是简单地检查A在测试集上的结果$M(A,x)$好于B在测试集上的结果$M(B,x)$。如果差很小的话，其实不一定能证明A的结果比B小，不具有统计学上的显著性。&lt;/li&gt;
&lt;li&gt;设计一个效应量$\delta (x)=M(A,x)-M(B,x)$，原假设是$H_0 :\delta (x)\leq 0$。这样计算p值是否小于阈值可以得出是否显著性地A比B要好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在NLP中，一般不用ANOVAs或者t检验，而是使用非参数检验：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;近似随机化检验&lt;/strong&gt;（Approximate Randomization Test）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bootstrap 检验&lt;/strong&gt;（Bootstrap Test）&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;方差分析和t检验都需要有假设：方差齐性或数据服从正态分布。所以只能用非参数检验。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;the-paired-bootstrap-test&#34;&gt;The Paired Bootstrap Test
&lt;/h4&gt;&lt;p&gt;Bootstrap 检验的核心是 “重抽样”—— 从原始测试集&lt;code&gt;x&lt;/code&gt;中&lt;strong&gt;有放回地随机抽取&lt;/strong&gt;生成新的测试集。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830181020970.png&#34;
	width=&#34;1097&#34;
	height=&#34;490&#34;
	srcset=&#34;https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830181020970_hu_8f406568f15b6414.png 480w, https://ionfeather.github.io/2025/speech-and-language-processing/assets/IMG-20250830181020970_hu_c48413deb765f0e1.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;223&#34;
		data-flex-basis=&#34;537px&#34;
	
&gt;
在新的测试集上，P 值等于 “重抽样测试集中，$d(x^{(i)})≥2d(x)$的数量占总重抽样次数b的比例”。&lt;strong&gt;判断此时的p值是否低于阈值就可以判断出是否是数据集本身的偏差导致了A比B要结果好。&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;LLM的解释：Bootstrap 重抽样，就是在 “不改变天平初始倾斜（测试集偏向）” 的前提下，反复放 “随机重量（重抽样的样本）”，看左边会比右边多低多少格 —— 如果只是天平本身歪了，随机放重量时，左边最多低 2 格左右（常规波动）；如果左边真的更重，就可能低 4 格以上（极端情况）。这种极端情况多了，超过了阈值，我们就更相信是天平本身的问题。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;avoiding-harms-in-classification&#34;&gt;Avoiding Harms in Classification
&lt;/h3&gt;&lt;p&gt;representational harms：由于对特定社会群体的贬低或刻板印象导致的伤害。&lt;/p&gt;
&lt;p&gt;toxic detection&lt;/p&gt;
&lt;p&gt;model card&lt;/p&gt;
&lt;h3 id=&#34;interpereting-models&#34;&gt;Interpereting Models
&lt;/h3&gt;&lt;p&gt;模型的可解释性也是很重要的。逻辑回归就是比较好的可解释的模型。&lt;/p&gt;
&lt;h3 id=&#34;advanced-regularization&#34;&gt;Advanced: Regularization
&lt;/h3&gt;&lt;p&gt;regularization来解决过拟合的问题，提高模型泛化能力。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L2 regulaization&lt;/li&gt;
&lt;li&gt;L1 regulaization&lt;/li&gt;
&lt;li&gt;lasso&lt;/li&gt;
&lt;li&gt;ridge&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;embeddings&#34;&gt;Embeddings
&lt;/h2&gt;&lt;p&gt;分布假说：相似的上下文总会表现出相似的意思。&lt;/p&gt;
&lt;p&gt;Embeddings分类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;static embeddings&lt;/li&gt;
&lt;li&gt;contextualized embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;学习embeddings和它的意义的理论称为向量语义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自监督模型&lt;/li&gt;
&lt;li&gt;representation learning的一种&lt;/li&gt;
&lt;li&gt;无需通过特征工程的人工制造representations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lexical-semantics&#34;&gt;Lexical Semantics
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;lemma：citation form，词元，引用形式，一个词的基本形式。&lt;/li&gt;
&lt;li&gt;wordform：词形，一个词的具体使用形态。&lt;/li&gt;
&lt;li&gt;word sense：词义。&lt;/li&gt;
&lt;li&gt;synonymy：同义关系。&lt;/li&gt;
&lt;li&gt;word similarity：词语相似度。
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1408.3456&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;SimLex-999&lt;/a&gt;中就让人们给一个词和另一个词的相似度打分。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;word relatedness/association：词汇关联性，所有能让词汇有关联感的关系。
&lt;ul&gt;
&lt;li&gt;semantic fields&lt;/li&gt;
&lt;li&gt;topic models：特别地有Latent Dirichlet Allocation，LDA&lt;/li&gt;
&lt;li&gt;最常见的关系
&lt;ul&gt;
&lt;li&gt;hypernymy or IS-A&lt;/li&gt;
&lt;li&gt;antonymy&lt;/li&gt;
&lt;li&gt;mernoymy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;connotation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;vector-semantics-the-intuition&#34;&gt;Vector Semantics: The Intuition
&lt;/h3&gt;&lt;p&gt;一个单词可以表示为&lt;strong&gt;多维语义空间中的一个点&lt;/strong&gt;。而这个多维语义空间是从单词邻居的分布规律中推导而来的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tf-idf&lt;/li&gt;
&lt;li&gt;word2vec&lt;/li&gt;
&lt;li&gt;cosine&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;simple-count-based-embeddings&#34;&gt;Simple Count-based Embeddings
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;词汇表一般在1-5万之间&lt;/li&gt;
&lt;li&gt;稀疏向量表示：多数数值为0，目前有有效算法来有效存储和计算&lt;/li&gt;
&lt;li&gt;权重函数
&lt;ul&gt;
&lt;li&gt;计数的时候可以用权重函数&lt;/li&gt;
&lt;li&gt;目前最流行的方法是tf-idf&lt;/li&gt;
&lt;li&gt;还有一些历史权重方式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;cosine-for-measuring-similarity&#34;&gt;Cosine for Measuring Similarity
&lt;/h4&gt;&lt;p&gt;使用余弦计算相似度。适用于稀疏长向量。&lt;/p&gt;
&lt;h4 id=&#34;word2vec&#34;&gt;Word2vec
&lt;/h4&gt;&lt;p&gt;embeddings要区别于原来的稀疏长向量，通常指短而稠密的向量。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学习的权重变少，学习更快&lt;/li&gt;
&lt;li&gt;有助于泛化和避免过拟合&lt;/li&gt;
&lt;li&gt;能够更好捕捉同义性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;skip-gram with negative sampling（SGNS）是word2vec两种方法的一种。word2vec是一种&lt;strong&gt;静态embedding&lt;/strong&gt;方法，区别于动态embedding，如BERT表示。&lt;/p&gt;
&lt;p&gt;这里有一个极具创新性的想法 ——&lt;strong&gt;不直接计算 “词与词的关联”（共现矩阵），而是通过一个 “预测任务” 让模型自动学习这种关联，再将学习成果（权重）作为词嵌入&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这被称为&lt;strong&gt;自监督方法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Skip-gram 模型的核心思路如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将目标词与其相邻的语境词视为&lt;strong&gt;正例&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;从词汇表中随机选取其他词语，作为&lt;strong&gt;负例&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;利用&lt;strong&gt;逻辑回归训练一个分类器，使其能够区分上述两种情况&lt;/strong&gt;（即区分 “目标词与语境词是相邻关系” 和 “目标词与随机词无相邻关系”）。&lt;/li&gt;
&lt;li&gt;将训练过程中学到的权重作为embedding。&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;the-classifier&#34;&gt;The Classifier
&lt;/h5&gt;&lt;p&gt;Skip-gram目标是训练一个分类器，计算这个地方填这个词的概率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;核心思路：&lt;strong&gt;一个词是否可能出现在目标词附近，取决于它的嵌入向量与目标词的嵌入向量是否相似&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;相似度计算：点积&lt;/li&gt;
&lt;li&gt;点积结果并非概率值，还需要经过Sigmoid函数运算&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>学习笔记 | LangChain学习笔记</title>
        <link>https://ionfeather.github.io/2024/langchain-learning/</link>
        <pubDate>Tue, 26 Nov 2024 13:45:58 +0800</pubDate>
        
        <guid>https://ionfeather.github.io/2024/langchain-learning/</guid>
        <description>&lt;img src="https://ionfeather.github.io/2024/langchain-learning/cover.png" alt="Featured image of post 学习笔记 | LangChain学习笔记" /&gt;&lt;h2 id=&#34;为什么要学习langchain&#34;&gt;为什么要学习LangChain
&lt;/h2&gt;&lt;p&gt;我希望能够构建一个能阅读PDF论文的Agent，并且能够输出对论文优缺点的评价。&lt;/p&gt;

&lt;div class=&#34;chat --other&#34;&gt;
    &lt;div class=&#34;chat__inner&#34;&gt;
        &lt;div class=&#34;chat__meta&#34;&gt;导师&amp;nbsp;&amp;nbsp;&amp;nbsp;2024-10-12 14:30&lt;/div&gt;
        &lt;div class=&#34;chat__text&#34;&gt;
              
做一个论文阅读的大模型。  

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;


&lt;style&gt;
    .chat {
        margin: 10px;
        padding: 10px;
        position: relative;
         
        transition: transform 0.2s;
         
        max-width: 80%;
        min-width: 15%;
    }
    
    .chat:hover {
        transform: scale(1.05);
    }
    
    .chat.--self {
        text-align: left;
        background-color: #ecf5ff;
        color: #000000;
        border-radius: 15px;
        width: fit-content;
        margin-left: auto;
    }
     
    
    .chat.--self::before {
        content: &#34;&#34;;
        position: absolute;
        right: -18px;
         
        bottom: 5px;
        transform: translateY(-50%);
        border-width: 15px 0 0 20px;
        border-style: solid;
        border-color: transparent transparent transparent #ecf5ff;
         
    }
     
    
    .chat.--other {
        text-align: left;
        background-color: #ffecec;
        color: #333;
        border-radius: 15px;
        position: relative;
        width: fit-content;
    }
     
    
    .chat.--other::before {
        content: &#34;&#34;;
        position: absolute;
        left: -18px;
        bottom: 5px;
        transform: translateY(-50%);
        border-width: 15px 20px 0 0;
        border-style: solid;
        border-color: transparent #ffecec transparent transparent;
    }
     
    
    .chat__meta {
        font-weight: bold;
        font-size: 0.67em;
        color: #707070;
        margin-bottom: 5px;
    }
     
    
    .chat__text {
        font-size: 0.9em;
        margin-left: 10px;
        word-break: break-all;
    }
    
    [data-scheme=&#34;dark&#34;] {
        .chat.--self {
            color: #fefefe;
            background-color: #253958;
        }
        .chat.--self::before {
            border-color: transparent transparent transparent #253958;
        }
        .chat.--other {
            color: #fefefe;
            background-color: #1a1a1a;
        }
        .chat.--other::before {
            border-color: transparent #1a1a1a transparent transparent;
        }
        .chat__meta {
            color: #b1b1b1;
        }
    }
&lt;/style&gt;


&lt;div class=&#34;chat --self&#34;&gt;
    &lt;div class=&#34;chat__inner&#34;&gt;
        &lt;div class=&#34;chat__meta&#34; style=&#34;text-align: right;&#34;&gt;2024-10-12 14:45&amp;nbsp;&amp;nbsp;&amp;nbsp;我&lt;/div&gt;
        &lt;div class=&#34;chat__text&#34;&gt;
              
好的老师。  

        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;


&lt;style&gt;
    .chat {
        margin: 10px;
        padding: 10px;
        position: relative;
         
        transition: transform 0.2s;
         
        max-width: 80%;
        min-width: 15%;
    }
    
    .chat:hover {
        transform: scale(1.05);
    }
    
    .chat.--self {
        text-align: left;
        background-color: #ecf5ff;
        color: #000000;
        border-radius: 15px;
        width: fit-content;
        margin-left: auto;
    }
     
    
    .chat.--self::before {
        content: &#34;&#34;;
        position: absolute;
        right: -18px;
         
        bottom: 5px;
        transform: translateY(-50%);
        border-width: 15px 0 0 20px;
        border-style: solid;
        border-color: transparent transparent transparent #ecf5ff;
         
    }
     
    
    .chat.--other {
        text-align: left;
        background-color: #ffecec;
        color: #333;
        border-radius: 15px;
        position: relative;
        width: fit-content;
    }
     
    
    .chat.--other::before {
        content: &#34;&#34;;
        position: absolute;
        left: -18px;
        bottom: 5px;
        transform: translateY(-50%);
        border-width: 15px 20px 0 0;
        border-style: solid;
        border-color: transparent #ffecec transparent transparent;
    }
     
    
    .chat__meta {
        font-weight: bold;
        font-size: 0.67em;
        color: #707070;
        margin-bottom: 5px;
    }
     
    
    .chat__text {
        font-size: 0.9em;
        margin-left: 10px;
        word-break: break-all;
    }
    
    [data-scheme=&#34;dark&#34;] {
        .chat.--self {
            color: #fefefe;
            background-color: #253958;
        }
        .chat.--self::before {
            border-color: transparent transparent transparent #253958;
        }
        .chat.--other {
            color: #fefefe;
            background-color: #1a1a1a;
        }
        .chat.--other::before {
            border-color: transparent #1a1a1a transparent transparent;
        }
        .chat__meta {
            color: #b1b1b1;
        }
    }
&lt;/style&gt;

&lt;p&gt;使用LangChain听说比较方便。&lt;/p&gt;
&lt;h2 id=&#34;langchain是用来做什么的&#34;&gt;LangChain是用来做什么的？
&lt;/h2&gt;&lt;p&gt;LangChain是一个用于开发由LLM驱动的应用程序的框架。也就是说我们可以把LLM作为内核，LangChain作为外壳，搭建一个程序出来。&lt;/p&gt;
&lt;p&gt;LangChain提供了&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;组件：处理LLM的组件的抽象；&lt;/li&gt;
&lt;li&gt;定制链：把组件拼起来，实现一个特定用例。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于阅读PDF，目前有两个想法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将PDF转为JSON，然后输入到LLM中；&lt;/li&gt;
&lt;li&gt;构建RAG。使用LangChain能够比较方便地实现这个功能，听ZLB说这个也不是很难。我之前的畏难情绪可能太重了，现在写一个文档，激励和记录一下自己学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
    &lt;summary&gt;RAG是什么？&lt;/summary&gt;
    &lt;p&gt;虽然LLM非常强大，但它们对于它们未经训练的信息一无所知。如果您想使用LLM来回答它未经训练的文档相关问题，您需要向其提供这些文档的信息。最常用的方法是通过“检索增强生成”（ retrieval augmented generation，RAG ）。&lt;/p&gt;
&lt;p&gt;检索增强生成的思想是，在给定一个问题时，首先进行检索步骤以获取任何相关文档。然后将这些文档与原始问题一起传递给语言模型，并让它生成一个回答。然而，为了做到这一点，首先需要将文档以适合进行此类查询的格式呈现。&lt;/p&gt;

&lt;/details&gt;

&lt;h2 id=&#34;构造一个语义搜索引擎&#34;&gt;构造一个语义搜索引擎
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/tutorials/retrievers/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Build a semantic search engine | 🦜️🔗 LangChain&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;读取pdf&#34;&gt;读取PDF
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/how_to/document_loader_pdf/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;How to load PDFs | 🦜️🔗 LangChain&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这里，文档中推荐使用了pypdf库。这里&lt;/p&gt;
&lt;p&gt;在实际应用中可以使用其他提取效果更好的库。LangChain支持的PDF格式很多，可以选择一下。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Document Loader&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
          &lt;th&gt;Package/API&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/pypdfloader&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PyPDF&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Uses &lt;code&gt;pypdf&lt;/code&gt; to load and parse PDFs&lt;/td&gt;
          &lt;td&gt;Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/unstructured_file&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Unstructured&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Uses Unstructured&amp;rsquo;s open source library to load PDFs&lt;/td&gt;
          &lt;td&gt;Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/amazon_textract&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Amazon Textract&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Uses AWS API to load PDFs&lt;/td&gt;
          &lt;td&gt;API&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/mathpix&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MathPix&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Uses MathPix to load PDFs&lt;/td&gt;
          &lt;td&gt;Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/pdfplumber&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDFPlumber&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Load PDF files using PDFPlumber&lt;/td&gt;
          &lt;td&gt;Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/pypdfdirectory&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PyPDFDirectry&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Load a directory with PDF files&lt;/td&gt;
          &lt;td&gt;Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/pypdfium2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PyPDFium2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Load PDF files using PyPDFium2&lt;/td&gt;
          &lt;td&gt;Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/pymupdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PyMuPDF&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Load PDF files using PyMuPDF&lt;/td&gt;
          &lt;td&gt;Package&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/integrations/document_loaders/pdfminer&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PDFMiner&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Load PDF files using PDFMiner&lt;/td&gt;
          &lt;td&gt;Package&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此外，导师之前还给我推荐了&lt;a class=&#34;link&#34; href=&#34;https://github.com/titipata/scipdf_parser&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;titipata/scipdf_parser&lt;/a&gt;库，能够更好地处理图像和扫描文本，并且运行在docker上，便于部署。&lt;/p&gt;
&lt;details&gt;
    &lt;summary&gt;pypdf的介绍&lt;/summary&gt;
    &lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://pypdf.readthedocs.io/en/stable/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Welcome to pypdf — pypdf 5.1.0 documentation&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;PyPDF 是一个用于处理 PDF 文件的 Python库。它提供了一组工具和功能，用于读取、解析和操作 PDF 文件的内容。&lt;/p&gt;

&lt;/details&gt;

&lt;h3 id=&#34;splitting&#34;&gt;Splitting
&lt;/h3&gt;&lt;details&gt;
    &lt;summary&gt;原文&lt;/summary&gt;
    &lt;p&gt;For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve &lt;code&gt;Document&lt;/code&gt; objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not &amp;ldquo;washed out&amp;rdquo; by surrounding text.&lt;/p&gt;
&lt;p&gt;We can use &lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/concepts/text_splitters/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;text splitters&lt;/a&gt; for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the &lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/how_to/recursive_text_splitter/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RecursiveCharacterTextSplitter&lt;/a&gt;, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.&lt;/p&gt;
&lt;p&gt;We set &lt;code&gt;add_start_index=True&lt;/code&gt; so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”.&lt;/p&gt;
&lt;p&gt;See &lt;a class=&#34;link&#34; href=&#34;https://python.langchain.com/docs/how_to/document_loader_pdf/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;this guide&lt;/a&gt; for more detail about working with PDFs, including how to extract text from specific sections and images.&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;对于问题提问的文本来说，直接回答一整页肯定是太粗略了。我们最终的目标是检索回答输入查询的文档对象，进一步拆分 PDF 将有助于确保文档相关部分的含义不会被周围的文本“冲淡”。&lt;/p&gt;
&lt;p&gt;所以接下来应该用文本分割器来进行分割（Splitting）处理。这里用一个&lt;code&gt;RecursiveCharacterTextSplitter&lt;/code&gt;进行分割。这里使用常见分隔符来对文档进行分割，适用于一般的文本。&lt;/p&gt;

&lt;div class=&#34;notice notice-warning&#34; &gt;
    &lt;div class=&#34;notice-title&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; class=&#34;icon notice-icon&#34; viewBox=&#34;0 0 576 512&#34; fill=&#34;#704343&#34;&gt;&lt;path d=&#34;M570 440c18 32-5 72-42 72H48c-37 0-60-40-42-72L246 24c19-32 65-32 84 0l240 416zm-282-86a46 46 0 100 92 46 46 0 000-92zm-44-165l8 136c0 6 5 11 12 11h48c7 0 12-5 12-11l8-136c0-7-5-13-12-13h-64c-7 0-12 6-12 13z&#34;/&gt;&lt;/svg&gt;&lt;/div&gt;&lt;p&gt;使用&lt;code&gt;RecursiveCharacterTextSplitter&lt;/code&gt;无法读取图像或特定区域的文本。&lt;/p&gt;&lt;/div&gt;

&lt;h3 id=&#34;embeddings&#34;&gt;Embeddings
&lt;/h3&gt;&lt;p&gt;接下来将文本嵌入到向量中去，便于进行相似度指标来识别相关文本。&lt;/p&gt;
&lt;p&gt;这里LangChain支持数十种Embeddings方法。这里我选择了使用Hugging Face，可以选择将模型下载至本地或者使用&lt;code&gt;Hugging Face Inference API&lt;/code&gt;来调用接口。这里可以直接使用&lt;code&gt;HuggingFaceEmbeddings&lt;/code&gt;来进行处理。非常方便。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain_huggingface&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HuggingFaceEmbeddings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;embeddings_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HuggingFaceEmbeddings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sentence-transformers/all-mpnet-base-v2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;embeddings&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embeddings_model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;vector_1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embeddings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embed_query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_splits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;page_content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;vector_2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embeddings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embed_query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_splits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;page_content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;assert&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Generated vectors of length &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;vector-stores&#34;&gt;Vector Stores
&lt;/h3&gt;&lt;p&gt;LangChain的Vector Stores对象包括了一些把文本和Document对象加入到Stores中的方法，然后通过相似性进行一个排列。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain_core.vectorstores&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;InMemoryVectorStore&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;vector_store&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;InMemoryVectorStore&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embeddings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ids&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_documents&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;documents&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;all_splits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;此时就完成了存储和排列。&lt;/p&gt;
&lt;p&gt;这里向量存储一般来说是可以连接到现有的Vector Stores中的。&lt;/p&gt;
&lt;h3 id=&#34;usage&#34;&gt;Usage
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;查询和这句话相似的句子&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;results&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similarity_search&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;Diffusion is a image generation method.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;results&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;异步查询（用于流程控制）&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;results&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;await&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;asimilarity_search&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;What is diffusion?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;results&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;返回分数&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Note that providers implement different scores; &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# the score here is a distance metric that varies inversely with similarity.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;results&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similarity_search_with_score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;What is Diffusion?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;results&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Score: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;通过和embedded query的相似度进行查询&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;embedding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embeddings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embed_query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;What is diffusion&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;results&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similarity_search_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;results&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;retrievers&#34;&gt;Retrievers
&lt;/h3&gt;&lt;p&gt;检索器（Retriever）可以从向量存储中进行构建，但是也可以和非向量形式进行交互。如果我们要构建一个能够检索文档的方法的话，我们可以创建一个runnable的检索器。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;typing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain_core.documents&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Document&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain_core.runnables&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chain&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nd&#34;&gt;@chain&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;retriever&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Document&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similarity_search&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;retriever&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;What is diffusion?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;What is forward process?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;至此，我们构建了一个能够读多篇PDF文章的、能够对PDF文章进行查询的语义搜索引擎。&lt;/p&gt;
&lt;h2 id=&#34;chat-models和prompt模板&#34;&gt;Chat Models和Prompt模板
&lt;/h2&gt;&lt;p&gt;这里通过Vllm启动LLM，以Qwen2.5-7B-Instruct模型为例。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain_community.llms&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VLLM&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;llm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VLLM&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/home/ubuntu/jjq/Qwen/Qwen2.5-7B-Instruct/&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;trust_remote_code&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;max_new_tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;top_k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;top_p&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.95&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;temperature&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;max_model_len&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;30000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;llm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;What is the capital of France ?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;接下来设计Prompt模板。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LLMChain&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain.prompts&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PromptTemplate&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain.memory&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ConversationBufferMemory&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain.chains&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ConversationalRetrievalChain&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain.prompts.chat&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ChatPromptTemplate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SystemMessagePromptTemplate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HumanMessagePromptTemplate&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        【任务描述】
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        请仔细阅读论文，回答用户给出的问题，尽量具有批判性。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        【论文】
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        {{context}}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        -----------
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{question}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 检索器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;retriever&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;db&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;as_retriever&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 记忆&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ConversationBufferMemory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;memory_key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;chat_history&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;return_messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 构建Agent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;qa&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ConversationalRetrievalChain&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_llm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;llm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;retriever&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;qa&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;question&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;能不能用中文给出论文的优势或者前景？&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
        
    </channel>
</rss>
